---
sidebar_position: 1
tags: ["Chain-of-Thought reasoning", "Autonomous Agents", "ReAct framework", "LLMs", "Prompt Engineering", "ReAct implementation", "LangChain agents", "OpenAI Functions", "Tool creation", "Agent comparison", "Agent Toolkits", "Memory in LangChain", "Custom Agents", "SQLDatabase Agent", "ConversationBufferMemory", "Memory Types", "Agent Frameworks", "LangChain", "Tree of Thoughts", "Token Counting", "OpenAI", "Chain-of-Thought Reasoning"]
title: "06. Autonomous Agents with Memory and Tools"
---

## titles

Autonomous Agents with Memory and Tools Advanced Autonomous Agents with Memory and Tools Advanced Autonomous Agents with Memory and Tools Advanced Memory and Agent Frameworks in LangChain Token Counting and Chapter Summary on Autonomous Agents

## summary

- This chapter explores the significance of chain-of-thought reasoning in enhancing the problem-solving capabilities of large language models (LLMs) by breaking down complex problems into manageable components.
- Chain-of-thought reasoning (CoT) is highlighted as a method to guide LLMs through logical steps to reach conclusions, especially useful for tasks requiring deep contextual understanding.
- The chapter provides examples of ineffective and effective CoT, emphasizing the importance of step-by-step reasoning in prompts for generating relevant responses.
- Autonomous agents are introduced as entities that perceive, act, and make decisions within an environment to achieve goals, with components like inputs, goal/reward functions, and available actions.
- The ReAct framework is detailed as an advanced version of CoT, integrating reasoning with actions through tools, and includes a thought loop for problem-solving.
- A practical implementation of ReAct in Python is discussed, focusing on extracting actions and inputs from LLM responses using regular expressions.
- The chapter underscores the adaptability of LLMs through the integration of tools, expanding their action space beyond text generation to include API calls and database interactions.
- Memory, agent planning/execution strategies, and retrieval methods are identified as key components enhancing LLM applications, particularly in chatbots.
- The importance of crafting goal-driven directives and functional tools for LLMs is emphasized to guide their reasoning and expand their capabilities.
- A diagram illustrating the ReAct framework's comparative advantage over 'Reason only' and 'Act only' approaches is described, highlighting its integration of memory and tools for enhanced cognitive capabilities.
- The chapter warns about the unpredictability of LLM responses and suggests handling regex parsing errors by using an LLM to fix previous responses or making new requests.
- A step-by-step guide is provided for initializing a ChatOpenAI instance, defining available tools, and setting a base prompt template for ReAct implementation.
- The process of generating model output, extracting actions and inputs, and calling relevant functions is detailed, emphasizing the importance of structured outputs.
- The chapter introduces the concept of using tools to extend LLM capabilities, detailing three approaches: creating custom tools, using preexisting tools, and leveraging AgentToolkits.
- A practical example demonstrates creating a custom tool in LangChain to count characters in a string, showcasing the integration of tools with LLMs.
- The use of AgentExecutor for detailed logging and the importance of expressive names for Python functions and tool descriptions are highlighted.
- OpenAI Functions are presented as an alternative to the ReAct pattern, with a focus on their ease of implementation and suitability for tasks requiring single tool execution.
- A comparison between OpenAI Functions and ReAct is provided, outlining their unique capabilities and ideal use cases.
- The chapter concludes with recommendations for monitoring tool usage and dividing tasks appropriately to enhance LLM agent performance.
- An image description illustrates the function calling flow using OpenAI functions, emphasizing the practical application of autonomous agents in gathering information dynamically.
- The chapter provides a feature comparison between OpenAI functions and ReAct, highlighting their strengths and trade-offs for different AI frameworks.
- Agent toolkits in LangChain are introduced as a way to quickly automate tasks by bundling multiple tools together, with examples including CSV Agent and SQLDatabase Agent.
- A practical guide is given for creating a CSV Agent to interact with .csv files, demonstrating how to quantify data, identify column names, and create correlation matrices.
- The SQLDatabase agent is showcased for interacting with SQL databases, including adding new users and querying the database, emphasizing the importance of understanding database schemas.
- Customizing standard agents in LangChain is discussed, with key function arguments like prefix, suffix, max_iterations, and max_execution_time highlighted for limiting API and compute costs.
- The chapter explains how to create custom agents using LCEL (LangChain Expression Language), including setting up prompts, binding tools to LLMs, and setting up agent chains.
- Memory in LangChain is explored, distinguishing between long-term memory (LTM) and short-term memory (STM) and their applications in enhancing LLM interactions.
- Practical applications of memory in QA conversation agents are illustrated, showing how STM allows for seamless continuation of conversations and anticipation of follow-up questions.
- The ConversationBufferMemory in LangChain is detailed as a popular memory type that stores multiple chat messages without restriction on history size, with examples of integration into chains.
- The chapter concludes with instructions on adding memory to an agent by incorporating a MessagesPlaceholder into the ChatPromptTemplate and memory into the AgentExecutor, enabling the use of previous messages to answer new queries.
- The chapter explores advanced memory management techniques in LangChain, including ConversationBufferWindowMemory, ConversationSummaryMemory, ConversationSummaryBufferMemory, and ConversationTokenBufferMemory, each suited for different scenarios based on conversation length and token limits.
- It highlights the importance of choosing the right memory type for specific needs, such as maintaining recent interactions or summarizing extended conversations to save token space.
- The chapter introduces advanced agent frameworks like Plan-and-Execute Agents (including BabyAGI and AutoGPT) and the Tree of Thoughts (ToT) framework, which enhances problem-solving by allowing exploration of multiple reasoning paths.
- A detailed example of BabyAGI's agent architecture is provided, showcasing how it integrates OpenAI LLMs with vector databases for adaptive task management.
- The Tree of Thoughts framework is presented as a paradigm shift in AI problem-solving, enabling models to explore various reasoning trajectories and significantly improving success rates in complex tasks.
- LangChain's callbacks are introduced as a powerful tool for monitoring and debugging applications, with examples of global and request-specific callbacks and their appropriate use cases.
- The chapter concludes with practical advice on using verbose arguments for debugging and the strategic application of constructor and request callbacks in large projects.
- The chapter concludes with a focus on token counting in LangChain, demonstrating how to measure token usage during interactions with generative AI models using the `get_openai_callback` context manager.
- It explains the importance of monitoring token usage for budgeting and optimizing prompts, with examples showing how to count tokens for single and multiple requests, including concurrent executions.
- The `get_openai_callback` context manager provides detailed metrics such as successful requests, total cost, total tokens, prompt tokens, and completion tokens, offering insights into API usage and response verbosity.
- A summary of the chapter revisits the key concepts covered, including chain-of-thought reasoning, agent-based architecture in generative AI, memory integration, and advanced agent frameworks like ReAct and OpenAI function calling.
- The chapter sets the stage for the next topic, which will explore image generation using generative AI, promising to cover the history, strengths, and weaknesses of various models and vendors.

## code snippets
```
import re
# Sample text:
text = """
Action: search_on_google
Action_Input: Tom Hanks's current wife
action: search_on_wikipedia
action_input: How old is Rita Wilson in 2023
action : search_on_google
action input: some other query
"""
# Compile regex patterns:
action_pattern = re.compile(r"(?i)action\s*:\s*([^\n]+)", re.MULTILINE)
action_input_pattern = re.compile(r"(?i)action\s*_*input\s*:\s*([^\n]+)",
re.MULTILINE)
# Find all occurrences of action and action_input:
actions = action_pattern.findall(text)
action_inputs = action_input_pattern.findall(text)
def extract_last_action_and_input(text):
    # Compile regex patterns
    action_pattern = re.compile(r"(?i)action\s*:\s*([^\n]+)", re.MULTILINE
    action_input_pattern = re.compile(
        r"(?i)action\s*_*input\s*:\s*([^\n]+)", re.MULTILINE
    )
    # Find all occurrences of action and action_input
    actions = action_pattern.findall(text)
    action_inputs = action_input_pattern.findall(text)
    # Extract the last occurrence of action and action_input
    last_action = actions[-1] if actions else None
    last_action_input = action_inputs[-1] if action_inputs else None
    return {"action": last_action, "action_input": last_action_input}
def extract_final_answer(text):
    final_answer_pattern = re.compile(
        r"(?i)I've found the answer:\s*([^\n]+)", re.MULTILINE
    )
    final_answers = final_answer_pattern.findall(text)
from langchain_openai.chat_models import ChatOpenAI
from langchain.prompts.chat import SystemMessagePromptTemplate
chat = ChatOpenAI(model_kwargs={"stop": ["tool_result:"],})
def count_characters_in_string(string):
    return len(string)
tools = [
    Tool.from_function(
        func=count_characters_in_string,
        name="Count Characters in a text string",
        description="Count the number of characters in a text string",
    )
]
from langchain.agents import AgentExecutor, create_react_agent
from langchain import hub
from langchain_openai import ChatOpenAI
from langchain.tools import Tool
model = ChatOpenAI()
prompt = hub.pull("hwchase17/react")
agent = create_react_agent(model, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
from langchain.chains import LLMMathChain
from langchain import hub
from langchain.agents import create_openai_functions_agent, Tool, AgentExecutor
from langchain_openai.chat_models import ChatOpenAI
model = ChatOpenAI(temperature=0)
llm_math_chain = LLMMathChain.from_llm(llm=model, verbose=True)
prompt = hub.pull("hwchase17/openai-functions-agent")
def google_search(query: str) -> str:
    return "James Phoenix is 31 years old."
tools = [
    Tool(
        func=llm_math_chain.run,
        name="Calculator",
        description="useful for when you need to answer questions about math",
    ),
    Tool(
        func=google_search,
        name="google_search",
        description="useful for when you need to find out about someones age",
    ),
]
from langchain.agents.agent_types import AgentType
from langchain_experimental.agents.agent_toolkits import create_csv_agent
from langchain_openai.chat_models import ChatOpenAI
agent = create_csv_agent(
    ChatOpenAI(temperature=0),
    "data/heart_disease_uci.csv",
    verbose=True,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
)
from langchain.agents import create_sql_agent
from langchain_community.agent_toolkits import SQLDatabaseToolkit
from langchain.sql_database import SQLDatabase
from langchain.agents.agent_types import AgentType
from langchain_openai.chat_models import ChatOpenAI
db = SQLDatabase.from_uri("sqlite:///./data/demo.db")
toolkit = SQLDatabaseToolkit(db=db, llm=ChatOpenAI(temperature=0))
agent_executor = create_sql_agent(
    llm=ChatOpenAI(temperature=0),
    toolkit=toolkit,
    verbose=True,
    agent_type=AgentType.OPENAI_FUNCTIONS,
)
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
llm = ChatOpenAI(temperature=0)
@tool
def get_word_length(word: str) -> int:
    """Returns the length of a word."""
    return len(word)
tools = [get_word_length]
from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory()
memory.save_context({"input": "hi"}, {"output": "whats up"})
memory.load_memory_variables({})
from langchain.memory import ConversationBufferMemory
from langchain_openai.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda
from operator import itemgetter
memory = ConversationBufferMemory(return_messages=True)
model = ChatOpenAI(temperature=0)
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "Act as a chatbot that helps users with their queries."),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}"),
    ]
)
chain = (
    {
        "input": lambda x: x["input"],
        "history": RunnableLambda(memory.load_memory_variables) | itemgetter("history"),
    }
    | prompt
    | model
    | StrOutputParser()
)
from langchain.memory import ConversationBufferWindowMemory
memory = ConversationBufferWindowMemory(k=1)
memory.save_context({"input": "hi"}, {"output": "whats up"})
memory.save_context({"input": "not much you"}, {"output": "not much"})
memory.load_memory_variables({})
from langchain.memory import ConversationSummaryMemory, ChatMessageHistory
from langchain_openai import OpenAI
memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))
memory.save_context({"input": "hi"}, {"output": "whats up"})
memory.load_memory_variables({})
from langchain.memory import ConversationSummaryBufferMemory
from langchain_openai.chat_models import ChatOpenAI
memory = ConversationSummaryBufferMemory(llm=ChatOpenAI(), max_token_limit=40)
memory.save_context({"input": "hi"}, {"output": "whats up"})
memory.load_memory_variables({})
from langchain.memory import ConversationTokenBufferMemory
from langchain_openai.chat_models import ChatOpenAI
memory = ConversationTokenBufferMemory(llm=ChatOpenAI(), max_token_limit=50)
memory.save_context({"input": "hi"}, {"output": "whats up"})
memory.load_memory_variables({})
from langchain.callbacks import StdOutCallbackHandler
from langchain.chains import LLMChain
from langchain_openai import OpenAI
from langchain_core.prompts import PromptTemplate
handler = StdOutCallbackHandler()
llm = OpenAI()
prompt = PromptTemplate.from_template("What is 1 + {number} = ")
chain = LLMChain(llm=llm, prompt=prompt)
chain.invoke({"number": 2}, {"callbacks": [handler]})
import asyncio
from langchain.callbacks import get_openai_callback
from langchain_core.messages import SystemMessage
from langchain_openai.chat_models import ChatOpenAI
model = ChatOpenAI()
with get_openai_callback() as cb:
    model.invoke([SystemMessage(content="My name is James")])
total_tokens = cb.total_tokens
print(total_tokens)
# 25
with get_openai_callback() as cb:
    model.invoke([SystemMessage(content="My name is James")])
    model.invoke([SystemMessage(content="My name is James")])
assert cb.total_tokens > 0
print(cb.total_tokens)
# 50
# Async callbacks:
with get_openai_callback() as cb:
    await asyncio.gather(
        model.agenerate(
            [
                [SystemMessage(content="Is the meaning of life 42?")],
                [SystemMessage(content="Is the meaning of life 42?")],
            ],
        )
    )
print(cb.__dict__)
# {'successful_requests': 2, 'total_cost': 0.000455,
# 'total_tokens': 235, 'prompt_tokens': 30,
# 'completion_tokens': 205}
```
