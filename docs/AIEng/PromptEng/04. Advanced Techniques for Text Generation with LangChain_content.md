---
sidebar_position: 1
tags: ["LangChain", "LLM", "Prompt Engineering", "Chat Models", "Output Parsers", "Evaluation Metrics", "Function Calling", "Pydantic", "Query Planning", "Few-Shot Learning", "Data Connection", "Text Splitters", "Task Decomposition", "Document Loaders", "Prompt Chaining", "Sequential Chains", "Document Chains", "Summarization", "Map Reduce", "Refine Documents"]
title: "04. Advanced Techniques for Text Generation with LangChain"
---

## titles

Advanced Techniques for Text Generation with LangChain Advanced Techniques for Text Generation with LangChain

 Output Parsers, Evaluations, and Function Calling Advanced Techniques for Text Generation with LangChain

 Function Calling, Query Planning, and Few-Shot Learning Advanced Techniques for Text Generation with LangChain

 Data Connection, Text Splitters, and Task Decomposition Advanced Techniques for Text Generation with LangChain

 Prompt Chaining, Sequential Chains, and Document Chains Advanced Techniques for Text Generation with LangChain

 Document Chains and Summarization Strategies

## summary

- LangChain is introduced as a versatile framework for creating applications that utilize Large Language Models (LLMs), available in both Python and TypeScript.
- The framework enhances data awareness and agency by connecting language models with external data sources and enabling them to interact with their environment.
- LangChain's modular design includes Model I/O, Retrieval, Chains, Agents, Memory, and Callbacks, each serving distinct functions in LLM workflows.
- Practical setup instructions for LangChain include installation via pip or conda, and the importance of using virtual environments and managing API keys securely is highlighted.
- Chat models like GPT-4 are discussed, emphasizing the use of message types (AIMessage, HumanMessage, SystemMessage) for generating LLM responses.
- A joke generator example demonstrates how to use ChatOpenAI with specific message types to produce tailored outputs.
- Streaming chat models are explained, noting their benefits in reducing latency and enhancing user interactivity, alongside the challenges of parsing streaming outputs.
- The capability to generate multiple LLM responses for dynamic content creation is showcased, along with the use of asynchronous functions for efficient task performance.
- LangChain Prompt Templates are introduced as a method for creating reproducible prompts, offering advantages over simple string formatting like input validation and composition.
- LangChain Expression Language (LCEL) is described, highlighting the use of the pipe operator to chain components for complex data processing pipelines.
- A business name generator example illustrates the application of prompt templates and LCEL in generating structured outputs.
- Output parsers in LangChain are overviewed, including various types like List, Datetime, and Pydantic (JSON) parsers, with a focus on their utility in extracting structured data from LLM responses.
- The Pydantic (JSON) parser is detailed for its flexibility and use of Python type annotations for data validation, demonstrated through a business name rating example.
- The chapter concludes with practical insights into leveraging LangChain for advanced text generation tasks, emphasizing the framework's role in simplifying complex model integrations and enhancing LLM applications.
- The chapter delves into advanced techniques for text generation using LangChain, focusing on output parsers, evaluation metrics, and OpenAI function calling.
- Output parsers in LangChain, particularly the Pydantic (JSON) parser, are highlighted for their ability to structure LLM outputs into predefined formats, enhancing data retrieval and organization.
- A practical example demonstrates using Pydantic models to parse business names and ratings, showcasing the flexibility and utility of output parsers in structuring LLM responses.
- LangChain's evaluation metrics (evals) are introduced as essential tools for measuring the performance of prompt responses, with examples of using exact string matches and pairwise comparisons for accuracy assessment.
- The chapter provides a detailed walkthrough of implementing evaluation metrics with Mistral and GPT-3.5 models, including setting up API keys and processing transaction data for classification.
- Advanced evaluators like labeled_pairwise_string are discussed for comparing outputs from different models or prompts, with GPT-4 used to provide reasoning for the comparisons.
- OpenAI function calling is presented as an alternative to output parsers, enabling models to generate JSON responses for predefined functions, useful for chatbots, API calls, and data extraction.
- A mock function for scheduling meetings illustrates the process of defining function schemas and integrating them with OpenAI models for practical applications.
- The importance of detailed JSON schemas in function calling is emphasized to guide models in understanding when and how to invoke functions properly.
- The chapter concludes with insights into automating evaluation metrics for faster iteration and improvement of LLM applications, reducing reliance on manual reviews.
- The chapter explores advanced techniques in LangChain, including function calling with OpenAI models, query planning, and few-shot learning for text generation.
- Function calling is demonstrated with OpenAI models, showing how to schedule meetings by defining functions and their JSON schemas, and handling parallel function calls.
- A practical example illustrates parallel function calling to schedule multiple meetings, showcasing the flexibility of AI-powered tools in handling complex requests.
- LangChain's integration with Pydantic for function calling simplifies structured data extraction from LLM responses, avoiding the need for manual JSON schema writing.
- Query planning is introduced as a method to parse user queries into executable steps with dependencies, using Pydantic models to structure the query plan.
- Few-shot learning is detailed with examples of creating fixed-length and dynamically selected few-shot prompts to guide LLM responses more effectively.
- The use of LengthBasedExampleSelector is highlighted for adapting prompts based on input length, ensuring responses stay within the LLM's context window limits.
- Limitations of few-shot learning are discussed, including overfitting to examples and token limits, with suggestions for mitigation such as prompt phrasing and fine-tuning.
- The chapter concludes with the benefits of saving and loading LLM prompts as files for better shareability, storage, and versioning, supported by LangChain's JSON and YAML compatibility.
- Practical code examples throughout the chapter demonstrate the implementation of these advanced techniques, providing a hands-on understanding of their application.
- The chapter covers advanced techniques in LangChain for text generation, focusing on data connection, text splitters, and task decomposition.
- Data connection involves loading, transforming, embedding, and storing unstructured data in vector databases, with LangChain providing components for these processes.
- Document loaders in LangChain facilitate the uploading of documents from various sources, including PDFs, Word documents, and web pages, into a cohesive data pipeline.
- Text splitters are introduced as tools to break down large chunks of text to fit the model's context window, with examples including CharacterTextSplitter, TokenTextSplitter, and RecursiveCharacterTextSplitter.
- The importance of balancing document length is highlighted to avoid surpassing the LLM's context length or losing significant contextual information.
- Task decomposition is presented as a strategy to dissect complex problems into manageable subproblems, enhancing the utility and effectiveness of LLMs in problem-solving scenarios.
- Practical examples demonstrate the application of these techniques, such as loading and splitting documents, adding metadata, and using recursive character splitting for large texts.
- The chapter concludes with insights into leveraging these advanced techniques to work seamlessly with various document sources and integrate them into robust data pipelines.
- Code examples throughout the chapter provide hands-on understanding of implementing these techniques, from document loading and splitting to task decomposition with LLMs.
- The chapter explores advanced techniques in LangChain for text generation, focusing on prompt chaining, sequential chains, and document chains to manage complex workflows.
- Task decomposition is introduced as a strategy to break down complex problems into manageable subproblems, enhancing the effectiveness of LLMs in various applications like content generation and interactive conversational agents.
- Prompt chaining is demonstrated through a film company example, where multiple prompt inputs/outputs are combined to automate film creation components such as character creation, plot generation, and scenes/world building.
- Sequential chains are detailed with examples of creating and integrating multiple LLM chains to orchestrate complicated workflows, using RunnablePassthrough, itemgetter, and RunnableParallel for data flow management.
- Document chains are highlighted for summarizing large amounts of text that exceed LLM context length restrictions, with practical examples using CharacterTextSplitter and load_summarize_chain for text summarization tasks.
- The chapter provides code examples for implementing these techniques, including creating LCEL chains for character, plot, and scene generation, and using Pandas for data manipulation in document summarization.
- Key insights include the importance of structuring LCEL chains correctly to avoid errors and the benefits of using different models for ideation and generation within sequential chains.
- Practical steps for generating character scripts and summarizing scenes are outlined, showcasing how to maintain narrative continuity and context in generated content.
- The chapter concludes with the application of these advanced techniques in real-world scenarios, such as automating film creation and summarizing large documents, demonstrating their versatility and power in text generation tasks.
- The chapter delves into advanced techniques for text generation using LangChain, focusing on document chains and summarization strategies to manage large datasets effectively.
- Four core document chain strategies are introduced: Stuff, Refine, Map Reduce, and Map Re-rank, each with unique advantages and disadvantages for different scenarios.
- The Stuff documents chain is highlighted for its simplicity, ideal for small documents, while the Refine documents chain offers iterative response refinement, beneficial for progressive extraction tasks.
- Map Reduce documents chain is detailed for its ability to handle large datasets by processing each document independently and combining results, with an optional compression step for context length management.
- Map Re-rank documents chain is noted for providing confidence scores for each answer, allowing for better response selection, though it may require complex ranking algorithms.
- Practical implementation tips are provided, including changing the chain type within the load_summarize_chain function to suit different summarization needs.
- The chapter concludes by summarizing the importance of document loaders, text splitters, task decomposition, and prompt chaining in enhancing text generation workflows with LangChain.
- A preview of the next chapter teases the integration of vector databases with LangChain documents to improve knowledge extraction accuracy from data.

## code snippets
```
pip install langchain langchain-openai
conda install -c conda-forge langchain langchain-openai
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install openai
from langchain_openai.chat_models import ChatOpenAI
chat = ChatOpenAI(api_key="api_key")
from langchain.schema import AIMessage, HumanMessage, SystemMessage
chat = ChatOpenAI(temperature=0.5)
messages = [SystemMessage(content='Act as a senior software engineer at a startup company.'), HumanMessage(content='Please can you provide a funny joke about software engineers?')]
response = chat.invoke(input=messages)
print(response.content)
for chunk in chat.stream(messages): print(chunk.content, end="", flush=True)
synchronous_llm_result = chat.batch([messages]*2)
from langchain_core.runnables.config import RunnableConfig
config = RunnableConfig(max_concurrency=5)
results = chat.batch([messages, messages], config=config)
from langchain_core.prompts import (SystemMessagePromptTemplate, ChatPromptTemplate)
template = "You are a creative consultant brainstorming names for businesses..."
system_prompt = SystemMessagePromptTemplate.from_template(template)
chat_prompt = ChatPromptTemplate.from_messages([system_prompt])
chain = chat_prompt | model
result = chain.invoke({"industry": "medical", "context":"creating AI solutions by automatically summarizing patient records", "principles":"1. Each name should be short and easy to remember. 2. Each name should be easy to pronounce."})
from langchain_core.prompts import PromptTemplate
prompt=PromptTemplate(template='You are a helpful assistant that translates {input_language} to {output_language}.', input_variables=["input_language", "output_language"],)
system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)
chat.invoke(system_message_prompt.format_messages(input_language="English",output_language="French"))
from langchain.output_parsers import PydanticOutputParser
from pydantic.v1 import BaseModel, Field
from typing import List
class BusinessName(BaseModel): name: str = Field(description="The name of the business") rating_score: float = Field(description='The rating score of the business. 0 is the worst, 10 is the best.')
class BusinessNames(BaseModel): names: List[BusinessName] = Field(description='A list of busines names')
parser = PydanticOutputParser(pydantic_object=BusinessNames)
model = ChatOpenAI()
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])
prompt_and_model = chat_prompt | model
result = prompt_and_model.invoke({"principles": principles, "industry": "Data Science", "format_instructions": parser.get_format_instructions(),})
print(parser.parse(result.content))
chain = prompt | model | output_parser
chain = chat_prompt | model | parser
result = chain.invoke({"principles": principles, "industry": "Data Science", "format_instructions": parser.get_format_instructions(),})
from langchain_mistralai.chat_models import ChatMistralAI
model = ChatMistralAI(model="mistral-small", mistral_api_key=mistral_api_key)
class EnrichedTransactionInformation(BaseModel): transaction_type: Union[Literal["Purchase", "Withdrawal", "Deposit", "Bill Payment", "Refund"], None] transaction_category: Union[Literal["Food", "Entertainment", "Transport", "Utilities", "Rent", "Other"], None,]
output_parser = PydanticOutputParser(pydantic_object=EnrichedTransactionInformation)
chain = prompt | model | StrOutputParser() | remove_back_slashes | output_parser
from langchain.evaluation import load_evaluator
evaluator = load_evaluator("labeled_pairwise_string")
from openai import OpenAI
def schedule_meeting(date, time, attendees): return { "event_id": "1234", "status": "Meeting scheduled successfully", "date": date, "time": time, "attendees": attendees }
functions = [{"type": "function", "function": {"type": "object", "name": "schedule_meeting", "description": "Set a meeting at a specified date and time for designated attendees", "parameters": {"type": "object", "properties": {"date": {"type": "string", "format": "date"}, "time": {"type": "string", "format": "time"}, "attendees": {"type": "array", "items": {"type": "string"}}}, "required": ["date", "time", "attendees"]}}}]
client = OpenAI(api_key=getenv("OPENAI_API_KEY"))
response = client.chat.completions.create(model="gpt-3.5-turbo-1106", messages=messages, tools=functions,)
from langchain.output_parsers.openai_tools import PydanticToolsParser
tools = [convert_to_openai_tool(p) for p in pydantic_schemas]
model = model.bind_tools(tools=tools)
chain = prompt | model | PydanticToolsParser(tools=pydantic_schemas)
from langchain.chains.openai_tools import create_extraction_chain_pydantic
chain = create_extraction_chain_pydantic(Person, model)
from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate
few_shot_prompt = FewShotChatMessagePromptTemplate(example_prompt=example_prompt, examples=examples,)
from langchain.prompts.example_selector import LengthBasedExampleSelector
example_selector = LengthBasedExampleSelector(examples=examples, example_prompt=story_prompt, max_length=1000, get_text_length=num_tokens_from_string,)
dynamic_prompt = FewShotPromptTemplate(example_selector=example_selector, example_prompt=story_prompt, prefix='Generate a story for {character}...', suffix="Character: {character}\nStory:", input_variables=["character"],)
from langchain_community.document_loaders import Docx2txtLoader
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders.csv_loader import CSVLoader
loader = PyPDFLoader("data/principles_of_marketing_book.pdf")
pages = loader.load_and_split()
from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=200, chunk_overlap=0)
from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20, length_function=len)
docs = text_splitter.create_documents(texts, metadatas=[metadatas] * len(texts))
splitted_docs = text_splitter.split_documents(docs)
from langchain_core.prompts.chat import ChatPromptTemplate
character_generation_prompt = ChatPromptTemplate.from_template(...)
from operator import itemgetter
from langchain_core.runnables import RunnablePassthrough
master_chain = RunnablePassthrough() | {...}
from langchain_core.runnables import RunnableParallel
character_script_generation_chain = ({...} | character_script_prompt | model | StrOutputParser())
from langchain_text_splitters import CharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain
text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1500, chunk_overlap=200)
docs = text_splitter.create_documents([all_character_script_text])
chain = load_summarize_chain(llm=model, chain_type="map_reduce")
summary = chain.invoke(docs)
chain = load_summarize_chain(llm=model, chain_type='refine')
```
