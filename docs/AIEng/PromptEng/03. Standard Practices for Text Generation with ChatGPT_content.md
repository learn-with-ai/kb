---
sidebar_position: 1
tags: ["ChatGPT", "Text Generation", "Prompt Engineering", "JSON", "YAML", "YAML Validation", "Mermaid Diagrams", "CSV Generation", "Text Style Unbundling", "Summarization", "Chunking", "SpaCy", "LLM Optimization", "Sliding Window", "Tiktoken", "Sentiment Analysis", "Flask", "Role Prompting", "Hallucination Avoidance", "Thinking Time", "Classification", "Majority Vote", "Criteria Evaluation", "Meta Prompting", "LangChain"]
title: "03. Standard Practices for Text Generation with ChatGPT"
---

## titles

Standard Practices for Text Generation with ChatGPT Advanced Techniques in Text Generation with ChatGPT Advanced Text Processing Techniques with ChatGPT Advanced Techniques in Text Processing with ChatGPT Advanced Prompt Engineering Techniques with ChatGPT Advanced Techniques in Text Generation with ChatGPT

## summary

- Chapter 3 focuses on standard practices for text generation with ChatGPT, emphasizing the importance of tailored prompts for maximizing output.
- Generating lists with GPT-4 is powerful but requires careful prompt design to avoid pitfalls like unexpected formats or commentary.
- Optimizing prompts can control list size, format, and content, such as generating only male Disney characters without film titles.
- Hierarchical lists and structured data formats like JSON and YAML are introduced for more complex outputs, such as detailed article outlines.
- Regular expressions in Python can parse hierarchical lists into structured data, but JSON and YAML offer more straightforward solutions for structured data generation.
- JSON generation with ChatGPT can be streamlined by specifying the format and providing examples, though challenges like invalid payloads may arise.
- YAML offers advantages over JSON, including readability, no need to escape characters, and support for comments, making it suitable for complex or nested structures.
- Using LLMs for filtering and validating YAML payloads demonstrates their capability as reasoning engines for control flow tasks traditionally requiring programming.
- The chapter concludes with strategies for handling invalid payloads and creating Python scripts to manage various LLM response scenarios.
- An image description details a decision-making process for generating text using a YAML schema in a chat-based AI system, highlighting considerations around query validation and output formatting.
- The chapter delves into advanced techniques for text generation with ChatGPT, including validating YAML responses against custom schemas to ensure compatibility with downstream applications.
- Custom exceptions and a validation function are introduced to handle various edge cases in YAML responses, ensuring data integrity and correctness.
- ChatGPT's versatility is showcased through its ability to generate diverse formats like Mermaid diagrams for flowcharts and mock CSV data for testing purposes.
- The 'Explain It like I’m Five' prompt style is highlighted as an effective method for simplifying complex technical documents into understandable summaries.
- Language models' capabilities as universal translators are explored, demonstrating their proficiency in translating and simplifying text across languages, albeit with some loss in translation.
- The importance of providing sufficient context to LLMs for accurate and valuable responses is emphasized, with strategies for refining prompts based on model recommendations.
- Text style unbundling is introduced as a technique for extracting and replicating specific textual features, ensuring consistency in tone and style across communications.
- The chapter concludes with insights into the evolving boundaries of information and the potential of combining language models with other generative models for multimodal content creation.
- An image description details a decision-making flowchart for ChatGPT, emphasizing the model's reliance on context-rich inputs for generating informed responses.
- Practical steps for implementing these techniques, including code snippets for YAML validation and examples of diverse format generation, are provided throughout the chapter.
- Text style unbundling is introduced as a technique to extract and replicate specific textual features like tone, length, and vocabulary, ensuring consistency across communications.
- The process involves analyzing a document to create a style writing guide, which is then used to generate new content with similar characteristics.
- Summarization techniques are explored as essential tools for condensing large texts into digestible summaries, highlighting AI's role in efficient information retrieval.
- For documents exceeding an LLM's context window, a chunking strategy is recommended, involving splitting the text, summarizing each part, and combining these summaries.
- Chunking benefits include fitting within token limits, reducing costs, improving performance, and increasing flexibility, with strategies like splitting by sentence, paragraph, or topic.
- Poor chunking examples demonstrate the loss of context and increased processing load, emphasizing the importance of proper text segmentation.
- Sentence detection using SpaCy is showcased as a method for preserving context and structure in NLP tasks, with a practical code example provided.
- Different chunking strategies are compared in a table, outlining advantages and disadvantages to guide the selection based on specific use cases.
- The chapter concludes with the importance of choosing the right chunking strategy to optimize LLM performance and accuracy in text processing tasks.
- An image description details a summarization pipeline and a topic extraction process after chunking, illustrating efficient text management with AI tools.
- The chapter explores advanced text processing techniques, starting with building a simple chunking algorithm in Python to manage large texts efficiently.
- Sliding window chunking is introduced as a method to divide text into overlapping chunks, preserving context and improving LLM performance.
- Tiktoken, a fast byte pair encoding tokenizer, is highlighted for its advantages in accurate token breakdown and effective resource utilization with OpenAI models.
- Sentiment analysis techniques are discussed, emphasizing the importance of clear instructions, examples, and output format for accurate classification.
- The chapter concludes with the least to most technique for detailed knowledge extraction and a brief introduction to planning a Flask web application architecture.
- Practical code examples and strategies for text chunking, tokenization, and sentiment analysis are provided to enhance understanding and application.
- Challenges in sentiment analysis, such as handling sarcasm and context-specific sentiments, are acknowledged, offering insights into limitations.
- The use of Tiktoken for estimating token usage in Chat API calls is explained, aiding in efficient resource management and cost optimization.
- An image description illustrates the sliding window technique, providing a visual aid for understanding text chunking in generative AI applications.
- The chapter sets the stage for further exploration into Flask application development, hinting at practical applications of the discussed techniques.
- The chapter delves into advanced techniques for text generation with ChatGPT, starting with the creation of a simple Flask 'Hello World' application, including route definition and test case generation.
- The least to most technique is highlighted for its benefits in progressive exploration, flexibility, improved comprehension, and collaborative learning, despite challenges like overreliance on prior knowledge.
- Role prompting is introduced as a method to guide AI responses by assigning specific roles, enhancing creativity and tailored outputs, while acknowledging potential biases and consistency issues.
- Strategies to avoid hallucinations in AI responses are discussed, emphasizing the importance of reference texts and dynamic information retrieval to ensure accuracy.
- The concept of giving GPTs 'thinking time' is explored to improve response accuracy, alongside the inner monologue tactic for hiding reasoning processes in tutoring applications.
- Self-evaluation of LLM responses is presented as a method to refine outputs, demonstrated through the iterative improvement of a 'Hello World' Python function.
- Classification with LLMs is covered, distinguishing between zero-shot and few-shot learning strategies for predicting data categories without extensive training.
- Practical code examples for Flask applications, sentiment analysis, and Python function improvements are provided, showcasing real-world applications of the discussed techniques.
- The importance of evaluating the quality of LLM responses, especially in role prompting and when using reference texts, is emphasized to maintain relevance and accuracy.
- The chapter concludes with insights into the versatility of LLMs in classification tasks, leveraging their extensive training for effective zero-shot and few-shot learning.
- The chapter explores advanced techniques for text generation with ChatGPT, including zero-shot and few-shot learning for classification tasks, demonstrating how LLMs can classify data with high accuracy without prior examples.
- Majority vote classification is introduced as a method to reduce variance in classification labels by soliciting multiple classifications and selecting the most frequent one, though it comes with increased time and cost.
- Criteria evaluation using LLMs like GPT-4 to assess responses from smaller models is discussed, highlighting its cost-effectiveness and speed compared to human-based evaluation, despite mixed evidence on its reliability.
- Meta prompting is presented as a technique to generate other prompts for creating assets in various mediums, such as image generation from product descriptions or optimizing prompts for specific goals.
- The chapter concludes with a summary of key learnings, including the importance of clear directions and examples, and introduces LangChain for creating more advanced prompt engineering workflows in the next chapter.
- Practical applications of these techniques are showcased, such as customer reviews classification, email filtering, and social media sentiment analysis, emphasizing the versatility of LLMs in various domains.
- The benefits and challenges of role prompting are examined, including its ability to tailor response styles and enhance creativity, while acknowledging potential biases and consistency issues.
- A detailed example of building a classification model for sentiment analysis is provided, illustrating the process of few-shot learning to classify text into 'Compliment', 'Complaint', or 'Neutral'.
- The concept of giving GPTs 'thinking time' to improve response accuracy is introduced, alongside the inner monologue tactic for hiding reasoning processes in applications like tutoring.
- The chapter also touches on the ethical considerations and potential risks of exposing sensitive data to LLMs, advising caution when using emails or résumés for classification tasks.

## code snippets
```
Example 3-1. Parsing a hierarchical list
Example 3-2. Parsing a hierarchical list into a Python dictionary
import re
import json
class InvalidResponse(Exception):
class InvalidItemType(Exception):
class InvalidItemKeys(Exception):
class InvalidItemName(Exception):
class InvalidItemQuantity(Exception):
class InvalidItemUnit(Exception):
def validate_response(response, schema):
import yaml
yaml.safe_load(response)
graph TD
    ChooseFood[Choose Food] --> AddToCart[Add to Cart]
    AddToCart --> ConfirmCart[Confirm Cart]
    ConfirmCart --> PayForMeal[Pay for Meal]
import spacy
nlp = spacy.load("en_core_web_sm")
text = "This is a sentence. This is another sentence."
doc = nlp(text)
for sent in doc.sents:
    print(sent.text)
with open("hubspot_blog_post.txt", "r") as f:
text = f.read()
chunks = [text[i : i + 200] for i in range(0, len(text), 200)]
def sliding_window(text, window_size, step_size):
import tiktoken
encoding = tiktoken.get_encoding("cl100k_base")
def count_tokens(text_string: str, encoding_name: str) -> int:
def num_tokens_from_messages(messages, model="gpt-3.5-turbo-0613"):
from flask import Flask
app = Flask(__name__)
@app.route('/')
def hello_world():
    return 'Hello, World!'
def print_hello_world():
    print("Hello, World!")
def print_message(message: str = "Hello, World!") -> None:
    print(message)
from openai import OpenAI
import os
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
def most_frequent_classification(responses):
    count_dict = {}
    for classification in responses:
        count_dict[classification] = count_dict.get(classification, 0) + 1
    return max(count_dict, key=count_dict.get)
```
