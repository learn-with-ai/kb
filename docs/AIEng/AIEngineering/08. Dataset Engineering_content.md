---
sidebar_position: 1
tags: ["Data-centric AI", "Model-centric AI", "Data Curation", "Chain-of-thought (CoT)", "Tool use data", "Finetuning", "PEFT (Parameter-Efficient Fine-Tuning)", "LoRA (Low-Rank Adaptation)", "Data Augmentation", "Data Synthesis", "Synthetic Data", "AI-Powered Data Synthesis", "Rule-based Data Synthesis", "Simulation", "Instruction Data Synthesis", "Llama 3", "AI-generated data", "Code translation", "Unit testing", "Model distillation", "Data Deduplication", "Data Cleaning", "Data Formatting"]
title: "08. Dataset Engineering"
---

## summary

- The quality of a model is heavily dependent on the quality of its training data, emphasizing the importance of dataset engineering in AI development.
- Data-centric AI focuses on improving AI performance through enhancing data quality and processing techniques, contrasting with model-centric AI which focuses on model architecture and training techniques.
- Data curation is critical for teaching models desired behaviors, requiring close collaboration between dataset builders and model developers.
- Chain-of-thought (CoT) and tool use data are highlighted as complex behaviors that require specific types of training data to teach models effectively.
- The chapter discusses the importance of data quality, coverage, and quantity, comparing them to ingredients in cooking to illustrate their impact on model training.
- High-quality data is characterized by being relevant, aligned with task requirements, consistent, correctly formatted, sufficiently unique, and compliant with policies.
- Data coverage ensures a model can handle a wide range of problems by including diverse usage patterns in the training data.
- The Llama 3 model's performance improvements are attributed to enhancements in data quality and diversity, showcasing the impact of data on model success.
- Practical steps for data curation include defining what high-quality data means for specific tasks and ensuring data diversity to cover expected model applications.
- Real-world applications of these concepts are seen in competitions and benchmarks that focus on data-centric approaches to improve AI performance.
- The chapter concludes with the insight that meaningful progress in AI requires investment in both model and data improvements, highlighting the intertwined nature of data and model development.
- Important images or descriptions mentioned in the book include comparisons of data mixes for different training phases of the Llama 3 model and the impact of data diversity and quality on model performance.
- A 7B-parameter model finetuned on a high-quality and diverse dataset outperforms the same model finetuned on datasets that are either diverse or high-quality alone, as illustrated in Figure 8-1.
- The quantity of data needed varies widely, from a single example to millions, with foundation models like Llama 2 and Llama 3 trained on trillions of tokens.
- Finetuning on pre-trained models is typically more efficient than training from scratch, though ossification can occur, especially in smaller models.
- Factors influencing data needs include finetuning techniques, task complexity, and the base model's performance.
- Full finetuning offers the best performance but requires significantly more data than PEFT methods like LoRA, making PEFT preferable for smaller datasets.
- Advanced models perform better with fewer examples, but all models show similar performance with large datasets, as shown in Figure 8-2.
- Starting with a small, well-crafted dataset can help assess the potential benefits of finetuning before investing in larger datasets.
- Improvements with small datasets suggest that more data will further enhance performance, while no improvement may indicate other issues like hyperparameter choices.
- Reducing high-quality data needs can be achieved by first finetuning on lower-quality or less-relevant data, followed by finetuning on the target dataset.
- Experimenting with subsets of a dataset can help estimate the impact of additional data on model performance, with diminishing returns typically observed as dataset size increases.
- Diversity in finetuning tasks significantly impacts model performance, with benefits plateauing after a certain number of tasks, as shown in Figure 8-4.
- Data acquisition involves gathering data through various methods, with application data being ideal due to its relevance and alignment with the task.
- Publicly available datasets can be found on platforms like Hugging Face, Kaggle, and government open data portals, but always require validation.
- Annotation is a critical and challenging part of dataset engineering, requiring clear guidelines for both manual and AI-powered annotations.
- Data augmentation and synthesis are methods to generate additional data, with augmentation creating new data from existing real data and synthesis generating data to mimic real data properties.
- Synthetic data has evolved from simple formats to sophisticated content like doctor's notes and contracts, thanks to AI's capabilities.
- Important images include Figure 8-1 showing the impact of dataset quality and diversity, Figure 8-2 on model performance with varying data sizes, and Figure 8-4 on the effect of task diversity in finetuning.
- Synthetic data is highlighted as a valuable tool for augmenting human-generated data, especially in scenarios where real-world data is scarce or privacy concerns are paramount.
- The chapter discusses the golden data trio: quantity, coverage, and quality, emphasizing how synthetic data can enhance each aspect.
- AI-generated data can surpass human-generated data in certain contexts, such as generating complex math problems or ensuring consistency in preference ratings.
- Privacy concerns in sensitive fields like healthcare and insurance can be mitigated through the use of synthetic data that mimics real data without containing sensitive information.
- Data distillation is introduced as a method to train cheaper, faster models by using data generated by more complex models.
- Traditional data synthesis techniques, including rule-based and simulation methods, are outlined, showcasing their applicability across various industries.
- AI-powered data synthesis opens new possibilities, such as simulating API outcomes without actual calls and using self-play for training game bots and general agents.
- The chapter details how AI's paraphrasing and translation abilities can augment datasets, including the use of back-translation for quality verification.
- Instruction data synthesis is explored as a method for supervised finetuning, with examples of generating instructions and responses for AI models.
- Creative approaches to synthetic data, such as reverse instruction and long-context finetuning, are discussed to improve model performance and handle longer contexts.
- The importance of understanding different types of tasks in dataset engineering is highlighted, with seed tasks and generated tasks serving as foundational and innovative solutions, respectively.
- The chapter concludes by emphasizing the growing role of synthetic data in AI development and the continuous innovation in data synthesis techniques.
- An educational illustration in the chapter contrasts seed tasks and generated tasks, visually representing the concepts of dataset engineering discussed.
- The Llama 3 paper serves as a case study for instruction data synthesis, detailing methods like code translation, back-translation, and AI-generated coding instruction data.
- A workflow for generating programming problem descriptions and solutions is outlined, emphasizing the importance of general programming rules and Chain-of-Thought (CoT) reasoning for quality.
- A correctness analysis and error correction pipeline is described, involving parsers, linters, unit tests, and AI prompts for code revision.
- The combination of code translation, back-translation, and generation methods in Llama 3's data synthesis workflow is highlighted, showcasing its ability to generate over 2.7 million synthetic coding examples.
- Data verification techniques are discussed, focusing on functional correctness and AI judges, with coding-related examples being a popular use case due to their verifiability.
- The limitations of AI-generated data are explored, including quality control, superficial imitation, potential model collapse, and obscured data lineage.
- Model distillation is introduced as a method to train smaller models to mimic larger ones, with examples like DistilBERT and Alpaca demonstrating its effectiveness.
- The importance of data processing and inspection is emphasized, with tips for optimizing efficiency and ensuring data quality.
- Visual aids comparing GPT-4 and GPT-3's performance in dataset engineering are mentioned, illustrating differences in verb-noun pair usage and response lengths.
- The chapter concludes with the significance of manual data inspection and the value it adds to machine learning projects.
- The chapter emphasizes the importance of verifying dataset annotations for trustworthiness and uniqueness to avoid biases.
- Duplicated data can skew model performance, with studies showing significant negative impacts even with minimal duplication.
- Different types of duplications in datasets include whole document, intra-document, and cross-document duplications, each requiring specific detection strategies.
- Deduplication techniques leverage similarity measurements, hashing, and dimensionality reduction, with tools like dupeGuru and Dedupe available for assistance.
- Data cleaning involves removing extraneous formatting tokens, PII, and low-quality data to improve model performance and safety.
- Manual inspection and heuristics are crucial for detecting non-obvious low-quality data patterns, such as annotations made later in sessions being of lower quality.
- Filtering data based on importance or using active learning can optimize the dataset for model training within compute budgets.
- Proper data formatting is essential for model finetuning, requiring adherence to specific tokenizers and chat templates to avoid performance issues.
- The chapter highlights the shift from prompt engineering to finetuning, where the model learns expected behaviors directly from examples without extensive task descriptions.
- Experimentation with different finetuning data formats can significantly impact model performance, emphasizing the need for matching prompt formats during model use.
- Synthetic data has become a practical solution for many use cases, though it requires rigorous evaluation to ensure quality before training.
- The chapter concludes with the observation that dataset design is a creative process, with a wide range of synthesis and verification techniques available for inspiration.
- The increasing importance of data in AI development is reflected in the growing number of professionals involved in data processes for models like GPT-3 and GPT-4.
- Interesting insights include the potential of procedural generation in creating vast, immersive worlds in games and the theoretical possibility of models improving upon themselves.
- Practical steps include ensuring data compliance, which can be a full-time job, and the careful consideration of data quality dimensions for finetuning.
- Real-world applications include training models to detect AI-generated content and the use of synthetic data to overcome the challenges of acquiring high-quality datasets.
- Additional key insights highlight the challenges of automating dataset creation steps, such as annotation guidelines and data verification.
- The final takeaway encourages creativity in dataset design, inspired by the diverse techniques discussed in the chapter.
- Important images or descriptions mentioned in the book include a toy dataset with duplicate examples and example training data for a food classification task.

## code snippets
```
No direct code references found in the chapter.
```
