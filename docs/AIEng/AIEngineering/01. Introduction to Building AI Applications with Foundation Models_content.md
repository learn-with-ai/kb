---
sidebar_position: 1
tags: ["Foundation Models", "Large Language Models (LLMs)", "Self-supervision", "Tokenization", "Multimodal Models", "CLIP", "Prompt Engineering", "Retrieval-Augmented Generation (RAG)", "Finetuning", "Generative AI", "AI Coding Tools", "AI in Education", "Conversational Bots", "AI Applications", "Data Processing", "Workflow Automation", "AI Agents", "Model Adaptation", "Inference Optimization", "Dataset Engineering", "AI Engineering", "ML Engineering"]
title: "01. Introduction to Building AI Applications with Foundation Models"
---

## summary

- The chapter introduces the concept of scale in AI post-2020, highlighting the massive consumption of resources by models like ChatGPT, Google’s Gemini, and Midjourney.
- It discusses the two major consequences of scaling up AI models: increased power and capability for more tasks, and the emergence of model as a service due to high training costs.
- AI engineering is presented as a fast-growing discipline, focusing on building applications on top of readily available models without the need for upfront investment in model building.
- The chapter traces the evolution from language models to large language models (LLMs) and foundation models, emphasizing the role of self-supervision in enabling this growth.
- Language models are explained in detail, including their statistical nature, tokenization process, and the difference between masked and autoregressive language models.
- The importance of self-supervision in overcoming the data labeling bottleneck is highlighted, allowing models to scale up by learning from vast amounts of text data.
- The transition from LLMs to foundation models is discussed, noting their ability to process multiple data modalities beyond text, such as images and videos, making them more powerful and versatile.
- The chapter concludes with an overview of the new AI stack and how the role of an AI engineer differs from that of a traditional ML engineer, focusing on the opportunities and challenges presented by foundation models.
- Interesting insights include the historical context of language models, the probabilistic nature of model completions, and the potential of multimodal models to understand and generate content across different data modalities.
- Practical steps include understanding tokenization, the difference between supervised and self-supervised learning, and the importance of model size and data in training effective AI models.
- Real-world applications range from text generation and translation to image understanding and beyond, showcasing the broad impact of foundation models.
- Additional key insights include the concept of tokens balancing efficiency and meaning, the open-ended nature of model outputs, and the significance of multimodal models in advancing AI research.
- The final takeaway emphasizes the transformative potential of foundation models in AI applications and the evolving role of AI engineers in leveraging these models for innovative solutions.
- Important images mentioned include Figure 1-1 showing GPT-4 tokenization, Figure 1-2 comparing autoregressive and masked language models, and Figure 1-3 illustrating multimodal model token generation.
- The chapter introduces foundation models, highlighting their transition from task-specific to general-purpose models capable of a wide range of tasks without additional training.
- CLIP is presented as an embedding model that generates joint embeddings of texts and images, serving as a backbone for generative multimodal models.
- The Super-NaturalInstructions benchmark is mentioned to illustrate the variety of tasks foundation models can perform.
- Three common AI engineering techniques—prompt engineering, RAG, and finetuning—are introduced as methods to adapt models to specific needs.
- The chapter discusses the advantages of foundation models, including reduced development time and cost, and the ability to perform tasks previously thought impossible.
- Increased AI investments and the low barrier to entry for building AI applications are highlighted as factors contributing to the rapid growth of AI engineering.
- The term 'AI engineering' is justified as the preferred term for building applications on top of foundation models, differentiating it from traditional ML engineering.
- A wide range of foundation model use cases across various industries is presented, emphasizing the versatility and potential of AI applications.
- The chapter concludes with a visual representation of the distribution of use cases among open source applications, highlighting the dominance of conversational bots.
- Important images include conceptual diagrams of AI tasks, a chart of S&P 500 companies mentioning AI, and a pie chart of application categories in open source repositories.
- Enterprises prefer deploying internal-facing AI applications to minimize risks related to data privacy and compliance, as shown in a 2024 a16z Growth report.
- Foundation models are versatile but many applications built on them are close-ended, like classification tasks, due to easier risk estimation.
- AI coding tools, exemplified by GitHub Copilot, have seen significant success, with startups raising hundreds of millions and open-source tools gaining rapid popularity.
- AI's role in software engineering is debated, with figures like NVIDIA's CEO predicting the replacement of human engineers, while others see AI as a productivity booster.
- McKinsey research indicates AI can double productivity for documentation and significantly improve code generation and refactoring, with less impact on complex tasks.
- AI excels in creative tasks, with startups like Midjourney and Adobe Firefly leading in image and video production, showing the technology's broad application potential.
- AI's impact on writing is profound, with tools like ChatGPT reducing task completion time by 40% and improving output quality by 18%, as per an MIT study.
- Education sectors are adapting to AI, using it for personalized learning, though concerns about cheating and the replacement of traditional learning tools persist.
- Conversational bots are becoming versatile companions, from customer support to therapy, with advancements in voice and 3D interfaces enhancing their utility.
- AI's ability to aggregate and summarize information is transforming how we manage data overload, with applications in both consumer and enterprise contexts.
- The organization of unstructured data is a growing challenge, with AI playing a key role in generating searchable descriptions for images, videos, and documents.
- Important images mentioned include charts on enterprise willingness to deploy AI applications, productivity improvements with AI, and stages of AI application development.
- AI's capability extends to generating images when existing ones don't meet user needs, showcasing its versatility beyond data analysis.
- AI excels in data analysis, enabling tasks like data visualization, outlier identification, and revenue forecasting.
- Enterprises leverage AI for extracting structured information from unstructured data, with applications ranging from simple tasks like extracting credit card information to complex ones like parsing contracts.
- The intelligent data processing (IDP) industry is projected to grow significantly, highlighting the economic impact of AI in data management.
- Workflow automation with AI can transform both end-user experiences and enterprise operations by handling repetitive tasks and synthesizing data for model improvement.
- AI agents, capable of planning and using tools, represent a frontier in AI application development, promising to enhance productivity and economic value.
- The chapter emphasizes the importance of thoughtful planning before building AI applications, considering factors like use case evaluation and the role of AI and humans in the application.
- Use case evaluation involves assessing risks and opportunities, from existential threats to business continuity to productivity enhancements.
- The role of AI in applications can vary from critical to complementary, influencing development priorities and user acceptance of AI's imperfections.
- AI product defensibility is crucial, with competitive advantages stemming from technology, data, and distribution channels.
- Setting clear expectations and metrics for success is essential for AI applications, including quality, latency, cost, and user satisfaction metrics.
- Milestone planning for AI products must account for the initial ease of creating demos versus the long-term challenges of developing a fully functional product.
- Maintenance of AI products requires adaptability to rapid technological changes, regulatory shifts, and evolving intellectual property laws.
- The AI engineering stack is foundational to building AI applications, evolving from ML engineering and encompassing application development, model development, and infrastructure.
- Images and descriptions in the book highlight the evolution of AI model performance and cost over time, as well as the job market's demand for AI engineering skills.
- The chapter introduces the three layers of the AI engineering stack: model development, infrastructure, and application development, each with distinct responsibilities and tooling.
- Model development focuses on modeling and training, dataset engineering, and inference optimization, with evaluation being a critical component across all layers.
- Infrastructure is highlighted as the foundational layer, encompassing tooling for model serving, data and compute management, and monitoring, remaining consistent despite advancements in models and applications.
- The evolution of the AI landscape is illustrated through GitHub repository analysis, showing significant growth in AI tooling post-2023, especially in applications and application development categories.
- AI engineering is differentiated from ML engineering by its focus on model adaptation rather than model development, with emphasis on working with larger, compute-intensive models that produce open-ended outputs.
- Model adaptation techniques are categorized into prompt-based techniques, which do not require updating model weights, and finetuning, which does, each with its own advantages and use cases.
- The importance of dataset engineering is underscored, especially in the context of foundation models, where data annotation and manipulation present new challenges.
- Inference optimization is identified as increasingly important due to the higher inference cost and latency associated with foundation models, necessitating more efficient training and inference methods.
- Application development with foundation models shifts differentiation to the development process itself, emphasizing evaluation, prompt engineering, and AI interface creation.
- Evaluation is noted as more challenging with foundation models due to their open-ended nature and the variety of adaptation techniques available, requiring careful benchmarking and selection.
- Prompt engineering's impact on model performance is demonstrated through the example of Gemini's evaluation, highlighting the technique's potential to significantly improve outcomes.
- The AI interface is discussed as a critical component for integrating AI applications into existing products or serving them as standalone solutions, with various interfaces gaining popularity.
- The chapter concludes by emphasizing the enduring principles of AI application development while acknowledging the unique innovations and challenges introduced by foundation models.
- Important images include Figure 1-14, illustrating the three layers of the AI engineering stack, and Figure 1-15, showing the cumulative count of repositories by category over time, highlighting the rapid growth in AI tooling.
- The chapter introduces AI engineering as a discipline emerging from the availability of foundation models.
- It highlights the transition from language models to large language models through self-supervision.
- The importance of interfaces in AI applications is discussed, noting a shift towards attracting frontend engineers.
- Python remains popular in AI, but JavaScript support is growing with libraries like LangChain.js and Transformers.js.
- AI engineering allows for starting with product development before investing in data and models, unlike traditional ML.
- The chapter mentions the challenge of keeping up with the rapid evolution of AI technologies and the community's enthusiasm.
- A visual representation in the book explains the workflow from product to data and model to product in AI engineering.
- The chapter concludes by emphasizing the need for a framework to navigate the overwhelming AI space, which the book aims to provide.
- Interesting insights include the comparison of AI investments to US public school expenditures and the fun fact about the number of AIs listed for various tasks.
- Practical steps include evaluating when to build an AI application and the importance of user feedback in natural language.
- Real-world applications range from browser extensions to chatbots integrated into popular chat apps.
- The chapter also touches on the automation potential in marketing and the challenges of scaling AI solutions in enterprises.
- Additional key insights include the role of AI in content generation and the potential distrust of internet content.
- The final takeaway is the exploration of AI engineering's framework in the subsequent chapters, starting with foundation models.
- Important images include a flowchart showing the AI engineering workflow and a table comparing traditional ML and foundation models in app development.

## code snippets
```
No direct code references found in the chapter.
GitHub Copilot
gpt-engineer
screenshot-to-code
AgentGPT
DB-GPT
SQL Chat
PandasAI
draw-a-ui
GPT-Migrate
AI Code Translator
Autodoc
PentestGPT
AI Commits
```
