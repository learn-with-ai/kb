---
sidebar_position: 1
tags: ["Finetuning", "PEFT (Parameter-Efficient Finetuning)", "Transfer Learning", "LLMs (Large Language Models)", "Adapter-based Techniques", "RAG (Retrieval-Augmented Generation)", "Prompt Caching", "Memory Bottlenecks", "Backpropagation", "FP32", "FP16", "BF16", "Quantization", "Parameter-Efficient Finetuning (PEFT)", "LoRA", "PEFT", "Adapter-based methods", "Soft prompt-based methods", "Low-rank factorization", "QLoRA", "Model Merging", "Federated Learning", "Layer stacking", "Mixture-of-Experts (MoE)", "Hyperparameter tuning", "BFloat16", "Mixed Precision Training", "Transformer-based models"]
title: "07. Finetuning"
---

## summary

- Finetuning is the process of adapting a model to a specific task by further training the whole model or part of the model, adjusting its weights to enhance various aspects such as domain-specific capabilities, safety, and instruction-following ability.
- Compared to prompt-based methods, finetuning requires more up-front investment and incurs a higher memory footprint, making it expensive and challenging, especially with today’s foundation models.
- PEFT (parameter-efficient finetuning) is a memory-efficient approach that has become dominant in the finetuning space, differing from traditional finetuning by focusing on reducing memory requirements.
- Finetuning brings you into the realm of model training where ML knowledge is required, contrasting with prompt-based methods where such knowledge is recommended but not strictly necessary.
- Transfer learning, a concept introduced by Bozinovski and Fulgosi in 1976, is foundational to finetuning, allowing knowledge from one task to accelerate learning for a new, related task.
- Finetuning can be done by both model developers and application developers, with model developers typically post-training a model before release and application developers finetuning pre-trained models for specific tasks.
- The chapter discusses when to finetune, highlighting that finetuning is generally attempted after extensive experiments with prompt-based methods due to its resource-intensive nature.
- Reasons to finetune include improving a model’s quality, enhancing its ability to generate outputs following specific structures, and mitigating biases.
- Reasons not to finetune include the potential degradation of a model’s performance for other tasks, the high up-front investments required, and the availability of alternative methods like carefully crafted prompts.
- The chapter also covers finetuning domain-specific tasks, noting that general-purpose models are becoming more capable and can outperform domain-specific models in many cases.
- An image described in the book illustrates a finetuning process for training large language models, specifically focusing on "Infilling code" using Llama2 Foundation Models with different sizes, serving as an educational tool within the chapter.
- Prompt caching is introduced as a method to reuse repetitive prompt segments, though the number of examples usable with a prompt is limited by the maximum context length, unlike finetuning which has no such limit.
- The chapter discusses the decision between using RAG or finetuning based on whether the model's failures are information-based or behavior-based, with RAG being more effective for information-based failures and finetuning for behavioral issues.
- A study by Ovadia et al. (2024) is mentioned, showing RAG's superiority over finetuning for tasks requiring up-to-date information, such as questions about current events, and how finetuning can enhance a model's performance on specific tasks but may degrade performance in other areas.
- The chapter provides a detailed workflow for adapting a model to a task, starting with prompting, adding examples, implementing RAG for information-based failures, and considering finetuning for behavioral issues, with evaluation being a continuous process throughout.
- Memory bottlenecks in finetuning are highlighted as a significant challenge, with techniques to minimize memory footprint discussed, including parameter-efficient finetuning (PEFT) and quantization.
- The importance of understanding memory requirements for both inference and training is emphasized, with formulas provided for approximate calculations to help in hardware selection.
- Backpropagation is explained as the mechanism behind neural network training, detailing the forward and backward passes and how gradients and optimizer states contribute to memory usage during training.
- The chapter concludes with practical advice on reducing memory needed for activations through gradient checkpointing or activation recomputation, despite the trade-off of increased training time.
- Key images include a flowchart for finetuning within an AI engineering context, a diagram illustrating forward and backward passes in a neural network, and a chart comparing memory usage for different stages of model fine-tuning across various hardware configurations.
- The chapter discusses the impact of numerical representations on a model's memory footprint, highlighting common floating point formats like FP32, FP16, and BF16, and their trade-offs between range and precision.
- Quantization is introduced as a method to reduce memory usage by lowering the precision of numerical values, with practical considerations on what and when to quantize.
- The text explains the benefits and challenges of reduced precision, including improved computation speed but potential performance degradation due to value changes.
- Inference in lower precision has become standard, with major ML frameworks offering post-training quantization (PTQ) for free.
- Training quantization, including quantization-aware training (QAT) and training directly in lower precision, is discussed as a way to reduce training time and cost.
- Parameter-Efficient Finetuning (PEFT) techniques are highlighted as a solution to achieve performance close to full finetuning with significantly fewer trainable parameters.
- The chapter includes a discussion on model merging as a complementary approach to finetuning, combining multiple models for tailored applications.
- Practical implementation opportunities are flagged, such as using quantization to fit models on consumer GPUs and employing PEFT techniques for memory-efficient finetuning.
- The text mentions the importance of loading models in their intended numerical format to avoid performance degradation, citing the example of Llama 2.
- Interesting insights include the introduction of BitNet b1.58, a 1.58-bit per parameter model, and the performance comparison between different numerical formats.
- The chapter concludes with a note on the active research area of reducing precision with minimal impact on model performance and the standard practice of training in higher precision for inference in lower precision.
- Important images mentioned include diagrams of different numerical formats with their range and precision bits, and illustrations of adapter modules in transformer blocks for PEFT.
- Parameter-Efficient Fine-Tuning (PEFT) methods, including adapter-based and soft prompt-based techniques, enable finetuning models with significantly fewer parameters and data, making it accessible on more affordable hardware.
- Adapter-based methods, like LoRA, add trainable parameters to the model without introducing extra inference latency by merging additional modules back into the original layers.
- LoRA (Low-Rank Adaptation) decomposes weight matrices into smaller matrices, updating only these during finetuning, which is both parameter and sample efficient.
- Soft prompt-based methods modify model input processing by introducing trainable tokens that guide model behavior, differing from hard prompts by being continuous and trainable.
- LoRA's popularity is highlighted by its dominance in usage among PEFT methods, as evidenced by GitHub issue analysis, due to its efficiency and effectiveness.
- The chapter discusses the configuration of LoRA, including decisions on which weight matrices to apply it to and the rank of factorization, impacting performance and memory usage.
- Multi-LoRA serving strategies are explored, comparing the trade-offs between merging LoRA weights before serving versus keeping them separate to reduce storage needs and facilitate model switching.
- The potential of low-rank pre-training is considered, with recent attempts showing promise but indicating that full-rank pre-training may still be necessary to sufficiently reduce a model's intrinsic dimension.
- Practical implementation opportunities include experimenting with LoRA configurations for specific tasks and utilizing multi-LoRA serving for efficient model deployment across different use cases.
- Interesting insights include the observation that larger models tend to have lower intrinsic dimensions after pre-training, making them easier to finetune with fewer parameters and data.
- The chapter concludes with a look towards future developments in low-rank pre-training and the ongoing exploration of PEFT methods for model finetuning.
- Important images mentioned include visualizations of LoRA's application to weight matrices, the combination of hard and soft prompts, and the popularity of different PEFT methods based on GitHub issue analysis.
- LoRA adapters reduce the memory footprint of base models and can be shared and reused, available on platforms like Hugging Face and AdapterHub.
- LoRA's main drawback is its performance compared to full finetuning and the complexity involved in modifying the model's implementation.
- Quantized LoRA, like QLoRA, stores model weights in 4 bits, significantly reducing memory usage and enabling finetuning of large models on single GPUs.
- Model merging combines multiple models to create a new, more useful model, offering flexibility and reduced memory footprint, especially for on-device deployment.
- Multi-task finetuning can be approached through simultaneous or sequential finetuning, with model merging offering a method to avoid catastrophic forgetting.
- Model merging is essential for federated learning, allowing the combination of models trained on different devices into a single base model.
- Ensembling combines model outputs for better performance but at a higher inference cost compared to model merging.
- Model merging techniques include summing, layer stacking, and concatenation, with summing further divided into linear combination and spherical linear interpolation (SLERP).
- Pruning redundant task-specific parameters before merging can significantly improve the quality of the final merged models.
- The chapter highlights practical steps for finetuning and model merging, including the use of specific frameworks and techniques for optimal performance.
- Images in the chapter illustrate concepts like ensembling vs. model merging, different model merging approaches, and the impact of pruning on model performance.
- Layer stacking, or frankenmerging, involves combining layers from different models to create unique architectures, requiring further finetuning for optimal performance.
- Goliath-120B is an example of successful frankenmerging, created from two finetuned Llama 2-70B models.
- Layer stacking can train Mixture-of-Experts (MoE) models by copying pre-trained model layers and adding a router for input distribution.
- Komatsuzaki et al. demonstrated that layer stacking can outperform MoE models trained from scratch, as seen with Together AI's Mixture-of-Agents.
- Model upscaling via layer stacking allows creating larger models from existing ones without training from scratch, optimizing resource use.
- Depthwise scaling is a technique for model upscaling, exemplified by the creation of SOLAR 10.7B from a 7B-parameter model.
- Concatenation merges model parameters by summing them, though it's not recommended due to memory footprint concerns.
- Finetuning tactics involve choosing a base model, finetuning method, and framework, with considerations for model size, licenses, and performance.
- OpenAI's finetuning best practices outline progression and distillation paths for model development.
- Adapter techniques like LoRA are cost-effective but may not match full finetuning performance, suitable for small datasets.
- Finetuning frameworks such as LLA MA-Factory and PEFT support various finetuning methods, offering flexibility but requiring compute resources.
- Hyperparameters like learning rate, batch size, number of epochs, and prompt loss weight are crucial for finetuning efficiency and performance.
- The chapter highlights the complexity of finetuning's context, including when to finetune versus using RAG, and the challenges of obtaining high-quality data.
- Interesting insights include the alignment tax phenomenon and the practical challenges of adopting finetuning in businesses.
- Practical steps involve experimenting with hyperparameters and choosing between finetuning methods based on data volume and project needs.
- Real-world applications include model upscaling and the creation of specialized models like Mixture-of-Agents.
- Additional key insights discuss the evolution of finetuning techniques to address memory and resource constraints.
- Final takeaway emphasizes the balance between finetuning's ease of implementation and the complexity of its surrounding context.
- Important images include diagrams illustrating the process of creating MoE models from pre-trained models and the depthwise scaling technique.
- The chapter discusses various finetuning techniques and memory optimization strategies in AI, including the use of surrogate gradients and direct feedback alignment for updating model weights.
- It highlights the importance of understanding model architecture for effectively using LoRA and mentions constraints in some finetuning frameworks due to hardware memory limitations.
- The text explores the confusion between FP16 and BF16 formats and the significance of designing numerical formats that do not compromise system quality.
- Personal anecdotes and references to external resources like Carol Chen’s “Transformer Inference Arithmetic” and EleutherAI’s “Transformer Math 101” are provided for deeper understanding.
- The chapter also touches on partial finetuning, emphasizing the finetuning of layers closest to the output layer for task-specific adjustments.
- Mentions of quantized LoRA works and the acquisition of Xnor.ai by Apple underscore the practical and commercial aspects of model compression technologies.
- The narrative includes a personal story about the pitfalls of model training, such as losing progress due to incorrect checkpoint saving.
- It concludes with a note on the magical effectiveness of simple components in AI, like averaging embeddings, and the ongoing challenges in understanding training stability with small batch sizes.
- Important images or descriptions mentioned in the book include references to external blogs and papers for further reading on inference and training memory calculations.

## code snippets
```
No direct code references found in the chapter.
```
