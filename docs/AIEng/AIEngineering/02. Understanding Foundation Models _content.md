---
sidebar_position: 1
tags: ["Foundation Models", "Transformer Architecture", "Training Data", "Multilingual Models", "Domain-Specific Models", "Seq2seq", "Transformer", "Attention Mechanism", "RNN", "LLM", "Mixture-of-Experts (MoE)", "FLOPs", "Chinchilla Scaling Law", "Scaling Extrapolation", "Supervised Finetuning (SFT)", "Reinforcement Learning from Human Feedback (RLHF)", "Direct Preference Optimization (DPO)", "AI-generated Data", "InstructGPT", "Reinforcement Learning", "Proximal Policy Optimization (PPO)", "Sampling Strategies", "Top-p sampling", "Top-k sampling", "Test time compute", "Structured outputs",  "Finetuning", "Classifier Head", "Constrained Sampling", "Post-training"]
title: "02. Understanding Foundation Models "
---

## summary

- Foundation models are complex and costly to train, with design decisions impacting downstream applications significantly.
- Training data quality and distribution are crucial for a model's capabilities and limitations, with Common Crawl being a common but questionable source.
- Multilingual models face challenges due to the dominance of English in training data, leading to underperformance in under-represented languages.
- Domain-specific models, like those in biomedicine, require curated datasets for tasks not well-covered by general-purpose models.
- The transformer architecture dominates language-based foundation models due to its attention mechanism, despite its limitations.
- Practical steps include curating datasets aligned with specific needs and understanding the impact of model architecture and size on usability.
- Real-world applications highlight the importance of domain-specific models for tasks like drug discovery and cancer screening.
- Interesting insights include the under-representation of languages in training data and the efficiency of tokenization varying by language.
- Final takeaway emphasizes the need for high-quality, domain-specific training data and understanding model architecture for effective application.
- Important images include performance benchmarks across languages and domain distributions in datasets like C4.
- The chapter discusses the evolution and mechanics of foundation models in AI, focusing on seq2seq and transformer architectures.
- Seq2seq models process input tokens sequentially, which can be slow and limit output quality due to reliance on the final hidden state.
- Transformer models address seq2seq limitations by using an attention mechanism, allowing parallel processing and dynamic focus on relevant input tokens.
- The attention mechanism uses key, value, and query vectors to weigh the importance of input tokens, improving model performance.
- Multi-headed attention splits these vectors into smaller parts, enabling the model to attend to different token groups simultaneously.
- Transformer blocks consist of attention and MLP modules, with the number of blocks referred to as the model's layers.
- Models like Llama 2 and Llama 3 are discussed, highlighting their dimensions, feedforward layers, and vocabulary sizes.
- Alternative architectures to transformers, such as RWKV and SSMs (including S4, H3, Mamba, and Jamba), are introduced as promising for long sequence processing.
- The chapter emphasizes the importance of model size, noting that larger models generally perform better due to increased learning capacity.
- Visualizations in the chapter help illustrate the differences between seq2seq and transformer architectures, the attention mechanism, and the composition of transformer models.
- The chapter concludes by noting the ongoing evolution of model architectures and the potential for future advancements to surpass transformers.
- Important images include visualizations of seq2seq versus transformer architectures, the attention mechanism in action, and the composition of transformer and Jamba blocks.
- Newer-generation models like Llama 3-8B outperform older-generation models such as Llama 2-70B, highlighting the rapid advancement in model training techniques.
- The number of parameters in a model helps estimate the compute resources needed for training and inference, with sparse models offering more efficient computation than dense models.
- Mixture-of-experts (MoE) models, such as Mixtral 8x7B, divide parameters into groups of experts, with only a subset active per token, optimizing cost and speed.
- The size of the training dataset, measured in tokens, is crucial for model performance, with models like Llama 3 trained on up to 15 trillion tokens.
- FLOPs measure the compute requirement for training models, with examples like Google’s PaLM-2 requiring 10^25 FLOPs.
- The Chinchilla scaling law suggests that for compute-optimal training, the number of training tokens should be approximately 20 times the model size.
- Scaling extrapolation is a niche research area focusing on predicting optimal hyperparameters for large models based on smaller model studies.
- Emergent abilities in large models, not observable in smaller models, complicate hyperparameter extrapolation.
- Scaling bottlenecks include the limited availability of training data and electricity, with concerns about running out of internet data for training.
- The cost of achieving a given model performance is decreasing, but improving model performance remains expensive.
- The graph illustrates the relationship between training loss, model size, FLOPs, and the number of training tokens, showing how larger models require more computational resources but can achieve lower loss rates.
- Another graph projects the effective stock of tokens for various models over time, highlighting the rapid growth in dataset sizes and the potential for future bottlenecks.
- The chapter discusses the nuanced impact of AI-generated data on models and the competitive advantage of unique proprietary data in the AI race.
- Data restrictions from web sources have significantly increased, rendering over 28% of critical sources in the popular public dataset C4 fully restricted.
- Electricity consumption by data centers is a growing concern, estimated to consume 1–2% of global electricity, potentially reaching 4% to 20% by 2030.
- Post-training is introduced as a critical phase to align pre-trained models with human preferences, addressing issues like inappropriate outputs and optimizing for conversations.
- Supervised finetuning (SFT) and preference finetuning are key steps in post-training, with techniques like RLHF, DPO, and RLAIF highlighted for aligning models with human preferences.
- The difference between pre-training and post-training is likened to acquiring knowledge versus learning how to use that knowledge.
- The importance of high-quality labelers for generating demonstration data is emphasized, noting the cost and time involved in creating such datasets.
- The chapter explores the challenges of preference finetuning, including the ambitious goal of embedding universal human preference into AI models.
- RLHF's reliance on a reward model and the challenges of obtaining reliable comparison data are discussed, along with the cost and time of manual comparisons.
- The chapter concludes with insights into the practical steps and assessments for aligning AI models with human preferences, highlighting the complexity and cost involved.
- Important images include a diagram illustrating the RLHF framework and a humorous representation of RLHF as an octopus-like creature, aiding in understanding complex concepts.
- The chapter discusses the training process of foundation models like InstructGPT, focusing on how comparison data is used to train models to give concrete scores through an objective function that maximizes the difference in output scores for winning and losing responses.
- It explains the mathematical formula used by InstructGPT for training, detailing the components such as the reward model, training data format, and the sigmoid function.
- The importance of finetuning the reward model on top of the strongest foundation model is highlighted, along with the use of proximal policy optimization (PPO) for further training.
- The chapter explores sampling strategies in model outputs, including temperature, top-k, and top-p, to influence creativity and predictability in responses.
- It delves into the probabilistic nature of AI outputs, explaining how models compute probabilities for possible outcomes and the impact of different sampling strategies on these probabilities.
- The concept of logprobs (log probabilities) is introduced as a useful tool for building applications, evaluating them, and understanding model workings, despite some model providers limiting access to logprobs APIs for security reasons.
- Practical implementation opportunities include experimenting with temperature settings to balance creativity and predictability in model outputs, and using top-k sampling to reduce computational workload without sacrificing response diversity.
- Real-world applications mentioned include companies like Stitch Fix and Grab using the best of N strategy to improve model performance by selecting outputs with high scores from their reward models.
- The chapter provides insights into the evolving field of preference finetuning and the potential future where better pre-training data might eliminate the need for SFT and preference finetuning.
- Key images described include interfaces for generating comparison data, probability distributions over vocabulary, and workflow diagrams for computing logits, probabilities, and logprobs, aiding in understanding the practical aspects of foundation models.
- Top-p sampling, also known as nucleus sampling, dynamically selects values to sample from based on cumulative probability, allowing for more contextually appropriate outputs.
- Top-k sampling fixes the number of values considered for sampling, which can be less flexible compared to top-p sampling.
- Test time compute involves generating multiple responses to a query to increase the chance of a good response, though it comes with increased computational costs.
- Structured outputs are crucial for tasks requiring specific formats, such as semantic parsing, and for ensuring outputs can be parsed by downstream applications.
- Constrained sampling guides text generation towards certain constraints by filtering logits to only include tokens that meet specified conditions.
- The chapter discusses practical implementation opportunities like using application-specific heuristics to select the best response and overcoming latency challenges with parallel response generation.
- Interesting insights include the effectiveness of top-p sampling in practice and the significant performance boost from using verifiers, comparable to a 30× model size increase.
- Practical steps include using reward models to score outputs and employing post-processing to correct common mistakes in model outputs.
- Real-world applications include semantic parsing tasks like text-to-SQL and generating structured outputs for downstream applications.
- Additional key insights highlight the importance of model robustness and the benefits of sampling multiple outputs for tasks expecting exact answers.
- The chapter concludes with the importance of structured outputs in production and the various approaches to guide models in generating them, including prompting, post-processing, and constrained sampling.
- Important images include a bar chart on respondent sentiments towards foundation models and a graph illustrating the relationship between the number of completions per test problem and test solve rates.
- Constrained sampling is complex due to the need for specific grammars for each output format, increasing generation latency and limiting generalizability.
- Finetuning a model on examples of a desirable format is the most effective approach to ensure outputs match expected formats, though it doesn't guarantee 100% reliability.
- Modifying a model's architecture, such as adding a classifier head, can guarantee output formats for specific tasks like classification.
- The probabilistic nature of AI models leads to inconsistency and hallucinations, making them unpredictable in responses to the same or similar prompts.
- Inconsistency in AI models can be mitigated by caching answers, fixing sampling variables, and controlling hardware, but these measures don't ensure complete consistency.
- Hallucinations, where models generate unfounded facts, are a significant challenge, especially for tasks requiring factual accuracy.
- Two hypotheses explain hallucinations: self-delusion, where models cannot differentiate between given and generated data, and mismatched internal knowledge between models and labelers.
- Techniques to mitigate hallucinations include reinforcement learning to differentiate prompts and generated tokens, and supervised learning with factual and counterfactual signals in training data.
- Prompting techniques and asking models to base responses on known information can also help reduce hallucinations.
- The chapter highlights the importance of understanding and mitigating the probabilistic nature, inconsistency, and hallucinations in AI models for reliable applications.
- Important images include diagrams on adding a classifier head to a base model, examples of model inconsistency in scoring essays, and illustrations of hallucinations and self-delusion in models.
- The chapter explores the core design decisions in building foundation models, emphasizing the importance of training data quality and volume.
- Model development involves critical steps like architecting the model before training, with the transformer architecture being dominant for language-based models.
- The scale of a model is determined by parameters, training tokens, and FLOPs, with scaling laws helping optimize these factors within a compute budget.
- Post-training processes like supervised and preference finetuning address misalignments between model outputs and user expectations.
- Sampling introduces a probabilistic nature to models, enabling creativity but also leading to inconsistencies and hallucinations.
- Establishing a solid evaluation pipeline is crucial for systematic AI engineering to detect failures and unexpected changes.
- The chapter highlights the challenges of model hallucinations and the difficulty in detecting them, comparing it to detecting human lies.
- Real-world applications include the use of AI models for creative tasks and the importance of handling their probabilistic nature in workflows.
- Interesting insights include the argument by Ilya Sutskever on the difficulty of developing new neural network architectures that outperform existing ones.
- Practical steps involve curating training data for specific languages and domains, and the importance of evaluation pipelines in AI engineering.
- The chapter references studies and articles on AI model performance across languages and the impact of training data biases.
- Images and descriptions in the book provide visualizations of concepts like the chaotic probability distribution under higher temperature settings in sampling.
- Final takeaways include the necessity of building workflows around the probabilistic nature of AI models and the ongoing challenges in model alignment and consistency.

## code snippets
```
No direct code references found in the chapter.
```
