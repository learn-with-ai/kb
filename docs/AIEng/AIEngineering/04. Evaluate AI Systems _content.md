---
sidebar_position: 1
tags: ["AI evaluation", "model selection", "evaluation criteria", "factual consistency", "benchmarks",  "TruthfulQA", "RealToxicityPrompts", "BOLD", "Pareto optimization", "Model APIs", "Open source models", "Commercial models", "Inference service", "Open Source Models", "Proprietary Models", "Benchmarking", "API Cost", "Engineering Cost", "HELM", "MMLU", "Hugging Face", "toxicity classifier", "semantic similarity", "AI judge", "logprobs", "human evaluation"]
title: "04. Evaluate AI Systems "
---

## summary

- The chapter emphasizes the importance of evaluating AI models in the context of their intended applications, distinguishing between automatic evaluation approaches and practical application evaluation.
- It outlines three main parts: defining evaluation criteria, model selection amidst a plethora of foundation models, and developing an evaluation pipeline for application development.
- Evaluation-driven development is introduced as a methodology inspired by test-driven development, advocating for defining evaluation criteria before building AI applications.
- The chapter discusses the commonality of enterprise applications with clear evaluation criteria, such as recommender systems and fraud detection systems, highlighting the ease of evaluating classification tasks over open-ended ones.
- Domain-specific capabilities of models are explored, emphasizing the importance of evaluating these capabilities through public or private benchmarks, with a focus on coding and language understanding.
- The limitations of multiple-choice questions (MCQs) in evaluating foundation models are discussed, noting their suitability for knowledge and reasoning evaluation but not for generation capabilities.
- Generation capability evaluation is covered, detailing metrics like fluency, coherence, and factual consistency, and the evolution of these metrics with advancements in generative models.
- Factual consistency is broken down into local and global factual consistency, with methods for verification including AI as a judge, self-verification, and knowledge-augmented verification.
- The chapter concludes with a discussion on the challenges of determining facts in the presence of misinformation and the importance of designing benchmarks that focus on queries more likely to induce hallucinations.
- An image description is provided, detailing a flowchart for evaluating factual accuracy using Search-Augmented Factuality Evaluation (SAFE), illustrating the process of verifying individual facts against search engine results.
- The chapter discusses the importance of evaluating AI systems for factual consistency, safety, and instruction-following capability.
- Factual consistency is evaluated through entailment, contradiction, and neutral relationships, with specialized scorers trained for this purpose.
- DeBERTa-v3-base-mnli-fever-anli is highlighted as a model trained for factual consistency prediction.
- TruthfulQA is introduced as a benchmark for factual consistency, featuring questions designed to test AI's ability to avoid common human misconceptions.
- Safety concerns in AI outputs are categorized, including inappropriate language, harmful recommendations, hate speech, violence, stereotypes, and biases.
- The chapter mentions benchmarks like RealToxicityPrompts and BOLD for measuring toxicity in AI-generated texts.
- Instruction-following capability is crucial for AI models, with benchmarks like IFEval and INFOBench designed to evaluate this aspect.
- Roleplaying in AI is discussed as a common use case, with benchmarks like RoleLLM and CharacterEval for evaluating roleplaying capability.
- The chapter emphasizes the need for custom benchmarks to evaluate AI models based on specific application requirements.
- Important images include a performance chart of models on TruthfulQA and a classification matrix of AI systems' political and economic leanings.
- Balancing model quality, latency, and cost is crucial when evaluating AI systems, with many companies opting for models that offer better cost and latency even if they are of lower quality.
- Pareto optimization is introduced as a method for optimizing multiple objectives, emphasizing the importance of knowing which objectives cannot be compromised.
- Latency metrics for foundation models include time to first token, time per token, and time per query, with total latency influenced by the model's autoregressive nature and prompt design.
- Cost considerations differ between using model APIs, where cost is per token, and hosting models, where cost per token can decrease with scale.
- A table (Table 4-3) provides criteria for model selection, including cost, scale, latency, model quality, code generation capability, and factual consistency.
- Model selection involves evaluating models against application-specific criteria, with a workflow that includes filtering by hard attributes, public evaluation, private experiments, and continuous monitoring.
- The build versus buy decision is explored, discussing the pros and cons of using commercial model APIs versus hosting open source models, including considerations of data privacy, performance, and cost.
- Open source models' licenses and the implications for commercial use and model improvement are detailed, highlighting the complexity of navigating different licenses.
- The importance of data privacy and lineage in model selection is underscored, with examples of risks associated with using external model APIs.
- Performance benchmarks show the closing gap between open source and proprietary models, suggesting a trend towards more competitive open source options.
- The chapter concludes with the importance of designing a trustworthy evaluation pipeline due to the limitations of public benchmarks.
- Important images include a diagram of the evaluation workflow (Figure 4-5) and an inference service diagram (Figure 4-6), illustrating the process of model evaluation and the components of an inference service.
- The chapter discusses the ongoing debate between open source and proprietary AI models, highlighting the performance gap and the incentives for companies to keep their strongest models proprietary.
- Open source models may lag behind proprietary models due to lack of user feedback and the challenge of implementing necessary functionalities like scalability and function calling.
- Commercial model APIs offer ease of use and scalability but come with limitations such as restricted functionalities and potential over-censoring.
- Self-hosting models provides more control and customization but requires significant engineering effort and resources.
- The importance of model evaluation through benchmarks is emphasized, with tools like EleutherAI’s lm-evaluation-harness and OpenAI’s evals mentioned for assessing model capabilities.
- Public leaderboards like Hugging Face’s Open LLM Leaderboard and Stanford’s HELM Leaderboard are discussed as resources for comparing model performances across various benchmarks.
- The challenge of benchmark selection and aggregation is highlighted, noting the difficulty in covering all capabilities and the issue of benchmark correlation.
- The chapter concludes with the observation that as AI models improve, benchmarks become saturated, necessitating the development of new, more challenging benchmarks.
- The chart in Figure 4-7 shows the decreasing gap between open source and proprietary models on the MMLU benchmark, indicating progress in open source model performance.
- Figure 4-8 from a 2024 study by a16z illustrates the reasons enterprises care about open source models, with control and customizability being the top concerns.
- Table 4-4 summarizes the pros and cons of using model APIs versus self-hosting models, providing a clear comparison for decision-making.
- Table 4-5 presents the correlation scores among benchmarks used on Hugging Face’s leaderboard, offering insights into benchmark selection for model evaluation.
- The chapter discusses the importance of evaluating AI systems beyond public leaderboards, emphasizing the need for custom leaderboards tailored to specific applications.
- Public leaderboards may not always reflect a model's performance on specific tasks, highlighting the necessity of selecting relevant benchmarks for accurate evaluation.
- Data contamination is a common issue where models are trained on the same data they're evaluated on, leading to misleadingly high scores.
- Methods to detect and handle data contamination include n-gram overlapping and perplexity measurements, though each has its trade-offs in terms of accuracy and resource intensity.
- The chapter outlines steps to design an evaluation pipeline, including evaluating all components in a system, creating clear evaluation guidelines, and tying evaluation metrics to business metrics.
- Practical implementation opportunities include creating custom benchmarks for specific applications and designing evaluation pipelines that include both turn-based and task-based evaluations.
- The text mentions the high cost of running benchmarks, as exemplified by Stanford's expenditure on evaluating models on the HELM suite.
- Real-world applications discussed include coding agents, writing assistants, and customer support chatbots, with emphasis on the importance of defining what constitutes a 'good' response.
- The chapter highlights the challenge of balancing stickiness and engagement metrics with social responsibility in AI applications.
- Interesting insights include the satirical demonstration by Rylan Schaeffer on how training exclusively on benchmark data can lead to misleadingly high scores.
- The image descriptions provide context on evaluating AI systems' performance over time and the impact of clean versus dirty data on model evaluation.
- Different evaluation methods are required for different criteria in AI systems, such as toxicity detection, semantic similarity, and factual consistency.
- Mixing evaluation methods can balance cost and quality, using cheaper methods for bulk data and expensive ones for a subset to ensure confidence in the application.
- Logprobs are useful for measuring model confidence in predictions and can evaluate perplexity for generated text, aiding in fluency and factual consistency assessments.
- Automatic metrics are preferred, but human evaluation remains crucial, especially for open-ended responses, serving as a North Star metric for application development.
- Evaluation methods should be considered for both experimentation and production phases, with a focus on user feedback in production to improve the application.
- Annotated evaluation data is essential for assessing system components and criteria, with production data being ideal for creating a clear scoring rubric.
- Data slicing helps in understanding system performance across different subsets, avoiding biases, debugging, and identifying improvement areas.
- Simpson’s paradox is a phenomenon where aggregated data can misrepresent the performance of models on individual subsets, highlighting the need for careful analysis.
- Multiple evaluation sets representing different data slices are necessary to estimate overall system performance and handle specific scenarios like typos or out-of-scope inputs.
- The size of evaluation sets should ensure reliability without prohibitive costs, with bootstrapping methods used to assess the sufficiency of sample sizes.
- Evaluation pipelines must be reliable and efficient, with considerations for signal accuracy, reproducibility, metric correlation, and cost-effectiveness.
- Iteration on evaluation pipelines is necessary as needs and user behaviors evolve, with proper experiment tracking to ensure consistency and guide development.
- The chapter emphasizes the importance of a reliable evaluation pipeline for AI adoption, enabling risk reduction, performance improvement, and progress benchmarking.
- Public benchmarks and leaderboards can aid in model selection but may be contaminated and require careful consideration of selection and aggregation processes.
- Combining different evaluation methods and approaches can mitigate the limitations and biases inherent in assessing modern AI systems.
- The chapter concludes with the ongoing relevance of evaluation throughout the AI application development process, with subsequent chapters delving into specific evaluation aspects.
- Interesting insights include LinkedIn's process of manually evaluating up to 500 daily conversations with their AI systems and the rough estimation of evaluation samples needed for confidence in system improvements.
- Practical steps involve creating annotated evaluation data, slicing data for finer-grained analysis, and using bootstrapping to determine the reliability of evaluation sets.
- Real-world applications include using evaluation results to compare models, prompts, or components and to guide application development based on user feedback.
- Additional key insights highlight the challenges of evaluating open-ended responses and the trade-offs between different evaluation methods in terms of cost and quality.
- Final takeaways stress the iterative nature of evaluation pipeline development and the need for a balanced approach combining automatic and human evaluation methods.
- Important images or descriptions mentioned in the book include Table 4-6 illustrating Simpson’s paradox and Table 4-7 providing a rough estimation of evaluation samples needed for confidence in system improvements.

## code snippets
```
No direct code references found in the chapter.
```
