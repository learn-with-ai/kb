---
sidebar_position: 1
tags: ["Foundation Models", "Language Models", "Cross Entropy", "Perplexity", "Evaluation Benchmarks", "Cross entropy", "Functional correctness", "Similarity measurements", "Lexical similarity", "Semantic Similarity", "Embedding", "Cosine Similarity", "BERTScore", "MoverScore", "MLflow", "Ragas", "LlamaIndex", "GPT-4", "Claude-v1", "Comparative Evaluation", "Chatbot Arena", "Elo Rating Algorithm", "AI Judges", "Human Preference"]
title: "03. Evaluation Methodology"
---

## summary

- The chapter emphasizes the critical role of evaluation in mitigating risks and uncovering opportunities in AI applications, highlighting real-world failures due to inadequate AI evaluation.
- Evaluation is identified as a major hurdle in AI development, with significant effort required to figure out appropriate evaluation methods for different applications.
- The chapter introduces two main types of evaluation methods for open-ended models: exact and subjective evaluation, including the emerging approach of using AI as a judge.
- Challenges in evaluating foundation models are discussed, including their open-ended nature, the black-box problem, and the rapid saturation of evaluation benchmarks.
- The importance of language modeling metrics like cross entropy and perplexity is highlighted for evaluating foundation models, with a brief overview of their mathematical foundations.
- The chapter notes the exponential growth in research and tools dedicated to AI evaluation, yet points out the lag in investment compared to other areas of AI engineering.
- Practical steps for systematic evaluation are suggested, moving beyond ad hoc methods to more structured approaches for application iteration.
- Real-world applications of evaluation methodologies are implied through the discussion of mitigating AI risks and enhancing system robustness.
- The chapter concludes with a discussion on the expanded scope of evaluation for general-purpose models, including discovering new tasks beyond human capabilities.
- Images described include trends in LLM evaluation papers and growth in evaluation repositories, illustrating the increasing focus on evaluation in AI research.
- Cross entropy measures the efficiency of a language model in compressing text, with lower values indicating better performance.
- Perplexity, the exponential of entropy, measures a model's uncertainty in predicting the next token, with higher values indicating more uncertainty.
- Perplexity varies with data structure, vocabulary size, and context length, offering insights into model performance and data predictability.
- Functional correctness evaluates if a system performs its intended tasks correctly, crucial for applications like code generation and game bots.
- Similarity measurements against reference data, including exact match, lexical, and semantic similarity, assess the quality of generated outputs.
- Lexical similarity measures text overlap through token comparison, while semantic similarity assesses meaning closeness beyond lexical overlap.
- Exact match is suitable for tasks with short, exact responses, but lexical and semantic similarities are better for complex tasks.
- Perplexity can detect data contamination and abnormal texts, serving as a proxy for model performance on downstream tasks.
- Functional correctness in code generation is validated through execution accuracy, leveraging unit tests for automated evaluation.
- Reference-based evaluation is bottlenecked by the availability and quality of reference data, with AI increasingly used to generate such data.
- Images in educational content enhance comprehension by visualizing abstract concepts and providing contextual examples.
- The chapter emphasizes the importance of exact and subjective evaluation methods in assessing AI model performance.
- Semantic similarity is introduced as a method to compute the similarity in semantics by transforming text into numerical representations called embeddings.
- Embeddings are vectors that capture the meaning of the original data, with sizes typically between 100 and 10,000 elements.
- Models like BERT, CLIP, and Sentence Transformers are trained to produce embeddings, with proprietary models also available as APIs.
- The quality of embeddings is evaluated based on their ability to capture the essence of the original data and their utility in tasks like classification and recommender systems.
- Joint embeddings for different data modalities, such as text and images, are a new frontier, with models like CLIP and ULIP leading the way.
- AI as a judge is presented as a method to evaluate AI models, leveraging the speed and cost-effectiveness of AI to automate evaluation tasks.
- Studies show that AI judges can have high agreement with human evaluators, with GPT-4 reaching 85% agreement in some benchmarks.
- AI judges can evaluate responses based on various criteria and provide explanations for their decisions, offering flexibility in evaluation.
- The methodology for using AI as a judge includes evaluating responses by themselves, comparing them to reference responses, or comparing two generated responses.
- Limitations of AI as a judge include inconsistency in evaluation results and ambiguity in evaluation criteria, which can affect trust and reproducibility.
- Practical steps for using AI as a judge involve clear prompting, including task explanation, evaluation criteria, and scoring system.
- The chapter highlights the importance of embeddings in AI engineering, with applications in vector search and data deduplication.
- Interesting insights include the use of AI judges in production environments and their correlation with human evaluators.
- Real-world applications of semantic similarity and embeddings include text-based image search and evaluating AI model outputs.
- The chapter concludes with a discussion on the limitations of AI as a judge, including the probabilistic nature of AI and the potential for high costs.
- Important images mentioned include Figure 3-6, which visualizes CLIPâ€™s architecture, and Figure 3-8, showing an example of an AI judge evaluating an answer.
- Different tools like MLflow, Ragas, and LlamaIndex have unique default prompts for evaluating faithfulness, making their scores incomparable.
- AI judges' evolution over time can affect evaluation metrics, emphasizing the need for fixed evaluation methods to monitor application changes accurately.
- The importance of transparency in AI judges is highlighted, advising not to trust any AI judge without visibility into the model and prompt used.
- Using AI judges in production can increase costs and latency, with strategies like spot-checking and using weaker models to mitigate these issues.
- AI judges exhibit biases such as self-bias, first-position bias, and verbosity bias, which can affect evaluation outcomes.
- Specialized judges like reward models, reference-based judges, and preference models offer targeted evaluation capabilities, with examples like Cappy and BLEURT.
- Comparative evaluation is introduced as a method to rank models by comparing them directly, offering advantages in subjective quality assessments.
- The challenges of preference-based voting are discussed, including the potential for misalignment and user frustration in certain contexts.
- Rating algorithms like Elo, Bradleyâ€“Terry, and TrueSkill are adapted from other domains to compute model rankings from comparative evaluations.
- The chapter concludes with the insight that model ranking is a predictive problem, aiming to forecast future match outcomes based on historical data.
- Important images include diagrams comparing AI systems like PandaLM and LLamaMA, and screenshots from interactive platforms demonstrating practical applications of evaluation methodologies.
- Comparative evaluation is a method used to rank AI models based on their performance in predicting future match outcomes, without a ground truth for correct rankings.
- The quality of a ranking in comparative evaluation is determined by its predictive accuracy for future model match outcomes, with analysis available in the book's GitHub repo.
- Challenges of comparative evaluation include scalability bottlenecks due to the quadratic growth of model pairs and the assumption of transitivity in ranking algorithms.
- Lack of standardization and quality control in crowdsourced comparative signals can lead to preference inconsistencies and difficulty in enforcing evaluation standards.
- Practical implementation opportunities include developing better matching algorithms to reduce uncertainty in rankings and incorporating comparative evaluation into user workflows.
- Comparative evaluation's future is promising due to its ability to capture human preference and provide discriminating signals about models, despite its limitations.
- The chapter emphasizes the importance of evaluation as AI models become more powerful, highlighting the challenges and methodologies in automatic evaluation.
- Key insights include the difficulty in evaluating new and private models, the impact of non-transitivity in human preferences, and the potential of AI judges.
- Real-world applications involve using comparative evaluation alongside exact and human evaluation to build reliable evaluation pipelines for open-ended AI applications.
- Important images or descriptions mentioned in the book include analysis of Chatbot Arena's ranking and the discussion on the Elo rating algorithm's limitations.

## code snippets
```
from typing import List
def has_close_elements(numbers: List[float], threshold: float) -> bool:
      """ Check if in given list of numbers, are any two numbers closer to each 
      other than given threshold.
      >>> has_close_elements([1.0, 2.0, 3.0], 0.5) False
      >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True 
      """
def check(candidate):
      assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True
      assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False
      assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True
      assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False
      assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True
      assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True
      assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False
No direct code references found in the chapter.
```
