<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-AIEng/PromptEng/Vector Databases with FAISS and Pinecone_content" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">05. Vector Databases with FAISS and Pinecone | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://your-docusaurus-site.example.com/docs/AIEng/PromptEng/Vector Databases with FAISS and Pinecone_content"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="05. Vector Databases with FAISS and Pinecone | My Site"><meta data-rh="true" name="description" content="titles"><meta data-rh="true" property="og:description" content="titles"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://your-docusaurus-site.example.com/docs/AIEng/PromptEng/Vector Databases with FAISS and Pinecone_content"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/AIEng/PromptEng/Vector Databases with FAISS and Pinecone_content" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/AIEng/PromptEng/Vector Databases with FAISS and Pinecone_content" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="My Site Atom Feed"><link rel="stylesheet" href="/assets/css/styles.1724a0b2.css">
<script src="/assets/js/runtime~main.2d8c9eb9.js" defer="defer"></script>
<script src="/assets/js/main.ed51d313.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">My Site</b></a><a class="navbar__item navbar__link" href="/docs/SE/ModelDriven/Part1_">Tutorial</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/AIEng/AIDevops/Introduction_content">AI Engineering Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/tags">Tags</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/AIDevops/Introduction_content">AIDevops</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/AIEngineering/Introduction to Building AI Applications with Foundation Models_content">AIEngineering</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/LangChain/00Preface_content">LangChain</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/AIEng/PromptEng/The Five Principles of Prompting_content">PromptEng</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/PromptEng/The Five Principles of Prompting_content">01. The Five Principles of Prompting</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/PromptEng/Introduction to Large Language Models for Text Generation_content">02. Introduction to Large Language Models for Text Generation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/PromptEng/Standard Practices for Text Generation with ChatGPT_content">03. Standard Practices for Text Generation with ChatGPT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/PromptEng/Advanced Techniques for Text Generation with LangChain_content">04. Advanced Techniques for Text Generation with LangChain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/AIEng/PromptEng/Vector Databases with FAISS and Pinecone_content">05. Vector Databases with FAISS and Pinecone</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/PromptEng/Autonomous Agents with Memory and Tools_content">06. Autonomous Agents with Memory and Tools</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/PromptEng/Introduction to Diffusion Models for Image Generation_content">07. Introduction to Diffusion Models for Image Generation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/PromptEng/Standard Practices for Image Generation with Midjourney_content">08. Standard Practices for Image Generation with Midjourney</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/PromptEng/Advanced Techniques for Image Generation with Stable Diffusion_content">09. Advanced Techniques for Image Generation with Stable Diffusion</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/PromptEng/Building AI-Powered Applications_content">10. Building AI-Powered Applications</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/ScalableAI/Introduction to Scalable AI Systems_content">ScalableAI</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PromptEng</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">05. Vector Databases with FAISS and Pinecone</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>05. Vector Databases with FAISS and Pinecone</h1></header><h2 class="anchor anchorWithStickyNavbar_LWe7" id="titles">titles<a href="#titles" class="hash-link" aria-label="Direct link to titles" title="Direct link to titles">​</a></h2>
<p>Vector Databases with FAISS and Pinecone Advanced Vector Database Techniques with FAISS and Pinecone Advanced Vector Database Operations with FAISS and Pinecone Advanced Vector Database Techniques and Self-Querying Retrievers</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="summary">summary<a href="#summary" class="hash-link" aria-label="Direct link to summary" title="Direct link to summary">​</a></h2>
<ul>
<li>This chapter introduces embeddings and vector databases, explaining their use in providing relevant context in prompts to decrease AI hallucinations.</li>
<li>Vector databases store text data in a way that enables querying based on similarity or semantic meaning, improving the accuracy of LLM responses.</li>
<li>Vectors represent text or images as lists of numbers, with models like OpenAI&#x27;s text-embedding-ada-002 using 1,536 numbers to represent each word.</li>
<li>The concept of vector distances is illustrated with a simplified two-dimensional example, showing how related texts are closer in vector space.</li>
<li>Vector search allows querying based on similarity, returning closely associated records even without exact matches, useful for document reading, recommendation systems, and long-term memory in chatbots.</li>
<li>Retrieval Augmented Generation (RAG) is highlighted as a method to dynamically insert relevant knowledge into prompts, using vector databases to find and retrieve the most relevant documents.</li>
<li>The chapter details the process of generating embeddings using OpenAI&#x27;s API and Hugging Face&#x27;s Sentence Transformers, including code examples for both.</li>
<li>Differences between dense and sparse vectors are explained, with dense vectors being more common in AI applications for capturing semantic information.</li>
<li>The importance of the embedding model&#x27;s accuracy is emphasized, noting that biases or knowledge gaps in the model affect vector search results.</li>
<li>Training custom embedding models is discussed for cases requiring domain-specific vocabulary or recent context not covered by general models.</li>
<li>Figure 5-1 and Figure 5-2 visually represent vector distances in two-dimensional and multidimensional spaces, respectively, aiding in understanding semantic relationships.</li>
<li>The chapter continues with advanced techniques in vector databases, focusing on training custom embedding models using Gensim&#x27;s Word2Vec and comparing it with TF-IDF for similarity calculations.</li>
<li>Word2Vec model training is demonstrated with a small, repetitive dataset to illustrate how vectors can represent semantic relationships between words, though it&#x27;s noted that larger, more diverse corpora are typically needed for effective learning.</li>
<li>TF-IDF is introduced as a simpler, more robust alternative for smaller document sizes, with a code example showing how to compute cosine similarity between words using scikit-learn.</li>
<li>The importance of document chunking strategies is discussed, highlighting the trade-offs between context retention and specificity in vector searches, with an example using LangChain&#x27;s RecursiveCharacterTextSplitter.</li>
<li>Memory retrieval with FAISS is detailed, including how to create an index for efficient similarity search and how to perform vector searches to find the most relevant document chunks for a given query.</li>
<li>A practical example demonstrates integrating FAISS vector search with a chatbot to provide contextually relevant answers, showcasing the Retrieval Augmented Generation (RAG) technique to mitigate hallucinations.</li>
<li>The chapter emphasizes the significance of the embedding model&#x27;s accuracy and the strategy for embedding text chunks, noting the balance between context and similarity in vector searches.</li>
<li>Code examples are provided for training a Word2Vec model, computing TF-IDF similarities, splitting documents into chunks, and performing vector searches with FAISS.</li>
<li>The process of integrating search results into prompts for AI models is explained, with a focus on setting system messages to guide the model&#x27;s responses based on the provided context.</li>
<li>Practical considerations for working with vector databases, such as the cost and latency of recomputing vectors and the importance of not changing the embedding model once vectors are stored, are discussed.</li>
<li>The chapter delves into advanced operations with vector databases, focusing on FAISS and Pinecone, including saving and loading indices, merging indices, and using hosted vector databases for enhanced scalability and reliability.</li>
<li>A detailed explanation of the <code>vector_search</code> function showcases how to perform vector searches, integrate search results into prompts for AI models, and ensure responses are contextually relevant.</li>
<li>The advantages of hosted vector databases like Pinecone are highlighted, including maintenance, scalability, reliability, performance, support, and security benefits over self-managed solutions.</li>
<li>Practical steps for creating and managing a Pinecone index are provided, including setting up the API key, defining the index name and environment, and checking for existing indices.</li>
<li>The process of storing vectors in a Pinecone index is explained, including batch processing, handling retries for rate limits, and upserting records with metadata.</li>
<li>Querying the Pinecone index for similar vectors is demonstrated, showing how to retrieve the most relevant documents based on a user query and include metadata in the results.</li>
<li>The chapter emphasizes the importance of document chunking strategies for effective vector searches and the trade-offs between context retention and specificity.</li>
<li>LangChain&#x27;s implementation of RAG is praised for its efficiency and readability, enabling rapid development of retrieval-augmented generation applications.</li>
<li>Security and privacy considerations when using hosted vector databases are discussed, including the risks of vendor lock-in and data sharing with third parties.</li>
<li>The chapter concludes with practical examples of querying the Pinecone index, demonstrating how to retrieve and utilize the most similar vectors for generating contextually relevant AI responses.</li>
<li>The chapter explores advanced operations with vector databases, including emulating FAISS jobs with Pinecone, utilizing metadata for filtering queries, and the importance of metadata strategy alongside chunking strategy.</li>
<li>Self-querying retrievers are introduced, highlighting their benefits such as schema definition and dual-layer retrieval, which enhance precision and relevance in document retrieval.</li>
<li>A practical example demonstrates setting up a self-querying retriever with LangChain, including defining metadata attributes for books and querying based on specific criteria like genre or author.</li>
<li>Different retrieval mechanisms are compared, including MultiQueryRetriever, Contextual Compression, Ensemble Retriever, Parent Document Retriever, and Time-Weighted Vector Store Retriever, each with unique advantages and limitations.</li>
<li>The chapter concludes with a summary of vector databases&#x27; power for similarity-based text querying, the RAG process, and the cost-benefit analysis of retrieving embeddings from OpenAI versus open-source models.</li>
<li>A preview of the next chapter on autonomous agents is provided, teasing the exploration of AI agents&#x27; decision-making capabilities and the challenges they face.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="code-snippets">code snippets<a href="#code-snippets" class="hash-link" aria-label="Direct link to code snippets" title="Direct link to code snippets">​</a></h2>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">from openai import OpenAI</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">client = OpenAI()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Function to get the vector embedding for a given text</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def get_vector_embeddings(text):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    response = client.embeddings.create(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        input=text,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        model=&quot;text-embedding-ada-002&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    embeddings = [r.embedding for r in response.data]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return embeddings[0]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">get_vector_embeddings(&quot;Your text string goes here&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import requests</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import os</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model_id = &quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">hf_token = os.getenv(&quot;HF_TOKEN&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">api_url = &quot;https://api-inference.huggingface.co/&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">api_url += f&quot;pipeline/feature-extraction/{model_id}&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">headers = {&quot;Authorization&quot;: f&quot;Bearer {hf_token}&quot;}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def query(texts):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    response = requests.post(api_url, headers=headers,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    json={&quot;inputs&quot;: texts,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;options&quot;:{&quot;wait_for_model&quot;:True}})</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return response.json()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">texts = [&quot;mickey mouse&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;cheese&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;trap&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;rat&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;ratatouille&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;bus&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;airplane&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;ship&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">output = query(texts)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">output</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from gensim.models import Word2Vec</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Sample data: list of sentences, where each sentence is</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># a list of words.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># In a real-world scenario, you&#x27;d load and preprocess your</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># own corpus.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sentences = [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [&quot;the&quot;, &quot;cake&quot;, &quot;is&quot;, &quot;a&quot;, &quot;lie&quot;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from gensim.models import Word2Vec</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Sample data: list of sentences, where each sentence is</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># a list of words.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># In a real-world scenario, you&#x27;d load and preprocess your</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># own corpus.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sentences = [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [&quot;the&quot;, &quot;cake&quot;, &quot;is&quot;, &quot;a&quot;, &quot;lie&quot;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [&quot;if&quot;, &quot;you&quot;, &quot;hear&quot;, &quot;a&quot;, &quot;turret&quot;, &quot;sing&quot;, &quot;you&#x27;re&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;probably&quot;, &quot;too&quot;, &quot;close&quot;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [&quot;why&quot;, &quot;search&quot;, &quot;for&quot;, &quot;the&quot;, &quot;end&quot;, &quot;of&quot;, &quot;a&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;rainbow&quot;, &quot;when&quot;, &quot;the&quot;, &quot;cake&quot;, &quot;is&quot;, &quot;a&quot;, &quot;lie?&quot;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [&quot;there&#x27;s&quot;, &quot;no&quot;, &quot;cake&quot;, &quot;in&quot;, &quot;space,&quot;, &quot;just&quot;, &quot;ask&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;wheatley&quot;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [&quot;completing&quot;, &quot;tests&quot;, &quot;for&quot;, &quot;cake&quot;, &quot;is&quot;, &quot;the&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;sweetest&quot;, &quot;lie&quot;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [&quot;I&quot;, &quot;swapped&quot;, &quot;the&quot;, &quot;cake&quot;, &quot;recipe&quot;, &quot;with&quot;, &quot;a&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;neurotoxin&quot;, &quot;formula,&quot;, &quot;hope&quot;, &quot;that&#x27;s&quot;, &quot;fine&quot;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">] + [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [&quot;the&quot;, &quot;cake&quot;, &quot;is&quot;, &quot;a&quot;, &quot;lie&quot;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [&quot;the&quot;, &quot;cake&quot;, &quot;is&quot;, &quot;definitely&quot;, &quot;a&quot;, &quot;lie&quot;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [&quot;everyone&quot;, &quot;knows&quot;, &quot;that&quot;, &quot;cake&quot;, &quot;equals&quot;, &quot;lie&quot;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">] * 10  # repeat several times to emphasize</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Train the word2vec model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model = Word2Vec(sentences, vector_size=100, window=5,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">min_count=1, workers=4, seed=36)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Save the model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model.save(&quot;custom_word2vec_model.model&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># To load the model later</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># loaded_model = word2vec.load(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># &quot;custom_word2vec_model.model&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Get vector for a word</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vector = model.wv[&#x27;cake&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Find most similar words</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">similar_words = model.wv.most_similar(&quot;cake&quot;, topn=5)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">print(&quot;Top five most similar words to &#x27;cake&#x27;: &quot;, similar_words)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Directly query the similarity between &quot;cake&quot; and &quot;lie&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cake_lie_similarity = model.wv.similarity(&quot;cake&quot;, &quot;lie&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">print(&quot;Similarity between &#x27;cake&#x27; and &#x27;lie&#x27;: &quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cake_lie_similarity)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import numpy as np</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from sklearn.feature_extraction.text import TfidfVectorizer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from sklearn.metrics.pairwise import cosine_similarity</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Convert sentences to a list of strings for TfidfVectorizer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">document_list = [&#x27; &#x27;.join(s) for s in sentences]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Compute TF-IDF representation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vectorizer = TfidfVectorizer()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tfidf_matrix = vectorizer.fit_transform(document_list)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Extract the position of the words &quot;cake&quot; and &quot;lie&quot; in</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># the feature matrix</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cake_idx = vectorizer.vocabulary_[&#x27;cake&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">lie_idx = vectorizer.vocabulary_[&#x27;lie&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Extract and reshape the vector for &#x27;cake&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cakevec = tfidf_matrix[:, cake_idx].toarray().reshape(1, -1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Compute the cosine similarities</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">similar_words = cosine_similarity(cakevec, tfidf_matrix.T).flatten()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Get the indices of the top 6 most similar words</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># (including &#x27;cake&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">top_indices = np.argsort(similar_words)[-6:-1][::-1]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Retrieve and print the top 5 most similar words to</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># &#x27;cake&#x27; (excluding &#x27;cake&#x27; itself)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">names = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for idx in top_indices:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    names.append(vectorizer.get_feature_names_out()[idx])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">print(&quot;Top five most similar words to &#x27;cake&#x27;: &quot;, names)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Compute cosine similarity between &quot;cake&quot; and &quot;lie&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">similarity = cosine_similarity(np.asarray(tfidf_matrix[:,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cake_idx].todense()), np.asarray(tfidf_matrix[:, lie_idx].todense())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># The result will be a matrix; we can take the average or</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># max similarity value</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">avg_similarity = similarity.mean()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">print(&quot;Similarity between &#x27;cake&#x27; and &#x27;lie&#x27;&quot;, avg_similarity)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Show the similarity between &quot;cake&quot; and &quot;elephant&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">elephant_idx = vectorizer.vocabulary_[&#x27;sing&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">similarity = cosine_similarity(np.asarray(tfidf_matrix[:,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cake_idx].todense()), np.asarray(tfidf_matrix[:,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    elephant_idx].todense()))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">avg_similarity = similarity.mean()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">print(&quot;Similarity between &#x27;cake&#x27; and &#x27;sing&#x27;&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    avg_similarity)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.text_splitter import RecursiveCharacterTextSplitter</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    chunk_size=100, # 100 tokens</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    chunk_overlap=20, # 20 tokens of overlap</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">text = &quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Welcome to the &quot;Unicorn Enterprises: Where Magic Happens&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Employee Handbook! We&#x27;re thrilled to have you join our team</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">of dreamers, doers, and unicorn enthusiasts. At Unicorn</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Enterprises, we believe that work should be as enchanting as</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">it is productive. This handbook is your ticket to the</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">magical world of our company, where we&#x27;ll outline the</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">principles, policies, and practices that guide us on this</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">extraordinary journey. So, fasten your seatbelts and get</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ready to embark on an adventure like no other!</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">As we conclude this handbook, remember that at Unicorn</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Enterprises, the pursuit of excellence is a never-ending</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">quest. Our company&#x27;s success depends on your passion,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">creativity, and commitment to making the impossible</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">possible. We encourage you to always embrace the magic</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">within and outside of work, and to share your ideas and</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">innovations to keep our enchanted journey going. Thank you</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for being a part of our mystical family, and together, we&#x27;ll</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">continue to create a world where magic and business thrive</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">hand in hand!</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">chunks = text_splitter.split_text(text=text)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">print(chunks[0:3])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import numpy as np</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import faiss</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">#  The get_vector_embeddings function is defined in a preceding example</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">emb = [get_vector_embeddings(chunk) for chunk in chunks]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vectors = np.array(emb)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Create a FAISS index</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">index = faiss.IndexFlatL2(vectors.shape[1])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">index.add(vectors)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Function to perform a vector search</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def vector_search(query_text, k=1):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    query_vector = get_vector_embeddings(query_text)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    distances, indices = index.search(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        np.array([query_vector]), k)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return [(chunks[i], float(dist)) for dist,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        i in zip(distances[0], indices[0])]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Example search</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">user_query = &quot;do we get free unicorn rides?&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">search_results = vector_search(user_query)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">print(f&quot;Search results for {user_query}:&quot;, search_results)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Save the index to a file</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">faiss.write_index(index, &quot;data/my_index_file.index&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Load the index from a file</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">index = faiss.read_index(&quot;data/my_index_file.index&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Assuming index1 and index2 are two IndexFlatL2 indices</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">index1.add(index2.reconstruct_n(0, index2.ntotal))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_community.vectorstores.faiss import FAISS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_core.output_parsers import StrOutputParser</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_core.prompts import ChatPromptTemplate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_core.runnables import RunnablePassthrough</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_openai import ChatOpenAI, OpenAIEmbeddings</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 1. Create the documents:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">documents = [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;James Phoenix worked at JustUnderstandingData.&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;James Phoenix currently is 31 years old.&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;&quot;&quot;Data engineering is the designing and building systems for collect</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    storing, and analyzing data at scale.&quot;&quot;&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 2. Create a vectorstore:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vectorstore = FAISS.from_texts(texts=documents, embedding=OpenAIEmbedding</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">retriever = vectorstore.as_retriever()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 3. Create a prompt:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">template = &quot;&quot;&quot;Answer the question based only on the following context:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Context: {context}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">---</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Question: {question}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">prompt = ChatPromptTemplate.from_template(template)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 4. Create a chat model:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model = ChatOpenAI()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">chain = (</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    {&quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough()}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    | prompt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    | model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    | StrOutputParser()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">chain.invoke(&quot;What is data engineering?&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from pinecone import Pinecone, ServerlessSpec</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import os</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Initialize connection (get API key at app.pinecone.io):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os.environ[&quot;PINECONE_API_KEY&quot;] = &quot;insert-your-api-key-here&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">index_name = &quot;employee-handbook&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">environment = &quot;us-west-2&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pc = Pinecone()  # This reads the PINECONE_API_KEY env var</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Check if index already exists:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># (it shouldn&#x27;t if this is first time)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">if index_name not in pc.list_indexes().names():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # if does not exist, create index</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pc.create_index(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        index_name,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Using the same vector dimensions as text-embedding-ada-002</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        dimension=1536,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        metric=&quot;cosine&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        spec=ServerlessSpec(cloud=&quot;aws&quot;, region=environment),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Connect to index:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">index = pc.Index(index_name)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># View index stats:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">index.describe_index_stats()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from tqdm import tqdm # For printing a progress bar</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from time import sleep</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># How many embeddings you create and insert at once</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">batch_size = 10</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">retry_limit = 5  # maximum number of retries</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for i in tqdm(range(0, len(chunks), batch_size)):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Find end of batch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    i_end = min(len(chunks), i+batch_size)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    meta_batch = chunks[i:i_end]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Get ids</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ids_batch = [str(j) for j in range(i, i_end)]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Get texts to encode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    texts = [x for x in meta_batch]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Create embeddings</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # (try-except added to avoid RateLimitError)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    done = False</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    try:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Retrieve embeddings for the whole batch at once</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        embeds = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for text in texts:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            embedding = get_vector_embeddings(text)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            embeds.append(embedding)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        done = True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    except:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        retry_count = 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        while not done and retry_count &lt; retry_limit:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            try:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                for text in texts:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    embedding = get_vector_embeddings(text)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    embeds.append(embedding)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                done = True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            except:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                sleep(5)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                retry_count += 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    if not done:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        print(f&quot;&quot;&quot;Failed to get embeddings after</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        {retry_limit} retries.&quot;&quot;&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        continue</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Cleanup metadata</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    meta_batch = [{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &#x27;batch&#x27;: i,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &#x27;text&#x27;: x</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    } for x in meta_batch]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    to_upsert = list(zip(ids_batch, embeds, meta_batch))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Upsert to Pinecone</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    index.upsert(vectors=to_upsert)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Retrieve from Pinecone</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">user_query = &quot;do we get free unicorn rides?&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def pinecone_vector_search(user_query, k):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    xq = get_vector_embeddings(user_query)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    res = index.query(vector=xq, top_k=k, include_metadata=True)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return res</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pinecone_vector_search(user_query, k=1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">res = index.query(xq, filter={</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;batch&quot;: {&quot;$eq&quot;: 1}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }, top_k=1, include_metadata=True)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_core.documents import Document</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_community.vectorstores.chroma import Chroma</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_openai import OpenAIEmbeddings</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import lark</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import getpass</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import os</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import warnings</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Disabling warnings:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">warnings.filterwarnings(&quot;ignore&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_openai.chat_models import ChatOpenAI</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.retrievers.self_query.base \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    import SelfQueryRetriever</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.chains.query_constructor.base \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    import AttributeInfo</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Create the embeddings and vectorstore:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">embeddings = OpenAIEmbeddings()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">document_content_description = &quot;Brief summary of a movie&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llm = ChatOpenAI(temperature=0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">retriever = SelfQueryRetriever.from_llm(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    llm, vectorstore, document_content_description, metadata_field_info</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">retriever = SelfQueryRetriever.from_llm(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    llm,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    vectorstore,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    document_content_description,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    metadata_field_info,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    enable_limit=True,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">retriever.get_relevant_documents(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    query=&quot;Return 2 Fantasy books&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-tags-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vector-databases">Vector Databases</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/embeddings">Embeddings</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/faiss">FAISS</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/pinecone">Pinecone</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/rag">RAG</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/word-2-vec">Word2Vec</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/tf-idf">TF-IDF</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/document-loading">Document Loading</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vector-search">Vector Search</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/document-chunking">Document Chunking</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/self-querying">Self-Querying</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/AIEng/PromptEng/Advanced Techniques for Text Generation with LangChain_content"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">04. Advanced Techniques for Text Generation with LangChain</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/AIEng/PromptEng/Autonomous Agents with Memory and Tools_content"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">06. Autonomous Agents with Memory and Tools</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#titles" class="table-of-contents__link toc-highlight">titles</a></li><li><a href="#summary" class="table-of-contents__link toc-highlight">summary</a></li><li><a href="#code-snippets" class="table-of-contents__link toc-highlight">code snippets</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/learn-with-ai/kb" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>