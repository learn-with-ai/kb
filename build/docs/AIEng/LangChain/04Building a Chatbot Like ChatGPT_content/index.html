<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-AIEng/LangChain/04Building a Chatbot Like ChatGPT_content" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">04Building a Chatbot Like ChatGPT | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://your-docusaurus-site.example.com/docs/AIEng/LangChain/04Building a Chatbot Like ChatGPT_content"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="04Building a Chatbot Like ChatGPT | My Site"><meta data-rh="true" name="description" content="summary"><meta data-rh="true" property="og:description" content="summary"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://your-docusaurus-site.example.com/docs/AIEng/LangChain/04Building a Chatbot Like ChatGPT_content"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/AIEng/LangChain/04Building a Chatbot Like ChatGPT_content" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/AIEng/LangChain/04Building a Chatbot Like ChatGPT_content" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="My Site Atom Feed"><link rel="stylesheet" href="/assets/css/styles.1724a0b2.css">
<script src="/assets/js/runtime~main.2d8c9eb9.js" defer="defer"></script>
<script src="/assets/js/main.ed51d313.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">My Site</b></a><a class="navbar__item navbar__link" href="/docs/SE/ModelDriven/Part1_">Tutorial</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/AIEng/AIDevops/Introduction_content">AI Engineering Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/tags">Tags</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/AIDevops/Introduction_content">AIDevops</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/AIEngineering/Introduction to Building AI Applications with Foundation Models_content">AIEngineering</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/AIEng/LangChain/00Preface_content">LangChain</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/LangChain/00Preface_content">00Preface</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/LangChain/01What Is Generative AI_content">01What Is Generative AI</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/LangChain/02LangChain for LLM Apps_content">02LangChain for LLM Apps</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/LangChain/03Building Capable Assistants_content">03Building Capable Assistants</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/LangChain/03Getting Started with LangChain_content">03Getting Started with LangChain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/AIEng/LangChain/04Building a Chatbot Like ChatGPT_content">04Building a Chatbot Like ChatGPT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/LangChain/05Developing Software with Generative AI_content">05Developing Software with Generative AI</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/LangChain/06LLMs for Data Science_content">06LLMs for Data Science</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/LangChain/07Customizing LLMs and Their Output_content">07Customizing LLMs and Their Output</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/LangChain/08Generative AI in Production_content">08Generative AI in Production</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/LangChain/09The Future of Generative Models_content">09The Future of Generative Models</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/PromptEng/The Five Principles of Prompting_content">PromptEng</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/ScalableAI/Introduction to Scalable AI Systems_content">ScalableAI</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">LangChain</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">04Building a Chatbot Like ChatGPT</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>04Building a Chatbot Like ChatGPT</h1></header><h2 class="anchor anchorWithStickyNavbar_LWe7" id="summary">summary<a href="#summary" class="hash-link" aria-label="Direct link to summary" title="Direct link to summary">​</a></h2>
<ul>
<li>LLM-powered chatbots excel in conversational tasks but face limitations in domain-specific question answering due to lack of world knowledge and occasional errors.</li>
<li>Retrieval-Augmented Generation (RAG) enhances chatbots by grounding responses in external evidence sources, improving accuracy and informativeness.</li>
<li>The chapter covers representing documents as vectors, indexing methods for similarity lookups, and using vector databases like Milvus and Pinecone for managing embeddings.</li>
<li>Memory in LangChain is crucial for managing conversation history, tracking entities, and summarizing conversations, enabling context-aware conversational agents.</li>
<li>Moderation is discussed from a reputational and legal perspective, with LangChain allowing text to be checked for harmful content.</li>
<li>Chatbots mimic human conversation, with modern versions leveraging advanced AI like LLMs, though achieving truly human-like interaction remains challenging.</li>
<li>Use cases for chatbots span customer service, appointment scheduling, information retrieval, virtual assistants, language learning, mental health support, education, HR, entertainment, law, and medicine.</li>
<li>Proactive chatbots anticipate user needs based on prior interactions, aiming for more natural human-AI interaction, though mastering context and reasoning is still a challenge.</li>
<li>RAG combines LLMs with external knowledge sources, improving accuracy by grounding responses in real-world data through semantic search and vector embeddings.</li>
<li>Vector embeddings represent content numerically, enabling machines to process and understand semantic similarities between concepts.</li>
<li>LangChain provides a standard interface for working with text embedding models, supporting various providers like OpenAI, Cohere, and Hugging Face.</li>
<li>Vector storage and indexing are essential for efficient similarity searches, with vector databases like Milvus and Pinecone offering advantages over standalone solutions.</li>
<li>The chapter includes practical examples of obtaining embeddings and performing semantic search, highlighting the importance of vector arithmetic and distance metrics.</li>
<li>Interesting insights include the ability of embeddings to perform tasks like sentiment classification and the dynamic nature of RAG in improving LLM responses.</li>
<li>Practical steps involve setting up embeddings with different models and providers, and understanding the fundamentals of vector indexing and storage for RAG applications.</li>
<li>Real-world applications of RAG and vector embeddings include improving chatbot responses in healthcare by retrieving relevant medical information.</li>
<li>Additional key insights cover the evolution of chatbot technology and the balance between proactive dialog and responsiveness to user intent.</li>
<li>The final takeaway emphasizes the importance of grounding LLMs with external data through RAG for more accurate and contextually relevant responses.</li>
<li>Important images include a 3D scatter plot visualizing word embeddings and a confusion matrix for different categories, illustrating semantic relationships and classification performance.</li>
<li>Vector indexing organizes vectors to optimize retrieval, with algorithms like k-d trees, Annoy, and Faiss for efficient similarity searches.</li>
<li>Product quantization (PQ) and Locality Sensitive Hashing (LSH) are techniques for reducing dimensionality and enabling fast search, with trade-offs in accuracy.</li>
<li>Hierarchical Navigable Small World (HNSW) is a graph-based indexing algorithm known for high search accuracy and scalability.</li>
<li>Vector libraries like Faiss and Annoy provide functionality for working with vector data, offering efficient similarity search capabilities.</li>
<li>Vector databases are designed for handling vector embeddings, offering features like data management, metadata storage, and scalability.</li>
<li>Applications of vector databases include anomaly detection, personalization, and NLP tasks, leveraging their efficiency in high-dimensional spaces.</li>
<li>Popular vector databases include Chroma, Qdrant, Milvus, Weaviate, Pinecone, Vespa, and Marqo, each with unique features and business models.</li>
<li>LangChain&#x27;s vectorstores module facilitates the implementation of vector storage, with examples using Chroma for storing and querying vectors.</li>
<li>The process of building a chatbot with RAG involves loading documents, transforming them, embedding into vector stores, and querying for similar vectors.</li>
<li>LangChain simplifies document loading from various sources and formats, integrating with vector stores and retrievers for efficient RAG systems.</li>
<li>Important images include a star history graph for open-source vector libraries and databases, and a diagram illustrating the data connection process for chatbots.</li>
<li>Document loaders in LangChain are used to load data from various sources as Document objects, which include text and associated metadata, supporting formats like .txt files, web pages, arXiv articles, and YouTube transcripts.</li>
<li>LangChain&#x27;s TextLoader simplifies loading text data from files, while other loaders like WikipediaLoader and ArxivLoader provide specialized functionalities for loading documents from Wikipedia and arXiv, respectively.</li>
<li>Retrievers in LangChain fetch documents based on user queries, supporting various search methods including web indexes, databases, and specialized knowledge graphs, with examples like the kNN retriever and PubMed retriever.</li>
<li>Custom retrievers can be implemented in LangChain by inheriting from the BaseRetriever abstract class and implementing the get_relevant_documents method to perform custom retrieval operations.</li>
<li>Implementing a chatbot with a retriever involves setting up a document loader, storing documents in a vector store, and configuring a chatbot with retrieval capabilities, with Streamlit providing a web interface for interaction.</li>
<li>Vector storage in LangChain involves document loading, splitting, embedding, and storing embeddings in a vector database for efficient search, with techniques like similarity search and maximal marginal relevance (MMR) for retrieving relevant documents.</li>
<li>Contextual Compression enhances retrieval by extracting only relevant information from retrieved documents, using methods like LLMChainExtractor, LLMChainFilter, and EmbeddingsFilter to refine results and improve response quality.</li>
<li>The configure_chain function in LangChain sets up a conversational retrieval chain with memory for tracking conversation history, using a language model like ChatOpenAI for generating responses.</li>
<li>Streamlit is used to create a visual interface for the chatbot, allowing users to upload documents and ask questions, with the full implementation available on GitHub for further exploration.</li>
<li>Memory in LangChain is crucial for maintaining context and continuity in conversations, with various memory mechanisms like ConversationBufferMemory and ConversationSummaryMemory serving different use cases.</li>
<li>Important images include a screenshot of the chatbot interface for uploading documents and a diagram illustrating the process of building a chatbot with retrieval capabilities.</li>
<li>LangChain provides various memory mechanisms like ConversationBufferMemory, ConversationKGMemory, and CombinedMemory to manage conversation history, track entities, and summarize conversations, enabling the development of context-aware conversational agents.</li>
<li>ConversationBufferMemory stores messages in a buffer, allowing for a simple record of past interactions, while ConversationBufferWindowMemory maintains a fixed-sized window of recent interactions for efficiency.</li>
<li>ConversationSummaryMemory is designed for longer conversations, summarizing the conversation over time to maintain token efficiency and context.</li>
<li>ConversationKGMemory utilizes a knowledge graph to structure memory, representing entities as nodes and relationships as edges, enabling semantic reasoning and context-aware responses.</li>
<li>CombinedMemory allows for the integration of different memory types, leveraging their strengths for optimal performance in managing conversation history and context.</li>
<li>Long-term persistence in chatbots can be achieved through dedicated backends like Zep, which stores, summarizes, and searches chat histories using vector embeddings for enhanced context awareness.</li>
<li>Moderation is crucial in chatbot development for filtering inappropriate content, maintaining brand reputation, preventing abuse, and ensuring legal compliance, with tools like OpenAIModerationChain facilitating this process.</li>
<li>Guardrails in LLMs provide programmable constraints to control topics, define dialogue paths, specify language style, and extract structured data, ensuring outputs align with desired criteria for safety and trustworthiness.</li>
<li>The chapter concludes with a summary of retrieval mechanisms, memory options, and moderation practices, emphasizing their importance in developing accurate, context-aware, and respectful chatbots.</li>
<li>Important questions for review include understanding different chatbots, aspects of chatbot development, RAG, embeddings, vector search, vector databases, retrievers in LangChain, memory options, and moderation practices.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="code-snippets">code snippets<a href="#code-snippets" class="hash-link" aria-label="Direct link to code snippets" title="Direct link to code snippets">​</a></h2>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_openai import OpenAIEmbeddings</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">embeddings = OpenAIEmbeddings(model=&quot;text-embedding-3-large&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">text = &quot;This is a sample query.&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">query_result = embeddings.embed_query(text)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_community.embeddings import HuggingFaceBgeEmbeddings</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model_name = &quot;BAAI/bge-small-en&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model_kwargs = {&quot;device&quot;: &quot;cpu&quot;}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">encode_kwargs = {&quot;normalize_embeddings&quot;: True}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">hf = HuggingFaceBgeEmbeddings(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">words = [&quot;cat&quot;, &quot;dog&quot;, &quot;computer&quot;, &quot;animal&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">doc_vectors = hf.embed_documents(words)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from scipy.spatial.distance import pdist, squareform</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import numpy as np</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import pandas as pd</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">X = np.array(doc_vectors)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dists = squareform(pdist(X))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import pandas as pd</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df = pd.DataFrame(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    data=dists,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    index=words,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    columns=words</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.style.background_gradient(cmap=&#x27;coolwarm&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_community.document_loaders import WebBaseLoader</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">loader = WebBaseLoader(&quot;https://www.amazon.com/Generative-AI-LangChain-language-</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docs = loader.load()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_text_splitters import CharacterTextSplitter</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">splitter = CharacterTextSplitter(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    chunk_size=500,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    chunk_overlap=0,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    separator=&quot;\nReport&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">split_docs = splitter.split_documents(docs)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_community.vectorstores.chroma import Chroma</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_cohere import CohereEmbeddings</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vectorstore = Chroma.from_documents(documents=split_docs, embedding=CohereEmbedd</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">similar_vectors = vectorstore.similarity_search(query=&quot;This is a fantastic book!&quot;, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Showing 0 comments</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">There was a problem loading comments right now. Please try again later.MZ5.0 out of</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Great Book!</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Reviewed in the United States on January 1, 2024Verified Purchase</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">This book has substance and interesting material, best book so far in the market co</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2 people found this helpful</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              Helpful</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_community.document_loaders.text import TextLoader</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">loader = TextLoader(file_path=&quot;path/to/file.txt&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">documents = loader.load()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_community.document_loaders.wikipedia import WikipediaLoader</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">loader = WikipediaLoader(&quot;LangChain&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">documents = loader.load()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">print(documents[0].page_content[:102])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_core.output_parsers import StrOutputParser</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_anthropic import ChatAnthropic</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain import hub</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_community.document_loaders import ArxivLoader</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docs = ArxivLoader(query=&quot;2201.11903&quot;, load_max_docs=2).load()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_community.retrievers import KNNRetriever</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_openai import OpenAIEmbeddings</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">words = [&quot;cat&quot;, &quot;dog&quot;, &quot;computer&quot;, &quot;animal&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">retriever = KNNRetriever.from_texts(words, OpenAIEmbeddings())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">result = retriever.get_relevant_documents(&quot;dog&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">print(result)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_community.retrievers.pubmed import PubMedRetriever</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">retriever = PubMedRetriever() </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">documents = retriever.get_relevant_documents(&quot;COVID&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for document in documents:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    print(document.metadata[&quot;Title&quot;])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_core.documents import Document</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_core.retrievers import BaseRetriever</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class MyRetriever(BaseRetriever):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def get_relevant_documents(self, query: str, **kwargs) -&gt; list[Document]:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        relevant_documents = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return relevant_documents</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_community.vectorstores.docarray import DocArrayInMemorySearch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_text_splitters import RecursiveCharacterTextSplitter</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_core.retrievers import BaseRetriever</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def configure_retriever(docs: list[Document]) -&gt; BaseRetriever:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    splits = text_splitter.split_documents(docs)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    embeddings = HuggingFaceEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    retriever = vectordb.as_retriever(search_type=&quot;mmr&quot;, search_kwargs={&quot;k&quot;: 2, &quot;fetch_k&quot;: 4})</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return retriever</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.retrievers.document_compressors import EmbeddingsFilter</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.retrievers import ContextualCompressionRetriever</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def configure_retriever(docs: list[Document], use_compression: bool = True) -&gt; BaseRetriever:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    if not use_compression:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return retriever</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    embeddings_filter = EmbeddingsFilter(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        embeddings=embeddings, similarity_threshold=0.76</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return ContextualCompressionRetriever(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        base_compressor=embeddings_filter,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        base_retriever=retriever</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_openai import ChatOpenAI</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.chains.base import Chain</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.memory import ConversationBufferMemory</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def configure_chain(retriever: BaseRetriever) -&gt; Chain:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    memory = ConversationBufferMemory(memory_key=&quot;chat_history&quot;, return_messages=True)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    llm = ChatOpenAI(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        model_name=&quot;gpt-3.5-turbo&quot;, temperature=0, streaming=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return ConversationalRetrievalChain.from_llm(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        llm, retriever=retriever, memory=memory, verbose=True, max_tokens_limit=4000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import os</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import tempfile</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def configure_qa_chain(uploaded_files):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    docs = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    temp_dir = tempfile.TemporaryDirectory()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    for file in uploaded_files:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        temp_filepath = os.path.join(temp_dir.name, file.name)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        with open(temp_filepath, &quot;wb&quot;) as f:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            f.write(file.getvalue())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        docs.extend(load_document(temp_filepath))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    retriever = configure_retriever(docs=docs)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return configure_chain(retriever=retriever)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import streamlit as st</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from streamlit.external.langchain import StreamlitCallbackHandler</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">st.set_page_config(page_title=&quot;LangChain: Chat with Documents&quot;, page_icon=&quot;&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">st.title(&quot; LangChain: Chat with Documents&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">uploaded_files = st.sidebar.file_uploader(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    label=&quot;Upload files&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type=list(DocumentLoader.supported_extentions.keys()),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    accept_multiple_files=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">if not uploaded_files:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    st.info(&quot;Please upload documents to continue.&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    st.stop()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">qa_chain = configure_qa_chain(uploaded_files)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">assistant = st.chat_message(&quot;assistant&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">user_query = st.chat_input(placeholder=&quot;Ask me anything!&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">if user_query:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    stream_handler = StreamlitCallbackHandler(assistant)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    response = qa_chain.run(user_query, callbacks=[stream_handler])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    st.markdown(response)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.memory import ConversationBufferMemory</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.chains import ConversationChain</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_openai import ChatOpenAI</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llm = ChatOpenAI()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">memory = ConversationBufferMemory()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">chain = ConversationChain(llm=llm, memory=memory)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.memory import ConversationBufferWindowMemory</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">memory = ConversationBufferWindowMemory(k=1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">memory.save_context({&quot;input&quot;: &quot;hi&quot;}, {&quot;output&quot;: &quot;whats up&quot;})</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.memory import ConversationSummaryMemory</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_openai import OpenAI</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llm = OpenAI(temperature=0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">memory = ConversationSummaryMemory(llm=llm)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">memory.save_context({&quot;input&quot;: &quot;hi&quot;}, {&quot;output&quot;: &quot;whats up&quot;})</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.memory import ConversationKGMemory</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_openai import OpenAI</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llm = OpenAI(temperature=0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">memory = ConversationKGMemory(llm=llm)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain_openai import OpenAI</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.prompts import PromptTemplate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.chains import ConversationChain</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from langchain.memory import (ConversationBufferMemory, CombinedMemory, ConversationSummaryMemory)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llm = OpenAI(temperature=0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">conv_memory = ConversationBufferMemory(memory_key=&quot;chat_history_lines&quot;, input_key=&quot;input&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">summary_memory = ConversationSummaryMemory(llm=llm, input_key=&quot;input&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">memory = CombinedMemory(memories=[conv_memory, summary_memory])</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-tags-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/retrieval-augmented-generation-rag">Retrieval-Augmented Generation (RAG)</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vector-embeddings">Vector Embeddings</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/lang-chain">LangChain</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/chatbot-implementation">Chatbot Implementation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/semantic-search">Semantic Search</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vector-indexing">Vector Indexing</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vector-libraries">Vector Libraries</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vector-databases">Vector Databases</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/document-loaders">Document Loaders</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/retrievers">Retrievers</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vector-storage">Vector Storage</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/conversation-buffer-memory">ConversationBufferMemory</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/conversation-kg-memory">ConversationKGMemory</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/combined-memory">CombinedMemory</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/moderation">Moderation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/guardrails">Guardrails</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/AIEng/LangChain/03Getting Started with LangChain_content"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">03Getting Started with LangChain</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/AIEng/LangChain/05Developing Software with Generative AI_content"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">05Developing Software with Generative AI</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#summary" class="table-of-contents__link toc-highlight">summary</a></li><li><a href="#code-snippets" class="table-of-contents__link toc-highlight">code snippets</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/learn-with-ai/kb" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>