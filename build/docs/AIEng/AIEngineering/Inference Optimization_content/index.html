<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-AIEng/AIEngineering/Inference Optimization_content" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">09. Inference Optimization | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://your-docusaurus-site.example.com/docs/AIEng/AIEngineering/Inference Optimization_content"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="09. Inference Optimization | My Site"><meta data-rh="true" name="description" content="summary"><meta data-rh="true" property="og:description" content="summary"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://your-docusaurus-site.example.com/docs/AIEng/AIEngineering/Inference Optimization_content"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/AIEng/AIEngineering/Inference Optimization_content" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/AIEng/AIEngineering/Inference Optimization_content" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="My Site Atom Feed"><link rel="stylesheet" href="/assets/css/styles.1724a0b2.css">
<script src="/assets/js/runtime~main.2d8c9eb9.js" defer="defer"></script>
<script src="/assets/js/main.ed51d313.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">My Site</b></a><a class="navbar__item navbar__link" href="/docs/SE/ModelDriven/Part1_">Tutorial</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/AIEng/AIDevops/Introduction_content">AI Engineering Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/tags">Tags</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/AIDevops/Introduction_content">AIDevops</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/AIEng/AIEngineering/Introduction to Building AI Applications with Foundation Models_content">AIEngineering</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Introduction to Building AI Applications with Foundation Models_content">01. Introduction to Building AI Applications with Foundation Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Understanding Foundation Models _content">02. Understanding Foundation Models </a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Evaluation Methodology_content">03. Evaluation Methodology</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Evaluate AI Systems _content">04. Evaluate AI Systems </a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Prompt Engineering_content">05. Prompt Engineering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/RAG and Agents_content">06. RAG and Agents</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Finetuning_content">07. Finetuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Dataset Engineering_content">08. Dataset Engineering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/AIEng/AIEngineering/Inference Optimization_content">09. Inference Optimization</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Architecture and User Feedback_content">10. Architecture and User Feedback</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/LangChain/00Preface_content">LangChain</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/PromptEng/The Five Principles of Prompting_content">PromptEng</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/ScalableAI/Introduction to Scalable AI Systems_content">ScalableAI</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">AIEngineering</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">09. Inference Optimization</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>09. Inference Optimization</h1></header><h2 class="anchor anchorWithStickyNavbar_LWe7" id="summary">summary<a href="#summary" class="hash-link" aria-label="Direct link to summary" title="Direct link to summary">​</a></h2>
<ul>
<li>Inference optimization is crucial for making AI models faster and cheaper, focusing on model, hardware, and service levels.</li>
<li>The chapter introduces the importance of inference optimization for user patience and return on investment.</li>
<li>Different optimization techniques target compute-bound and memory bandwidth-bound bottlenecks, with examples like password decryption and data transfer rates.</li>
<li>The roofline chart is introduced as a tool to visualize whether an operation is compute-bound or memory bandwidth-bound.</li>
<li>Autoregressive language models&#x27; inference involves prefilling (compute-bound) and decoding (memory bandwidth-bound) steps.</li>
<li>Online APIs optimize for latency, while batch APIs optimize for cost, with examples of use cases for each.</li>
<li>Performance metrics like latency, TTFT, and TPOT are discussed for evaluating inference services.</li>
<li>The importance of considering latency in percentiles rather than averages to understand user experience better is highlighted.</li>
<li>Throughput and goodput are introduced as metrics to measure the efficiency of an inference service.</li>
<li>The chapter provides insights into real-world applications and the interdisciplinary nature of inference optimization.</li>
<li>Images described include a simple inference service diagram, a roofline chart for performance analysis, and a computational model for generating output based on input text.</li>
<li>Throughput is a critical metric for understanding how an inference service handles concurrent requests, often measured in requests per second (RPS) or completed requests per minute (RPM).</li>
<li>The cost of inference is directly linked to throughput, with higher throughput typically resulting in lower costs for processing tokens.</li>
<li>Goodput is introduced as a more user-centric metric, measuring the number of requests per second that satisfy the Service Level Objective (SLO), ensuring a better user experience.</li>
<li>Utilization metrics like MFU and MBU are explained to measure the efficiency of hardware usage, distinguishing between compute-bound and bandwidth-bound workloads.</li>
<li>The chapter discusses the evolution and importance of AI accelerators, highlighting the differences between CPUs and GPUs, and introduces various specialized chips for AI workloads.</li>
<li>Practical steps for calculating costs and utilization metrics are provided, emphasizing the importance of understanding hardware specifications for optimization.</li>
<li>Real-world applications include cost analysis for inference services and the selection of appropriate hardware based on workload characteristics.</li>
<li>The chapter concludes with insights into the future of AI accelerators, including specialized chips for inference and the importance of memory bandwidth and computational capabilities.</li>
<li>Important images include Figure 9-4 illustrating goodput, Figure 9-5 showing MBU across different GPUs, and Figure 9-6 explaining compute primitives for different data types.</li>
<li>The chapter discusses the memory hierarchy of AI accelerators and the challenges in optimizing GPU memory access, highlighting the lack of fine-grained control in popular frameworks like PyTorch and TensorFlow.</li>
<li>It introduces GPU programming languages such as CUDA, Triton, and ROCm as solutions for AI researchers and engineers interested in deeper optimization.</li>
<li>The significant power consumption and environmental impact of GPUs are examined, with examples like the NVIDIA H100 consuming approximately 7,000 kWh annually.</li>
<li>The concept of selecting accelerators based on workload characteristics (compute-bound vs. memory-bound) is explained, emphasizing the importance of FLOP/s, memory size, and bandwidth.</li>
<li>Inference optimization is categorized into model, hardware, and service levels, with an analogy to archery to illustrate their differences.</li>
<li>Model optimization techniques such as quantization, distillation, and pruning are discussed, including their benefits and practical challenges.</li>
<li>The autoregressive decoding bottleneck in language models is addressed, with speculative decoding presented as a method to improve generation speed without compromising quality.</li>
<li>Inference with reference is introduced as a technique to speed up generation by copying relevant tokens from the input, useful in scenarios with significant context-output overlap.</li>
<li>Parallel decoding techniques like Lookahead decoding and Medusa are explored for breaking sequential dependency in token generation, with verification mechanisms to ensure coherence.</li>
<li>The chapter concludes with practical insights into implementing these optimization techniques, noting their incorporation into popular inference frameworks.</li>
<li>Important images include the memory hierarchy of an AI accelerator, performance benchmarks of Llama models, and diagrams illustrating speculative decoding and inference with reference processes.</li>
<li>Jacobi decoding is introduced as a parallel decoding algorithm where only failed tokens are regenerated or adjusted, refining them until they pass verification and are integrated into the final output.</li>
<li>Medusa employs a tree-based attention mechanism to verify and integrate tokens, with each head producing several options for each position, organized into a tree-like structure for selecting the most promising combination.</li>
<li>The KV cache is highlighted as a method to avoid recomputing key and value vectors at each decoding step by storing them for reuse, significantly optimizing inference processes.</li>
<li>Attention mechanism optimization techniques are categorized into redesigning the attention mechanism, optimizing the KV cache, and writing kernels for attention computation, each addressing different aspects of inference efficiency.</li>
<li>Local windowed attention, cross-layer attention, and multi-query attention are discussed as methods to reduce the KV cache size and attention computation requirements.</li>
<li>The importance of kernel writing for optimizing attention computation on specific hardware is emphasized, with FlashAttention presented as a notable example of an optimized kernel.</li>
<li>Techniques such as vectorization, parallelization, loop tiling, and operator fusion are outlined as common methods to speed up computation in kernel writing.</li>
<li>The role of compilers in bridging ML models and the hardware they run on is discussed, highlighting the process of lowering model scripts to run efficiently on specific hardware.</li>
<li>A case study from PyTorch demonstrates throughput improvement through optimization steps like model compilation, quantization, and speculative decoding, showcasing practical applications of inference optimization techniques.</li>
<li>Key images include Figure 9-11 illustrating Medusa&#x27;s tree-based attention mechanism, Figure 9-12 showing the KV cache optimization, and Figure 9-13 depicting FlashAttention&#x27;s fused operators, each providing visual context to the discussed concepts.</li>
<li>Inference optimization techniques focus on improving the efficiency of AI model inference by managing resources and reducing latency and cost.</li>
<li>Different compilers like OpenXLA and TensorRT are optimized for specific hardware, such as NVIDIA GPUs, to speed up workloads.</li>
<li>Service-level optimization techniques aim to efficiently allocate fixed resources to dynamic workloads without modifying the model or affecting output quality.</li>
<li>Batching requests together can significantly reduce service throughput, with techniques including static, dynamic, and continuous batching to manage latency and compute efficiency.</li>
<li>Dynamic batching sets a maximum time window for each batch to keep latency under control, while continuous batching allows responses to be returned as soon as they are completed.</li>
<li>Decoupling prefill and decode steps in LLM inference can prevent resource competition and improve processing volume and latency.</li>
<li>Prompt caching stores overlapping text segments for reuse, significantly reducing latency and cost for applications with long system prompts or documents.</li>
<li>Parallelism strategies like data parallelism, model parallelism, and tensor parallelism are crucial for high-performance computing and can be combined for optimization.</li>
<li>Tensor parallelism partitions tensors across devices to serve large models and reduce latency, despite potential communication overhead.</li>
<li>Pipeline parallelism divides model computation into stages across devices, increasing throughput but may increase latency due to communication between stages.</li>
<li>Context and sequence parallelism are specialized techniques for efficient processing of long input sequences.</li>
<li>The chapter emphasizes the importance of understanding efficiency metrics like latency, throughput, and utilization for optimizing inference.</li>
<li>Practical implementation opportunities include applying batching techniques, decoupling prefill and decode, and utilizing prompt caching for cost and latency reduction.</li>
<li>Real-world applications include optimizing AI-powered decisions and integrating AI into more applications through faster and cheaper inference.</li>
<li>Interesting insights include the significant cost and latency savings from prompt caching, as demonstrated by Google Gemini and Anthropic.</li>
<li>The chapter concludes by highlighting the ongoing innovation in inference optimization to make AI more accessible and efficient.</li>
<li>Important images include diagrams illustrating static vs. dynamic batching, continuous batching, decoupling prefill and decode, and various parallelism strategies.</li>
<li>Inference optimization involves balancing cost and latency, with hardware playing a crucial role in model efficiency.</li>
<li>The chapter covers model-level and inference service-level optimization techniques, highlighting their impact on model behavior and serving methods.</li>
<li>Model-agnostic techniques like quantization and distillation are discussed, along with architecture-specific optimizations for transformer models and autoregressive language models.</li>
<li>Inference service-level techniques include batching, parallelism strategies, and specific methods for autoregressive language models like prefilling/decoding decoupling and prompt caching.</li>
<li>The choice of optimization techniques depends on workload characteristics and performance requirements, with quantization, tensor parallelism, replica parallelism, and attention mechanism optimization being particularly impactful.</li>
<li>Anecdotes and insights include the relationship between training and inference costs, the evolution of GPU usage beyond graphics, and the historical context of neural network limitations.</li>
<li>Practical considerations for inference services include separating APIs into online and batch to prioritize latency and the importance of prompt caching for applications with predictable system prompts.</li>
<li>The chapter concludes with a look ahead to integrating these optimization techniques into a cohesive system, emphasizing the importance of understanding underlying hardware and model architectures for effective optimization.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="code-snippets">code snippets<a href="#code-snippets" class="hash-link" aria-label="Direct link to code snippets" title="Direct link to code snippets">​</a></h2>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">No direct code references found in the chapter.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-tags-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/inference-optimization">Inference Optimization</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/compute-bound">Compute-bound</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/memory-bandwidth-bound">Memory bandwidth-bound</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/online-ap-is">Online APIs</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/batch-ap-is">Batch APIs</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/throughput">Throughput</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/goodput">Goodput</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/mfu-model-flop-s-utilization">MFU (Model FLOP/s Utilization)</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/mbu-model-bandwidth-utilization">MBU (Model Bandwidth Utilization)</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/ai-accelerators">AI Accelerators</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/gpu-optimization">GPU optimization</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/cuda">CUDA</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/triton">Triton</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/ro-cm">ROCm</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/model-compression">Model compression</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/jacobi-decoding">Jacobi decoding</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/medusa-heads">Medusa heads</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/kv-cache">KV cache</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/flash-attention">FlashAttention</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/py-torch-optimization">PyTorch optimization</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/tensor-rt">TensorRT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/open-xla">OpenXLA</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/dynamic-batching">Dynamic Batching</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/continuous-batching">Continuous Batching</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/prompt-caching">Prompt Caching</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/quantization">Quantization</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/distillation">Distillation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/kv-cache-management">KV cache management</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/tensor-parallelism">Tensor parallelism</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/replica-parallelism">Replica parallelism</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/AIEng/AIEngineering/Dataset Engineering_content"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">08. Dataset Engineering</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/AIEng/AIEngineering/Architecture and User Feedback_content"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">10. Architecture and User Feedback</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#summary" class="table-of-contents__link toc-highlight">summary</a></li><li><a href="#code-snippets" class="table-of-contents__link toc-highlight">code snippets</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/learn-with-ai/kb" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>