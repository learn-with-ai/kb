<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-AIEng/AIEngineering/Understanding Foundation Models _content" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">02. Understanding Foundation Models | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://your-docusaurus-site.example.com/docs/AIEng/AIEngineering/Understanding Foundation Models _content"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="02. Understanding Foundation Models | My Site"><meta data-rh="true" name="description" content="summary"><meta data-rh="true" property="og:description" content="summary"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://your-docusaurus-site.example.com/docs/AIEng/AIEngineering/Understanding Foundation Models _content"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/AIEng/AIEngineering/Understanding Foundation Models _content" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/AIEng/AIEngineering/Understanding Foundation Models _content" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="My Site Atom Feed"><link rel="stylesheet" href="/assets/css/styles.1724a0b2.css">
<script src="/assets/js/runtime~main.2d8c9eb9.js" defer="defer"></script>
<script src="/assets/js/main.ed51d313.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">My Site</b></a><a class="navbar__item navbar__link" href="/docs/SE/ModelDriven/Part1_">Tutorial</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/AIEng/AIDevops/Introduction_content">AI Engineering Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/tags">Tags</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/AIDevops/Introduction_content">AIDevops</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/AIEng/AIEngineering/Introduction to Building AI Applications with Foundation Models_content">AIEngineering</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Introduction to Building AI Applications with Foundation Models_content">01. Introduction to Building AI Applications with Foundation Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/AIEng/AIEngineering/Understanding Foundation Models _content">02. Understanding Foundation Models </a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Evaluation Methodology_content">03. Evaluation Methodology</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Evaluate AI Systems _content">04. Evaluate AI Systems </a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Prompt Engineering_content">05. Prompt Engineering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/RAG and Agents_content">06. RAG and Agents</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Finetuning_content">07. Finetuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Dataset Engineering_content">08. Dataset Engineering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Inference Optimization_content">09. Inference Optimization</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/AIEng/AIEngineering/Architecture and User Feedback_content">10. Architecture and User Feedback</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/LangChain/00Preface_content">LangChain</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/PromptEng/The Five Principles of Prompting_content">PromptEng</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/AIEng/ScalableAI/Introduction to Scalable AI Systems_content">ScalableAI</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">AIEngineering</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">02. Understanding Foundation Models </span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>02. Understanding Foundation Models </h1></header><h2 class="anchor anchorWithStickyNavbar_LWe7" id="summary">summary<a href="#summary" class="hash-link" aria-label="Direct link to summary" title="Direct link to summary">​</a></h2>
<ul>
<li>Foundation models are complex and costly to train, with design decisions impacting downstream applications significantly.</li>
<li>Training data quality and distribution are crucial for a model&#x27;s capabilities and limitations, with Common Crawl being a common but questionable source.</li>
<li>Multilingual models face challenges due to the dominance of English in training data, leading to underperformance in under-represented languages.</li>
<li>Domain-specific models, like those in biomedicine, require curated datasets for tasks not well-covered by general-purpose models.</li>
<li>The transformer architecture dominates language-based foundation models due to its attention mechanism, despite its limitations.</li>
<li>Practical steps include curating datasets aligned with specific needs and understanding the impact of model architecture and size on usability.</li>
<li>Real-world applications highlight the importance of domain-specific models for tasks like drug discovery and cancer screening.</li>
<li>Interesting insights include the under-representation of languages in training data and the efficiency of tokenization varying by language.</li>
<li>Final takeaway emphasizes the need for high-quality, domain-specific training data and understanding model architecture for effective application.</li>
<li>Important images include performance benchmarks across languages and domain distributions in datasets like C4.</li>
<li>The chapter discusses the evolution and mechanics of foundation models in AI, focusing on seq2seq and transformer architectures.</li>
<li>Seq2seq models process input tokens sequentially, which can be slow and limit output quality due to reliance on the final hidden state.</li>
<li>Transformer models address seq2seq limitations by using an attention mechanism, allowing parallel processing and dynamic focus on relevant input tokens.</li>
<li>The attention mechanism uses key, value, and query vectors to weigh the importance of input tokens, improving model performance.</li>
<li>Multi-headed attention splits these vectors into smaller parts, enabling the model to attend to different token groups simultaneously.</li>
<li>Transformer blocks consist of attention and MLP modules, with the number of blocks referred to as the model&#x27;s layers.</li>
<li>Models like Llama 2 and Llama 3 are discussed, highlighting their dimensions, feedforward layers, and vocabulary sizes.</li>
<li>Alternative architectures to transformers, such as RWKV and SSMs (including S4, H3, Mamba, and Jamba), are introduced as promising for long sequence processing.</li>
<li>The chapter emphasizes the importance of model size, noting that larger models generally perform better due to increased learning capacity.</li>
<li>Visualizations in the chapter help illustrate the differences between seq2seq and transformer architectures, the attention mechanism, and the composition of transformer models.</li>
<li>The chapter concludes by noting the ongoing evolution of model architectures and the potential for future advancements to surpass transformers.</li>
<li>Important images include visualizations of seq2seq versus transformer architectures, the attention mechanism in action, and the composition of transformer and Jamba blocks.</li>
<li>Newer-generation models like Llama 3-8B outperform older-generation models such as Llama 2-70B, highlighting the rapid advancement in model training techniques.</li>
<li>The number of parameters in a model helps estimate the compute resources needed for training and inference, with sparse models offering more efficient computation than dense models.</li>
<li>Mixture-of-experts (MoE) models, such as Mixtral 8x7B, divide parameters into groups of experts, with only a subset active per token, optimizing cost and speed.</li>
<li>The size of the training dataset, measured in tokens, is crucial for model performance, with models like Llama 3 trained on up to 15 trillion tokens.</li>
<li>FLOPs measure the compute requirement for training models, with examples like Google’s PaLM-2 requiring 10^25 FLOPs.</li>
<li>The Chinchilla scaling law suggests that for compute-optimal training, the number of training tokens should be approximately 20 times the model size.</li>
<li>Scaling extrapolation is a niche research area focusing on predicting optimal hyperparameters for large models based on smaller model studies.</li>
<li>Emergent abilities in large models, not observable in smaller models, complicate hyperparameter extrapolation.</li>
<li>Scaling bottlenecks include the limited availability of training data and electricity, with concerns about running out of internet data for training.</li>
<li>The cost of achieving a given model performance is decreasing, but improving model performance remains expensive.</li>
<li>The graph illustrates the relationship between training loss, model size, FLOPs, and the number of training tokens, showing how larger models require more computational resources but can achieve lower loss rates.</li>
<li>Another graph projects the effective stock of tokens for various models over time, highlighting the rapid growth in dataset sizes and the potential for future bottlenecks.</li>
<li>The chapter discusses the nuanced impact of AI-generated data on models and the competitive advantage of unique proprietary data in the AI race.</li>
<li>Data restrictions from web sources have significantly increased, rendering over 28% of critical sources in the popular public dataset C4 fully restricted.</li>
<li>Electricity consumption by data centers is a growing concern, estimated to consume 1–2% of global electricity, potentially reaching 4% to 20% by 2030.</li>
<li>Post-training is introduced as a critical phase to align pre-trained models with human preferences, addressing issues like inappropriate outputs and optimizing for conversations.</li>
<li>Supervised finetuning (SFT) and preference finetuning are key steps in post-training, with techniques like RLHF, DPO, and RLAIF highlighted for aligning models with human preferences.</li>
<li>The difference between pre-training and post-training is likened to acquiring knowledge versus learning how to use that knowledge.</li>
<li>The importance of high-quality labelers for generating demonstration data is emphasized, noting the cost and time involved in creating such datasets.</li>
<li>The chapter explores the challenges of preference finetuning, including the ambitious goal of embedding universal human preference into AI models.</li>
<li>RLHF&#x27;s reliance on a reward model and the challenges of obtaining reliable comparison data are discussed, along with the cost and time of manual comparisons.</li>
<li>The chapter concludes with insights into the practical steps and assessments for aligning AI models with human preferences, highlighting the complexity and cost involved.</li>
<li>Important images include a diagram illustrating the RLHF framework and a humorous representation of RLHF as an octopus-like creature, aiding in understanding complex concepts.</li>
<li>The chapter discusses the training process of foundation models like InstructGPT, focusing on how comparison data is used to train models to give concrete scores through an objective function that maximizes the difference in output scores for winning and losing responses.</li>
<li>It explains the mathematical formula used by InstructGPT for training, detailing the components such as the reward model, training data format, and the sigmoid function.</li>
<li>The importance of finetuning the reward model on top of the strongest foundation model is highlighted, along with the use of proximal policy optimization (PPO) for further training.</li>
<li>The chapter explores sampling strategies in model outputs, including temperature, top-k, and top-p, to influence creativity and predictability in responses.</li>
<li>It delves into the probabilistic nature of AI outputs, explaining how models compute probabilities for possible outcomes and the impact of different sampling strategies on these probabilities.</li>
<li>The concept of logprobs (log probabilities) is introduced as a useful tool for building applications, evaluating them, and understanding model workings, despite some model providers limiting access to logprobs APIs for security reasons.</li>
<li>Practical implementation opportunities include experimenting with temperature settings to balance creativity and predictability in model outputs, and using top-k sampling to reduce computational workload without sacrificing response diversity.</li>
<li>Real-world applications mentioned include companies like Stitch Fix and Grab using the best of N strategy to improve model performance by selecting outputs with high scores from their reward models.</li>
<li>The chapter provides insights into the evolving field of preference finetuning and the potential future where better pre-training data might eliminate the need for SFT and preference finetuning.</li>
<li>Key images described include interfaces for generating comparison data, probability distributions over vocabulary, and workflow diagrams for computing logits, probabilities, and logprobs, aiding in understanding the practical aspects of foundation models.</li>
<li>Top-p sampling, also known as nucleus sampling, dynamically selects values to sample from based on cumulative probability, allowing for more contextually appropriate outputs.</li>
<li>Top-k sampling fixes the number of values considered for sampling, which can be less flexible compared to top-p sampling.</li>
<li>Test time compute involves generating multiple responses to a query to increase the chance of a good response, though it comes with increased computational costs.</li>
<li>Structured outputs are crucial for tasks requiring specific formats, such as semantic parsing, and for ensuring outputs can be parsed by downstream applications.</li>
<li>Constrained sampling guides text generation towards certain constraints by filtering logits to only include tokens that meet specified conditions.</li>
<li>The chapter discusses practical implementation opportunities like using application-specific heuristics to select the best response and overcoming latency challenges with parallel response generation.</li>
<li>Interesting insights include the effectiveness of top-p sampling in practice and the significant performance boost from using verifiers, comparable to a 30× model size increase.</li>
<li>Practical steps include using reward models to score outputs and employing post-processing to correct common mistakes in model outputs.</li>
<li>Real-world applications include semantic parsing tasks like text-to-SQL and generating structured outputs for downstream applications.</li>
<li>Additional key insights highlight the importance of model robustness and the benefits of sampling multiple outputs for tasks expecting exact answers.</li>
<li>The chapter concludes with the importance of structured outputs in production and the various approaches to guide models in generating them, including prompting, post-processing, and constrained sampling.</li>
<li>Important images include a bar chart on respondent sentiments towards foundation models and a graph illustrating the relationship between the number of completions per test problem and test solve rates.</li>
<li>Constrained sampling is complex due to the need for specific grammars for each output format, increasing generation latency and limiting generalizability.</li>
<li>Finetuning a model on examples of a desirable format is the most effective approach to ensure outputs match expected formats, though it doesn&#x27;t guarantee 100% reliability.</li>
<li>Modifying a model&#x27;s architecture, such as adding a classifier head, can guarantee output formats for specific tasks like classification.</li>
<li>The probabilistic nature of AI models leads to inconsistency and hallucinations, making them unpredictable in responses to the same or similar prompts.</li>
<li>Inconsistency in AI models can be mitigated by caching answers, fixing sampling variables, and controlling hardware, but these measures don&#x27;t ensure complete consistency.</li>
<li>Hallucinations, where models generate unfounded facts, are a significant challenge, especially for tasks requiring factual accuracy.</li>
<li>Two hypotheses explain hallucinations: self-delusion, where models cannot differentiate between given and generated data, and mismatched internal knowledge between models and labelers.</li>
<li>Techniques to mitigate hallucinations include reinforcement learning to differentiate prompts and generated tokens, and supervised learning with factual and counterfactual signals in training data.</li>
<li>Prompting techniques and asking models to base responses on known information can also help reduce hallucinations.</li>
<li>The chapter highlights the importance of understanding and mitigating the probabilistic nature, inconsistency, and hallucinations in AI models for reliable applications.</li>
<li>Important images include diagrams on adding a classifier head to a base model, examples of model inconsistency in scoring essays, and illustrations of hallucinations and self-delusion in models.</li>
<li>The chapter explores the core design decisions in building foundation models, emphasizing the importance of training data quality and volume.</li>
<li>Model development involves critical steps like architecting the model before training, with the transformer architecture being dominant for language-based models.</li>
<li>The scale of a model is determined by parameters, training tokens, and FLOPs, with scaling laws helping optimize these factors within a compute budget.</li>
<li>Post-training processes like supervised and preference finetuning address misalignments between model outputs and user expectations.</li>
<li>Sampling introduces a probabilistic nature to models, enabling creativity but also leading to inconsistencies and hallucinations.</li>
<li>Establishing a solid evaluation pipeline is crucial for systematic AI engineering to detect failures and unexpected changes.</li>
<li>The chapter highlights the challenges of model hallucinations and the difficulty in detecting them, comparing it to detecting human lies.</li>
<li>Real-world applications include the use of AI models for creative tasks and the importance of handling their probabilistic nature in workflows.</li>
<li>Interesting insights include the argument by Ilya Sutskever on the difficulty of developing new neural network architectures that outperform existing ones.</li>
<li>Practical steps involve curating training data for specific languages and domains, and the importance of evaluation pipelines in AI engineering.</li>
<li>The chapter references studies and articles on AI model performance across languages and the impact of training data biases.</li>
<li>Images and descriptions in the book provide visualizations of concepts like the chaotic probability distribution under higher temperature settings in sampling.</li>
<li>Final takeaways include the necessity of building workflows around the probabilistic nature of AI models and the ongoing challenges in model alignment and consistency.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="code-snippets">code snippets<a href="#code-snippets" class="hash-link" aria-label="Direct link to code snippets" title="Direct link to code snippets">​</a></h2>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">No direct code references found in the chapter.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-tags-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/foundation-models">Foundation Models</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/transformer-architecture">Transformer Architecture</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/training-data">Training Data</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/multilingual-models">Multilingual Models</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/domain-specific-models">Domain-Specific Models</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/seq-2-seq">Seq2seq</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/transformer">Transformer</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/attention-mechanism">Attention Mechanism</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/rnn">RNN</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/mixture-of-experts-mo-e">Mixture-of-Experts (MoE)</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/flo-ps">FLOPs</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/chinchilla-scaling-law">Chinchilla Scaling Law</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/scaling-extrapolation">Scaling Extrapolation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/supervised-finetuning-sft">Supervised Finetuning (SFT)</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/reinforcement-learning-from-human-feedback-rlhf">Reinforcement Learning from Human Feedback (RLHF)</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/direct-preference-optimization-dpo">Direct Preference Optimization (DPO)</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/ai-generated-data">AI-generated Data</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/instruct-gpt">InstructGPT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/reinforcement-learning">Reinforcement Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/sampling-strategies">Sampling Strategies</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/top-p-sampling">Top-p sampling</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/top-k-sampling">Top-k sampling</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/test-time-compute">Test time compute</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/structured-outputs">Structured outputs</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/finetuning">Finetuning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/classifier-head">Classifier Head</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/constrained-sampling">Constrained Sampling</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/post-training">Post-training</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/AIEng/AIEngineering/Introduction to Building AI Applications with Foundation Models_content"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">01. Introduction to Building AI Applications with Foundation Models</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/AIEng/AIEngineering/Evaluation Methodology_content"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">03. Evaluation Methodology</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#summary" class="table-of-contents__link toc-highlight">summary</a></li><li><a href="#code-snippets" class="table-of-contents__link toc-highlight">code snippets</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/learn-with-ai/kb" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>