---
sidebar_position: 1
tags: ["MLOps", "Data Management", "Feature Engineering", "Model Training", "Data Splitting", "Testing", "Release", "ML Frameworks", "Model Registry", "DevOps", "IaC", "Model Evaluation", "System Build"]
title: "05. AI Model Life Cycle"
---

## summary

- The chapter discusses the comprehensive life cycle of AI models, emphasizing the importance of tool selection without overcomplicating the process.
- It highlights the transition from model selection to production, detailing the roles of various tools in data management, model training, and deployment.
- The text introduces the concept of MLOps as a set of practices for preparing and operating ML models in production, covering narrow ML models and foundation models (FMs).
- Key considerations in model development include data representativeness, generalization versus accuracy, and effective stress testing.
- Data management stages are outlined, including gathering, cleaning, and processing data, with tools like Amazon S3, Google Cloud Storage, and Databricks mentioned.
- Feature engineering is described as the process of extracting and selecting features that best predict the phenomenon of interest, with tools like scikit-learn and H2O.ai highlighted.
- The importance of dividing data into training, validation, and test sets is emphasized, with tools like scikit-learn, TensorFlow, and PyTorch DataLoader recommended for this purpose.
- Model generation involves a two-phase approach: exploratory model development and a more structured training process, often conducted in environments like Jupyter Notebooks.
- The chapter underscores the significance of data lineage and versioning for transparency, reproducibility, and compliance in AI model development.
- Practical implementation opportunities include experimenting with different model architectures, hyperparameters, and feature effectiveness during the exploratory phase.
- Interesting insights include the balance between generalization and accuracy in model training and the challenge of creating change-proof features.
- Real-world applications mentioned include medical diagnosis models and weather prediction models, highlighting the need for robust testing against rare conditions.
- The final takeaway emphasizes the labor-intensive yet critical nature of data preparation and the importance of careful tool and method selection for building robust ML models.
- Important images described include visual representations of the AI model life cycle, steps in model development, and data management stages.
- The chapter details the two-phase approach to model training: exploratory modeling on a smaller scale followed by full-scale training, emphasizing the importance of consistent Infrastructure as Code (IaC) practices.
- It discusses the resource requirements disparity between training and executing models, noting simpler models can be deployed on lightweight devices while complex models need robust infrastructure.
- Three main types of learning methods for task-specific ML models are outlined: supervised, unsupervised, and self-supervised learning, with foundation models (FMs) trained on vast amounts of unlabeled data.
- Tool support for model training is highlighted, mentioning ML frameworks like TensorFlow, PyTorch, scikit-learn, and XGBoost, and cloud vendor platforms such as Google Cloud Vertex AI, Amazon SageMaker, and Microsoft Azure Machine Learning.
- The concept of a model registry is introduced as a centralized repository for managing the ML model life cycle, including version control, metadata management, and access control.
- The build process of a model is described, focusing on packaging the trained model with an inference engine for predictions or classifications, and the options for deployment as a direct procedure call or a service.
- Testing the model involves unit tests in isolation and system tests after integration, with terms related to AI evaluation and testing defined, including model accuracy testing and quality and risk evaluation.
- The use of AI to generate tests is explored, mentioning autonomous test generation tools and leveraging FMs to test another, alongside the importance of test repeatability.
- Factors impacting test repeatability are discussed, such as the state of the test database, concurrency issues, the probabilistic nature of ML and FMs, and software updates.
- The release process of a model is outlined, emphasizing the need for human sign-off in critical applications for safety, legal compliance, accountability, and due to incomplete understanding of some AI models.
- Key points from the chapter are summarized, covering model preparation, data preparation, training, packaging, deployment, and the responsibilities of AI engineers regarding third-party models.
- Important images described include visual representations of the AI model life cycle and steps in model development and data management stages.
- The chapter concludes with a discussion on the relationship between DevOps and MLOps, highlighting their key similarities and differences.
- It poses questions about the core components of an MLOps pipeline and their interactions, as well as the use of Infrastructure as Code (IaC) in MLOps.
- The text provides recommendations for further reading, including resources on the life cycle of ML models, the intersection of software engineering and ML, and AI model versus system evaluation.
- A key takeaway is the emphasis on the importance of robust development practices in ML and the comprehensive overview of the ML model life cycle.
- The discussion questions encourage deeper exploration of MLOps practices and their application in AI model development and deployment.
- References to further reading materials are provided, offering readers avenues to expand their understanding of MLOps and AI model evaluation.

## code snippets
```
No direct code references found in the chapter.
```
