---
sidebar_position: 1
tags: ["System Life Cycle", "Microservices Architecture", "Co-Design/Development", "Modifiability", "Continuous Integration", "Continuous Deployment", "Blue/Green Deployment", "Rolling Upgrade", "Version Skew", "A/B Testing", "Monitoring", "Incidents", "Data Drift", "Dynamic Model Updating", "Chaos Engineering"]
title: "06. System Life Cycle"
---

## summary

- The chapter discusses the comprehensive steps involved in the system life cycle of AI development, emphasizing that creating an AI model is just one component of a larger process.
- Barry Boehm's quote highlights the complexity and iterative nature of the system development life cycle (SDLC), which includes design, development, testing, and deployment phases.
- The importance of co-design and development is underscored, advocating for parallel development of AI and non-AI modules to facilitate smoother integration and problem resolution.
- Microservices architecture is recommended for its benefits in development and deployment speed, though it's noted that network communication can impact system efficiency.
- Designing for modifiability is crucial, with strategies like reducing coupling and increasing cohesion to localize changes and accommodate future modifications.
- The build stage merges AI and non-AI modules into an executable image, facilitated by continuous integration (CI) servers like Jenkins, which also generate build metadata for traceability.
- Testing is a critical phase, encompassing unit tests, system-wide tests, and various specialized tests (regression, compatibility, install/uninstall, efficiency, security, and compliance testing) to ensure system robustness.
- Automating system tests is encouraged to speed up the testing process and minimize reliance on manual testing.
- The chapter concludes with considerations for release and deployment, including handling data drift in AI models to maintain system relevance and performance over time.
- Key images include diagrams of the system development life cycle, the merging of AI and non-AI modules into an executable image, and the activities of a CI server in the build process.
- The chapter delves into the nuances of releasing and deploying AI systems, highlighting the importance of a well-defined process that includes quality gates and thorough testing.
- It contrasts defined releases, which follow a schedule, with continuous deployment (CD), which automates the deployment of changes to production, emphasizing the benefits of CD in microservices architectures.
- Two deployment patterns, blue/green and rolling upgrade, are discussed for updating services without interruption, each with its own tradeoffs regarding financial resource utilization and error response.
- Canary and A/B testing are introduced as methods for testing new releases in production with a limited set of users before full deployment, with A/B testing particularly useful for comparing AI model versions.
- The concept of rollback and roll-forward is explained as strategies for addressing issues in new releases, with a note on the challenges in complex AI systems and the potential need for graceful degradation.
- Version skew, a challenge in CD due to independent deployment of services, is explored, including temporal inconsistency and interface mismatch, with mitigation strategies like version tagging and feature toggles.
- The chapter concludes with considerations for matching the model to resources, discussing deployment options based on model size and serving mode, and the advantages of packaging APIs as services within containers.
- Key images and diagrams in the chapter illustrate deployment patterns and the process of continuous deployment, enhancing understanding of these complex concepts.
- The chapter focuses on the operational aspects of AI systems post-deployment, emphasizing the importance of monitoring, incident management, and continuous analysis to ensure system reliability and performance.
- Monitoring is crucial for observing resource usage and activities within AI systems, with data sourced from infrastructure, system code, and service logs to generate alerts, perform forensics, and conduct analysis.
- Incidents in AI systems can be logical, efficiency, availability, or security problems, requiring immediate and delayed responses from first responders to ensure system continuity and problem resolution.
- Data drift is a significant challenge as models may become less reflective of real-world situations over time, with techniques like monitoring model performance metrics and statistical tests recommended for detection.
- Dynamic model updating, including online learning and incremental updates, offers rapid adaptation but poses risks if bypassing standard QA processes, highlighting the need for caution and quality control.
- Chaos engineering is introduced as a practice to test system reliability under turbulent conditions by deliberately introducing failures, with tests covering data, model, and infrastructure aspects.
- Analysis of AI systems aims to inform future design and development iterations, with a focus on interdependencies between AI and non-AI components and the impact of AI decisions on system performance.
- The chapter concludes with a summary of the AI system life cycle, from design and development through testing, deployment, and operation, underscoring the value of automated CI/CD pipelines and quality gates.
- Discussion questions and further reading suggestions are provided to deepen understanding of operational challenges and best practices in managing AI systems throughout their life cycle.
- Key images and diagrams likely illustrate monitoring systems, chaos engineering tests, and data drift detection methods, enhancing comprehension of these operational concepts.

## code snippets
```
No direct code references found in the chapter.
```
