---
sidebar_position: 1
tags: ["Security", "AI-based systems", "Confidentiality, Integrity, Availability (CIA)", "DevSecOps", "Zero Trust", "CI/CD pipeline", "Data canaries", "Foundation Models (FMs)"]
title: "09. Security"
---

## summary

- Security in AI-based systems encompasses traditional security concerns with new challenges, emphasizing the importance of a layered defense approach.
- Traditional security is defined by the CIA properties: Confidentiality, Integrity, and Availability, which are foundational to both traditional and AI-based systems.
- DevSecOps integrates security into the DevOps process, promoting shared responsibility for security among development and operations teams.
- AI introduces new security challenges such as semantic attacks on LLMs and image classification vulnerabilities, expanding the attack surface.
- Quantum computing poses future risks to encryption, highlighting the need for quantum-safe algorithms in critical applications.
- The attack surface in AI systems is broader, including vulnerabilities in training data and models, with attack vectors targeting confidentiality and integrity.
- NIST categorizes AI attacks into evasion, poisoning, privacy, and abuse/misuse attacks, each targeting different phases of AI system lifecycle.
- Zero Trust architecture is recommended for modern security, emphasizing continuous verification, least privilege access, and encrypted communication.
- Process and architectural approaches to security include threat modeling, risk assessment, and the implementation of security patterns like least functionality and privilege separation.
- Data preparation and testing in the deployment pipeline are crucial for detecting vulnerabilities and ensuring the integrity of AI systems.
- Important images include a comparison of traditional security concepts with AI-specific considerations and a depiction of attack types in AI systems.
- Security during the build phase emphasizes securing the CI/CD pipeline to prevent unauthorized code deployment and ensure the integrity of the software development process.
- Automated security checks within the CI/CD pipeline, including penetration testing and AI-specific tests, help catch vulnerabilities early and prevent security regressions.
- Data canaries are introduced as a novel practice for AI-based systems to detect data drift, faults, or malicious attacks by monitoring small samples of data during retraining.
- Attacks during system operations can target data, models, or the system itself, with specific strategies like data minimization and encryption recommended to mitigate risks.
- Complex models like deep neural networks are more susceptible to adversarial attacks due to their opacity, whereas simpler models like decision trees offer more transparency and security.
- Denial-of-Service (DoS) attacks pose a significant threat to AI systems, with throttling requests and comprehensive logging suggested as countermeasures.
- Foundation Models (FMs) face unique security challenges, including supply chain vulnerabilities and the potential for economic attacks through repeated queries.
- Proactive measures such as careful curation of training data and the use of constitutional LLMs can enhance the security of FM-based systems by ensuring compliance with predefined rules.
- The chapter concludes with a summary of key security principles for AI systems, emphasizing the importance of zero trust architecture and the challenges specific to FMs.
- Discussion questions and further reading materials are provided to encourage deeper exploration of security and usability trade-offs, DoS attack deterrence, and data poisoning defenses.

## code snippets
```
No direct code references found in the chapter.
```
