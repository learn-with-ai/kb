---
sidebar_position: 1
tags: ["Vector Databases", "Embeddings", "FAISS", "Pinecone", "RAG", "Word2Vec", "TF-IDF", "Document Loading", "Vector Search", "Document Chunking", "Self-Querying"]
title: "05. Vector Databases with FAISS and Pinecone"
---

## titles

Vector Databases with FAISS and Pinecone Advanced Vector Database Techniques with FAISS and Pinecone Advanced Vector Database Operations with FAISS and Pinecone Advanced Vector Database Techniques and Self-Querying Retrievers

## summary

- This chapter introduces embeddings and vector databases, explaining their use in providing relevant context in prompts to decrease AI hallucinations.
- Vector databases store text data in a way that enables querying based on similarity or semantic meaning, improving the accuracy of LLM responses.
- Vectors represent text or images as lists of numbers, with models like OpenAI's text-embedding-ada-002 using 1,536 numbers to represent each word.
- The concept of vector distances is illustrated with a simplified two-dimensional example, showing how related texts are closer in vector space.
- Vector search allows querying based on similarity, returning closely associated records even without exact matches, useful for document reading, recommendation systems, and long-term memory in chatbots.
- Retrieval Augmented Generation (RAG) is highlighted as a method to dynamically insert relevant knowledge into prompts, using vector databases to find and retrieve the most relevant documents.
- The chapter details the process of generating embeddings using OpenAI's API and Hugging Face's Sentence Transformers, including code examples for both.
- Differences between dense and sparse vectors are explained, with dense vectors being more common in AI applications for capturing semantic information.
- The importance of the embedding model's accuracy is emphasized, noting that biases or knowledge gaps in the model affect vector search results.
- Training custom embedding models is discussed for cases requiring domain-specific vocabulary or recent context not covered by general models.
- Figure 5-1 and Figure 5-2 visually represent vector distances in two-dimensional and multidimensional spaces, respectively, aiding in understanding semantic relationships.
- The chapter continues with advanced techniques in vector databases, focusing on training custom embedding models using Gensim's Word2Vec and comparing it with TF-IDF for similarity calculations.
- Word2Vec model training is demonstrated with a small, repetitive dataset to illustrate how vectors can represent semantic relationships between words, though it's noted that larger, more diverse corpora are typically needed for effective learning.
- TF-IDF is introduced as a simpler, more robust alternative for smaller document sizes, with a code example showing how to compute cosine similarity between words using scikit-learn.
- The importance of document chunking strategies is discussed, highlighting the trade-offs between context retention and specificity in vector searches, with an example using LangChain's RecursiveCharacterTextSplitter.
- Memory retrieval with FAISS is detailed, including how to create an index for efficient similarity search and how to perform vector searches to find the most relevant document chunks for a given query.
- A practical example demonstrates integrating FAISS vector search with a chatbot to provide contextually relevant answers, showcasing the Retrieval Augmented Generation (RAG) technique to mitigate hallucinations.
- The chapter emphasizes the significance of the embedding model's accuracy and the strategy for embedding text chunks, noting the balance between context and similarity in vector searches.
- Code examples are provided for training a Word2Vec model, computing TF-IDF similarities, splitting documents into chunks, and performing vector searches with FAISS.
- The process of integrating search results into prompts for AI models is explained, with a focus on setting system messages to guide the model's responses based on the provided context.
- Practical considerations for working with vector databases, such as the cost and latency of recomputing vectors and the importance of not changing the embedding model once vectors are stored, are discussed.
- The chapter delves into advanced operations with vector databases, focusing on FAISS and Pinecone, including saving and loading indices, merging indices, and using hosted vector databases for enhanced scalability and reliability.
- A detailed explanation of the `vector_search` function showcases how to perform vector searches, integrate search results into prompts for AI models, and ensure responses are contextually relevant.
- The advantages of hosted vector databases like Pinecone are highlighted, including maintenance, scalability, reliability, performance, support, and security benefits over self-managed solutions.
- Practical steps for creating and managing a Pinecone index are provided, including setting up the API key, defining the index name and environment, and checking for existing indices.
- The process of storing vectors in a Pinecone index is explained, including batch processing, handling retries for rate limits, and upserting records with metadata.
- Querying the Pinecone index for similar vectors is demonstrated, showing how to retrieve the most relevant documents based on a user query and include metadata in the results.
- The chapter emphasizes the importance of document chunking strategies for effective vector searches and the trade-offs between context retention and specificity.
- LangChain's implementation of RAG is praised for its efficiency and readability, enabling rapid development of retrieval-augmented generation applications.
- Security and privacy considerations when using hosted vector databases are discussed, including the risks of vendor lock-in and data sharing with third parties.
- The chapter concludes with practical examples of querying the Pinecone index, demonstrating how to retrieve and utilize the most similar vectors for generating contextually relevant AI responses.
- The chapter explores advanced operations with vector databases, including emulating FAISS jobs with Pinecone, utilizing metadata for filtering queries, and the importance of metadata strategy alongside chunking strategy.
- Self-querying retrievers are introduced, highlighting their benefits such as schema definition and dual-layer retrieval, which enhance precision and relevance in document retrieval.
- A practical example demonstrates setting up a self-querying retriever with LangChain, including defining metadata attributes for books and querying based on specific criteria like genre or author.
- Different retrieval mechanisms are compared, including MultiQueryRetriever, Contextual Compression, Ensemble Retriever, Parent Document Retriever, and Time-Weighted Vector Store Retriever, each with unique advantages and limitations.
- The chapter concludes with a summary of vector databases' power for similarity-based text querying, the RAG process, and the cost-benefit analysis of retrieving embeddings from OpenAI versus open-source models.
- A preview of the next chapter on autonomous agents is provided, teasing the exploration of AI agents' decision-making capabilities and the challenges they face.

## code snippets
```
from openai import OpenAI
client = OpenAI()
# Function to get the vector embedding for a given text
def get_vector_embeddings(text):
    response = client.embeddings.create(
        input=text,
        model="text-embedding-ada-002"
    )
    embeddings = [r.embedding for r in response.data]
    return embeddings[0]
get_vector_embeddings("Your text string goes here")
import requests
import os
model_id = "sentence-transformers/all-MiniLM-L6-v2"
hf_token = os.getenv("HF_TOKEN")
api_url = "https://api-inference.huggingface.co/"
api_url += f"pipeline/feature-extraction/{model_id}"
headers = {"Authorization": f"Bearer {hf_token}"}
def query(texts):
    response = requests.post(api_url, headers=headers,
    json={"inputs": texts,
    "options":{"wait_for_model":True}})
    return response.json()
texts = ["mickey mouse",
        "cheese",
        "trap",
        "rat",
        "ratatouille"
        "bus",
        "airplane",
        "ship"]
output = query(texts)
output
from gensim.models import Word2Vec
# Sample data: list of sentences, where each sentence is
# a list of words.
# In a real-world scenario, you'd load and preprocess your
# own corpus.
sentences = [
    ["the", "cake", "is", "a", "lie"],
from gensim.models import Word2Vec
# Sample data: list of sentences, where each sentence is
# a list of words.
# In a real-world scenario, you'd load and preprocess your
# own corpus.
sentences = [
    ["the", "cake", "is", "a", "lie"],
    ["if", "you", "hear", "a", "turret", "sing", "you're",
    "probably", "too", "close"],
    ["why", "search", "for", "the", "end", "of", "a",
    "rainbow", "when", "the", "cake", "is", "a", "lie?"],
    ["there's", "no", "cake", "in", "space,", "just", "ask",
    "wheatley"],
    ["completing", "tests", "for", "cake", "is", "the",
    "sweetest", "lie"],
    ["I", "swapped", "the", "cake", "recipe", "with", "a",
    "neurotoxin", "formula,", "hope", "that's", "fine"],
] + [
    ["the", "cake", "is", "a", "lie"],
    ["the", "cake", "is", "definitely", "a", "lie"],
    ["everyone", "knows", "that", "cake", "equals", "lie"],
] * 10  # repeat several times to emphasize
# Train the word2vec model
model = Word2Vec(sentences, vector_size=100, window=5,
min_count=1, workers=4, seed=36)
# Save the model
model.save("custom_word2vec_model.model")
# To load the model later
# loaded_model = word2vec.load(
# "custom_word2vec_model.model")
# Get vector for a word
vector = model.wv['cake']
# Find most similar words
similar_words = model.wv.most_similar("cake", topn=5)
print("Top five most similar words to 'cake': ", similar_words)
# Directly query the similarity between "cake" and "lie"
cake_lie_similarity = model.wv.similarity("cake", "lie")
print("Similarity between 'cake' and 'lie': ",
cake_lie_similarity)
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
# Convert sentences to a list of strings for TfidfVectorizer
document_list = [' '.join(s) for s in sentences]
# Compute TF-IDF representation
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(document_list)
# Extract the position of the words "cake" and "lie" in
# the feature matrix
cake_idx = vectorizer.vocabulary_['cake']
lie_idx = vectorizer.vocabulary_['lie']
# Extract and reshape the vector for 'cake'
cakevec = tfidf_matrix[:, cake_idx].toarray().reshape(1, -1)
# Compute the cosine similarities
similar_words = cosine_similarity(cakevec, tfidf_matrix.T).flatten()
# Get the indices of the top 6 most similar words
# (including 'cake')
top_indices = np.argsort(similar_words)[-6:-1][::-1]
# Retrieve and print the top 5 most similar words to
# 'cake' (excluding 'cake' itself)
names = []
for idx in top_indices:
    names.append(vectorizer.get_feature_names_out()[idx])
print("Top five most similar words to 'cake': ", names)
# Compute cosine similarity between "cake" and "lie"
similarity = cosine_similarity(np.asarray(tfidf_matrix[:,
    cake_idx].todense()), np.asarray(tfidf_matrix[:, lie_idx].todense())
# The result will be a matrix; we can take the average or
# max similarity value
avg_similarity = similarity.mean()
print("Similarity between 'cake' and 'lie'", avg_similarity)
# Show the similarity between "cake" and "elephant"
elephant_idx = vectorizer.vocabulary_['sing']
similarity = cosine_similarity(np.asarray(tfidf_matrix[:,
    cake_idx].todense()), np.asarray(tfidf_matrix[:,
    elephant_idx].todense()))
avg_similarity = similarity.mean()
print("Similarity between 'cake' and 'sing'",
    avg_similarity)
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=100, # 100 tokens
    chunk_overlap=20, # 20 tokens of overlap
    )
text = """
Welcome to the "Unicorn Enterprises: Where Magic Happens"
Employee Handbook! We're thrilled to have you join our team
of dreamers, doers, and unicorn enthusiasts. At Unicorn
Enterprises, we believe that work should be as enchanting as
it is productive. This handbook is your ticket to the
magical world of our company, where we'll outline the
principles, policies, and practices that guide us on this
extraordinary journey. So, fasten your seatbelts and get
ready to embark on an adventure like no other!
...
As we conclude this handbook, remember that at Unicorn
Enterprises, the pursuit of excellence is a never-ending
quest. Our company's success depends on your passion,
creativity, and commitment to making the impossible
possible. We encourage you to always embrace the magic
within and outside of work, and to share your ideas and
innovations to keep our enchanted journey going. Thank you
for being a part of our mystical family, and together, we'll
continue to create a world where magic and business thrive
hand in hand!
"""
chunks = text_splitter.split_text(text=text)
print(chunks[0:3])
import numpy as np
import faiss
#  The get_vector_embeddings function is defined in a preceding example
emb = [get_vector_embeddings(chunk) for chunk in chunks]
vectors = np.array(emb)
# Create a FAISS index
index = faiss.IndexFlatL2(vectors.shape[1])
index.add(vectors)
# Function to perform a vector search
def vector_search(query_text, k=1):
    query_vector = get_vector_embeddings(query_text)
    distances, indices = index.search(
        np.array([query_vector]), k)
    return [(chunks[i], float(dist)) for dist,
        i in zip(distances[0], indices[0])]
# Example search
user_query = "do we get free unicorn rides?"
search_results = vector_search(user_query)
print(f"Search results for {user_query}:", search_results)
# Save the index to a file
faiss.write_index(index, "data/my_index_file.index")
# Load the index from a file
index = faiss.read_index("data/my_index_file.index")
# Assuming index1 and index2 are two IndexFlatL2 indices
index1.add(index2.reconstruct_n(0, index2.ntotal))
from langchain_community.vectorstores.faiss import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# 1. Create the documents:
documents = [
    "James Phoenix worked at JustUnderstandingData.",
    "James Phoenix currently is 31 years old.",
    """Data engineering is the designing and building systems for collect
    storing, and analyzing data at scale.""",
]
# 2. Create a vectorstore:
vectorstore = FAISS.from_texts(texts=documents, embedding=OpenAIEmbedding
retriever = vectorstore.as_retriever()
# 3. Create a prompt:
template = """Answer the question based only on the following context:
Context: {context}
---
Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)
# 4. Create a chat model:
model = ChatOpenAI()
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)
chain.invoke("What is data engineering?")
from pinecone import Pinecone, ServerlessSpec
import os
# Initialize connection (get API key at app.pinecone.io):
os.environ["PINECONE_API_KEY"] = "insert-your-api-key-here"
index_name = "employee-handbook"
environment = "us-west-2"
pc = Pinecone()  # This reads the PINECONE_API_KEY env var
# Check if index already exists:
# (it shouldn't if this is first time)
if index_name not in pc.list_indexes().names():
    # if does not exist, create index
    pc.create_index(
        index_name,
        # Using the same vector dimensions as text-embedding-ada-002
        dimension=1536,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region=environment),
    )
# Connect to index:
index = pc.Index(index_name)
# View index stats:
index.describe_index_stats()
from tqdm import tqdm # For printing a progress bar
from time import sleep
# How many embeddings you create and insert at once
batch_size = 10
retry_limit = 5  # maximum number of retries
for i in tqdm(range(0, len(chunks), batch_size)):
    # Find end of batch
    i_end = min(len(chunks), i+batch_size)
    meta_batch = chunks[i:i_end]
    # Get ids
    ids_batch = [str(j) for j in range(i, i_end)]
    # Get texts to encode
    texts = [x for x in meta_batch]
    # Create embeddings
    # (try-except added to avoid RateLimitError)
    done = False
    try:
        # Retrieve embeddings for the whole batch at once
        embeds = []
        for text in texts:
            embedding = get_vector_embeddings(text)
            embeds.append(embedding)
        done = True
    except:
        retry_count = 0
        while not done and retry_count < retry_limit:
            try:
                for text in texts:
                    embedding = get_vector_embeddings(text)
                    embeds.append(embedding)
                done = True
            except:
                sleep(5)
                retry_count += 1
    if not done:
        print(f"""Failed to get embeddings after
        {retry_limit} retries.""")
        continue
    # Cleanup metadata
    meta_batch = [{
        'batch': i,
        'text': x
    } for x in meta_batch]
    to_upsert = list(zip(ids_batch, embeds, meta_batch))
    # Upsert to Pinecone
    index.upsert(vectors=to_upsert)
# Retrieve from Pinecone
user_query = "do we get free unicorn rides?"
def pinecone_vector_search(user_query, k):
    xq = get_vector_embeddings(user_query)
    res = index.query(vector=xq, top_k=k, include_metadata=True)
    return res
pinecone_vector_search(user_query, k=1)
res = index.query(xq, filter={
        "batch": {"$eq": 1}
    }, top_k=1, include_metadata=True)
from langchain_core.documents import Document
from langchain_community.vectorstores.chroma import Chroma
from langchain_openai import OpenAIEmbeddings
import lark
import getpass
import os
import warnings
# Disabling warnings:
warnings.filterwarnings("ignore")
from langchain_openai.chat_models import ChatOpenAI
from langchain.retrievers.self_query.base \
    import SelfQueryRetriever
from langchain.chains.query_constructor.base \
    import AttributeInfo
# Create the embeddings and vectorstore:
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())
document_content_description = "Brief summary of a movie"
llm = ChatOpenAI(temperature=0)
retriever = SelfQueryRetriever.from_llm(
    llm, vectorstore, document_content_description, metadata_field_info
)
retriever = SelfQueryRetriever.from_llm(
    llm,
    vectorstore,
    document_content_description,
    metadata_field_info,
    enable_limit=True,
)
retriever.get_relevant_documents(
    query="Return 2 Fantasy books",
)
```
