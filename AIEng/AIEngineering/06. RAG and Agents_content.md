---
sidebar_position: 1
tags: ["RAG (Retrieval-Augmented Generation)", "Agents", "Information Retrieval", "Embedding-based Retrieval", "Term-based Retrieval", "Embeddings", "Vector Databases", "FAISS", "ScaNN", "Annoy", "Contextual Retrieval", "Multimodal Embedding Models (e.g., CLIP)", "SQL Query Execution", "AI Agents", "AI models", "Tool use", "Planning", "Function calling", "Multi-agent systems", "Function Calling", "Planning Granularity", "Control Flows", "Reflection and Error Correction", "Tool Selection", "Memory Systems", "Tool Failures", "Planning Failures", "RAG", "LSTM", "BM25", "Transformer"]
title: "06. RAG and Agents"
---

## summary

- The chapter discusses the importance of providing AI models with both instructions and relevant context to perform tasks accurately, highlighting the limitations when context is missing.
- Two main patterns for context construction are introduced: RAG (Retrieval-Augmented Generation) and Agents, each serving different purposes in enhancing model capabilities.
- RAG enhances model generation by retrieving relevant information from external sources, while Agents use tools to gather information, enabling models to interact with the world directly.
- The chapter explains the architecture of RAG systems, emphasizing the roles of retrievers and generators, and the importance of quality retrieval for system success.
- Retrieval algorithms are categorized into term-based and embedding-based retrieval, each with its mechanisms for computing relevance scores.
- Term-based retrieval focuses on keyword matching and uses TF-IDF for scoring, while embedding-based retrieval aims for semantic alignment between queries and documents.
- The chapter also touches on the challenges of tokenization in term-based retrieval and the advantages of embedding-based retrieval in capturing semantic meanings.
- Practical implementation opportunities include building RAG systems with off-the-shelf components and finetuning them for improved performance.
- Real-world applications of RAG and Agents include automating tasks and enhancing the capabilities of conversational agents.
- The chapter concludes with insights into the future of RAG and Agents, suggesting their potential to revolutionize how models interact with information and the world.
- Important images include diagrams illustrating the retrieve-then-generate pattern, RAG architecture, and the workflow of embedding-based retrieval systems.
- Embedding-based retrieval relies on vectors to preserve the important properties of original data, with the quality of the embedding model being crucial for effectiveness.
- Vector databases are introduced as a new component, focusing on the storage and efficient search of vectors, which is framed as a nearest-neighbor search problem.
- The naive solution for vector search is k-nearest neighbors (k-NN), which is precise but computationally heavy and slow, making it suitable only for small datasets.
- For large datasets, approximate nearest neighbor (ANN) algorithms are used, with popular libraries including FAISS, ScaNN, Annoy, and Hnswlib.
- Vector search algorithms organize vectors into buckets, trees, or graphs, with different heuristics to increase the likelihood that similar vectors are close to each other.
- Vectors can be quantized or made sparse to reduce computational intensity, with LSH, HNSW, Product Quantization, IVF, and Annoy being significant vector search algorithms.
- Term-based retrieval is faster and cheaper than embedding-based retrieval but has limitations in performance improvement and term ambiguity.
- Embedding-based retrieval can outperform term-based retrieval with finetuning but is more expensive and can obscure specific keywords.
- The quality of a retriever is evaluated based on context precision and recall, with metrics like NDCG, MAP, and MRR used for ranking evaluation.
- Hybrid search combines term-based and embedding-based retrieval, using different algorithms in sequence or parallel to improve retrieval quality.
- Retrieval optimization tactics include chunking strategy, reranking, query rewriting, and contextual retrieval to increase the chance of fetching relevant documents.
- Chunking strategies impact retrieval performance, with considerations for chunk size, overlap, and the use of tokens as boundaries.
- Reranking improves the accuracy of initial document rankings, with strategies including the use of more precise mechanisms and time-based weighting.
- Query rewriting addresses ambiguity in user queries, using heuristics or AI models to reformulate queries for better retrieval results.
- The chapter highlights the importance of evaluating retrieval quality, RAG outputs, and embeddings for a comprehensive assessment of a RAG system.
- Important images or descriptions mentioned in the book include an example of query rewriting in a conversational AI system, demonstrating the importance of accurately interpreting user intent.
- The chapter discusses the importance of contextual retrieval in RAG systems to avoid hallucinations and improve answer accuracy by augmenting chunks with relevant metadata.
- Techniques for augmenting chunks include adding metadata like tags, keywords, and entities extracted from the content, as well as questions the chunk can answer.
- Anthropic's method of generating a short context for each chunk to situate it within the original document is highlighted, improving search retrieval.
- Key factors for evaluating retrieval solutions include supported retrieval mechanisms, scalability, query latency, and pricing structure.
- The chapter explores extending RAG systems to handle multimodal data (texts, images, videos) and tabular data, requiring different workflows like text-to-SQL conversion.
- AI agents are introduced as systems that perceive their environment and act upon it, with capabilities enhanced by access to tools like retrievers and SQL executors.
- The potential of AI agents in various applications is vast, but challenges include compound mistakes and higher stakes due to more impactful tasks.
- Tools for agents are categorized into knowledge augmentation, capability extension, and actions upon the environment, each expanding the agent's functionalities.
- Web browsing tools are discussed as a means to keep models up-to-date but caution is advised due to the risks of accessing unreliable information.
- The chapter concludes with the importance of carefully selecting tools for agents to balance capabilities with complexity and potential risks.
- Important images include diagrams of Anthropic's contextual retrieval process, multimodal RAG workflow, and SWE-agent's interaction with a computer environment.
- AI models can significantly improve their performance by leveraging external tools, such as calculators or code interpreters, instead of being trained to perform tasks they're inherently bad at.
- Tool use can transform text-only or image-only models into multimodal systems by integrating additional capabilities like image generation or audio processing.
- The Chameleon agent demonstrates the power of tool augmentation, outperforming standalone models on benchmarks by integrating tools like knowledge retrieval and Bing search.
- Write actions enable AI systems to automate complex workflows, such as customer outreach, by performing operations like sending emails or updating databases.
- Security and trust are paramount when granting AI systems the ability to perform write actions, due to risks like code injection attacks or unauthorized data alterations.
- Planning is essential for complex tasks, involving generating, validating, and executing plans, with the possibility of human oversight to mitigate risks.
- Foundation models as planners face skepticism regarding their ability to backtrack or predict outcomes, but integrating tools and state tracking systems may enhance their planning capabilities.
- Function calling allows models to invoke tools dynamically, with model APIs supporting this feature to enable more complex agent behaviors.
- The chapter highlights the importance of security measures, planning validation, and the potential convergence of foundation model agents and reinforcement learning agents.
- Practical steps for improving an agent's planning include better system prompts, tool descriptions, function refactoring, and using stronger models.
- The image description details a flowchart illustrating the process of executing tasks with tools, emphasizing the roles of the Planner, Executor, and Evaluator in a RAG agent's operation.
- Function calling in agents involves specifying tools the agent can use, with APIs allowing control over tool use per query through settings like required, none, or auto.
- An example illustrates an agent using tools for unit conversion, demonstrating how agents generate tool use and parameters automatically.
- Planning granularity discusses the trade-off between detailed and high-level plans, suggesting hierarchical planning as a solution to balance generation and execution difficulty.
- The text highlights the importance of using natural language in plans to make them robust against changes in tool APIs, despite the need for a translator to convert plans into executable commands.
- Different control flows in executing plans are introduced, including sequential, parallel, if statement, and for loop, with their implications for agent frameworks.
- Reflection and error correction are crucial for agent success, with mechanisms like self-critique prompts and specialized scorers to evaluate and adjust plans.
- The ReAct framework is mentioned as a common pattern for agents, interleaving reasoning and action to improve task accomplishment.
- Tool selection is critical for agent performance, with considerations on the number of tools, their descriptions, and the model's ability to use them effectively.
- Experiments show different tasks and models have varying tool preferences, emphasizing the need for experimentation in tool selection.
- The concept of tool transition is introduced, suggesting agents can combine frequently used tools to build more complex ones.
- Important images include Figure 6-10 showing function calling, Figure 6-11 illustrating control flows, and Figure 6-12 demonstrating the ReAct framework in action.
- The chapter discusses the importance of RAG and agent systems in AI, focusing on their functionality, evaluation, and enhancement through memory systems.
- Vogager introduces a skill manager for agents to track and reuse skills, highlighting the importance of tool inventory and planning capabilities for agent success.
- Agent failure modes include planning, tool execution, and efficiency issues, with evaluation methods to detect and measure these failures.
- Planning failures can stem from invalid tools, incorrect parameters, or goal failures, emphasizing the need for careful agent evaluation.
- Tool failures occur when correct tools produce wrong outputs, underlining the necessity for independent tool testing and domain-specific tool development.
- Efficiency in agents is measured by steps, cost, and time per task, comparing AI and human operational efficiencies.
- Memory systems in AI, categorized into internal knowledge, short-term, and long-term memory, enhance RAG and agent systems by managing information overflow and persisting data between sessions.
- Memory management strategies like FIFO and redundancy removal are discussed for efficient memory use in AI applications.
- The chapter concludes with the significance of RAG and agents in overcoming model context limitations and enhancing response quality and efficiency.
- Key images include a tool transition tree and diagrams illustrating the hierarchy of information for an agent and the use of RAG within an AI system framework.
- The chapter discusses how equipping models with more tools enhances their capabilities but also increases the potential for catastrophic failures and security risks.
- It highlights the necessity of defensive mechanisms for agents to operate effectively in real-world scenarios.
- Both RAG and agents manage large amounts of information, often exceeding the model's context length, requiring a memory system for efficient information management.
- RAG and agents are prompt-based methods that improve model quality through inputs without altering the model itself, though modifying the model could unlock further possibilities.
- The LSTM model is mentioned as a dominant architecture in NLP before the transformer architecture took over in 2018.
- Augmenting pre-trained language models with retrieval systems can significantly enhance performance on factual questions, as demonstrated by research.
- Parkinson’s Law is analogously applied to model context limits, suggesting that applications expand to utilize the full context available.
- The history of information retrieval dates back to the 1920s, with significant contributions over the years, including the development of BM25.
- Improving upon traditional search methods like BM25 is challenging, indicating the complexity of retrieval system advancements.
- Retrieval systems in RAG workflows resemble traditional recommender systems, with some teams finding question-and-answer formatted data most effective.
- Agents are defined as entities that perceive their environment through sensors and act through actuators, with early criticisms focusing on their inefficiency.
- Agentic workflows often involve multiple components, leading to the prevalence of multi-agent systems.
- The concept of a program generator in agents is likened to the actor-critic method in reinforcement learning.
- Usage-based strategies for information management in models are complex, requiring mechanisms to track information usage.
- The chapter concludes with a note on the potential for model modification to explore new possibilities, setting the stage for the next chapter's topic.

## code snippets
```
No direct code references found in the chapter.
SELECT SUM(units) AS total_units_sold FROM Sales WHERE product_name = 'Fruity Fedora' AND timestamp >= DATE_SUB(CURDATE(), INTERVAL 7 DAY);
get_today_date()
fetch_top_products(start_date, end_date, num_products)
fetch_product_info(product_name)
generate_query(task_history, tool_output)
generate_response(query)
response = ModelResponse(
   finish_reason='tool_calls',
   message=chat.Message(
       content=None,
       role='assistant',
       tool_calls=[
           ToolCall(
               function=Function(
                   arguments='{"lbs":40}',
                   name='lbs_to_kg'),
               type='function')
       ])
)
```
